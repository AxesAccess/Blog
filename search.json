[
  {
    "objectID": "posts/20250531-content-mate/index.html",
    "href": "posts/20250531-content-mate/index.html",
    "title": "Product Cards Creation Application",
    "section": "",
    "text": "This application is designed to create product cards for an online store. It utilizes a large language model (LLM) and programming libraries to generate detailed descriptions based on product specifications, images, and files. This allows a business to streamline the process of crafting informative and engaging product listings with minimal manual effort. This enhances both productivity and content quality."
  },
  {
    "objectID": "posts/20240927-euro-tech-money/index.html",
    "href": "posts/20240927-euro-tech-money/index.html",
    "title": "European Tech Salaries",
    "section": "",
    "text": "Recently, I stumbled upon a Reddit post where someone was gathering salary data from the tech sector throughout Europe. It piqued my interest to explore how these salaries vary among various countries and positions. So, I chose to employ R for data collection and analysis."
  },
  {
    "objectID": "posts/20240927-euro-tech-money/index.html#introduction",
    "href": "posts/20240927-euro-tech-money/index.html#introduction",
    "title": "European Tech Salaries",
    "section": "",
    "text": "Recently, I stumbled upon a Reddit post where someone was gathering salary data from the tech sector throughout Europe. It piqued my interest to explore how these salaries vary among various countries and positions. So, I chose to employ R for data collection and analysis."
  },
  {
    "objectID": "posts/20240927-euro-tech-money/index.html#data-collection",
    "href": "posts/20240927-euro-tech-money/index.html#data-collection",
    "title": "European Tech Salaries",
    "section": "Data Collection",
    "text": "Data Collection\nLet’s start by fetching the data from the Google Sheet.\n\n\nShow the code\n\ndocument &lt;- \"1iTNwiAQ0s5iD6RqI7B30uWqQ8wNJqRnmHvxo5zRffu8\"\nsheet &lt;- \"603717461\"\nurl = sprintf(\n  \"https://docs.google.com/spreadsheets/d/%s/gviz/tq?tqx=out:csv&sheet=%s\",\n  document,\n  sheet\n)\ndownload.file(url, destfile = \"data/euro-tech-money.csv\", mode = \"wb\")\n\nOnce downloaded, we can load the data into our R environment.\n\n\nShow the code\ndata &lt;- read.csv(\"data/euro-tech-money.csv\")\n\n\nThe dataset consists of 572 observations in 12 columns named Job.Title, Company, City, Seniority, Pre.Tax.TC, After.Tax.TC, Yearly.Savings, Lifestyle, Household.Size, Share.of.Household.Expenses, Country, and Timestamp. See summary below.\n\n\nShow summary\n\n\n\n  Job.Title           Company              City            Seniority        \n Length:572         Length:572         Length:572         Length:572        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n   Pre.Tax.TC      After.Tax.TC     Yearly.Savings    Lifestyle        \n Min.   : 13646   Min.   :  10000   Min.   : -2000   Length:572        \n 1st Qu.: 48000   1st Qu.:  32600   1st Qu.:  8000   Class :character  \n Median : 70000   Median :  46500   Median : 17000   Mode  :character  \n Mean   : 85140   Mean   :  58791   Mean   : 23915                     \n 3rd Qu.: 99600   3rd Qu.:  65190   3rd Qu.: 30000                     \n Max.   :700000   Max.   :1575000   Max.   :270000                     \n NA's   :11       NA's   :23        NA's   :68                         \n Household.Size  Share.of.Household.Expenses   Country         \n Min.   :1.000   Min.   :  0.0               Length:572        \n 1st Qu.:1.000   1st Qu.: 65.0               Class :character  \n Median :2.000   Median :100.0               Mode  :character  \n Mean   :1.853   Mean   : 82.3                                 \n 3rd Qu.:2.000   3rd Qu.:100.0                                 \n Max.   :7.000   Max.   :100.0                                 \n NA's   :22      NA's   :55                                    \n  Timestamp        \n Length:572        \n Class :character  \n Mode  :character"
  },
  {
    "objectID": "posts/20240927-euro-tech-money/index.html#geography",
    "href": "posts/20240927-euro-tech-money/index.html#geography",
    "title": "European Tech Salaries",
    "section": "Geography",
    "text": "Geography\nLet’s take a look at the location of respondents. We’re going to load a map of Europe and plot the cities where the respondents are located. The map will also show the number of respondents in each city and their median salary in USD.\n\n\nShow the code\nlibrary(giscoR)\nlibrary(maps)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(ggrepel)\n\n# Load the map of Europe\neurope &lt;- gisco_get_countries(\n  region = \"Europe\",\n  resolution = 1,\n  cache_dir = \"/tmp/giscoR\"\n)\n\n# Get the cities\ncities &lt;- world.cities |&gt;\n  filter(str_to_upper(country.etc) %in% unique(str_to_upper(data$Country))) |&gt;\n  select(name, country.etc, long, lat)\n\n# Change case in the column to title case\ndata &lt;- data |&gt; mutate(City = str_to_title(City))\n\n# Group responses data frame by the city\nby_city &lt;- select(data, City, Pre.Tax.TC) |&gt;\n  group_by(City) |&gt;\n  summarise(\n    resp_count = n(),\n    median_salary = median(Pre.Tax.TC, na.rm = TRUE)\n  )\n# Join the cities and responses data frames\nby &lt;- join_by(name == City)\n\nby_city &lt;- inner_join(cities, by_city, by)\n\np &lt;- by_city |&gt;\n  arrange(resp_count) |&gt;\n  mutate(name = factor(name, unique(name))) |&gt;\n  ggplot() +\n  geom_sf(\n    data = europe,\n    fill = \"grey\",\n    alpha = 0.3\n  ) +\n  geom_point(\n    aes(\n      x = long,\n      y = lat,\n      size = resp_count,\n      color = median_salary\n    ),\n    alpha = 0.9\n  ) +\n  scale_color_viridis_c(\n    trans = \"log\", option = \"plasma\",\n    breaks = c(25000, 50000, 100000, 200000, 400000)\n  ) +\n  theme_void() +\n  ylim(35, 65) +\n  xlim(-15, 40)\n\np1 &lt;- p + theme(\n  legend.position = \"none\",\n  plot.margin = grid::unit(c(50, 50, 50, 50), \"pt\")\n)\n\nggsave(\"image.png\", plot = p1, width = 8, height = 8)\n\np + geom_text_repel(\n  data = by_city |&gt; arrange(resp_count) |&gt; tail(20),\n  aes(x = long, y = lat, label = name),\n  size = 4\n) +\n  theme(\n    legend.position = \"right\",\n    legend.key.height = unit(20, \"pt\"),\n    legend.box.margin = margin(0, 0, 0, 20)\n  )\n\n\n\n\n\n\n\n\nFigure 1: Geography of respondents"
  },
  {
    "objectID": "posts/20240927-euro-tech-money/index.html#salaries-by-city",
    "href": "posts/20240927-euro-tech-money/index.html#salaries-by-city",
    "title": "European Tech Salaries",
    "section": "Salaries by City",
    "text": "Salaries by City\nThe total number of cities is 105. Below is a list of the top 10 and bottom 10 cities with at least 5 respondents, ranked by median salary. While these plots can provide a general idea of the salary distribution, it is not the best idea to compare salaries across cities directly, as the salary may vary depending on other factors like job title or seniority.\n\n\nShow the code\nlibrary(scales)\n\ncities_ranked &lt;- data |&gt;\n  inner_join(cities, by = c(\"City\" = \"name\")) |&gt;\n  group_by(City) |&gt;\n  summarize(median_salary = median(Pre.Tax.TC), resp_count = n()) |&gt;\n  filter(resp_count &gt; 4) |&gt;\n  arrange(desc(median_salary))\n\n\ndata &lt;- data |&gt;\n  mutate(city_rank = ifelse(City %in% cities_ranked$City[1:10], \"Top\", \"Other\"))\ndata &lt;- data |&gt;\n  mutate(city_rank = ifelse(City %in% tail(cities_ranked, 10)$City, \"Tail\", city_rank))\ndata &lt;- data |&gt;\n  mutate(city_rank = factor(city_rank, levels = c(\"Top\", \"Tail\")))\n\nxlim &lt;- c(\n  0,\n  max(data$Pre.Tax.TC)\n)\n\ndata |&gt;\n  inner_join(cities_ranked, by = c(\"City\" = \"City\")) |&gt;\n  filter(city_rank %in% c(\"Top\", \"Tail\")) |&gt;\n  ggplot(aes(\n    x = reorder(factor(City), Pre.Tax.TC, median),\n    y = Pre.Tax.TC\n  )) +\n  facet_wrap(~ as.factor(city_rank), scales = \"free_y\") +\n  geom_boxplot(aes(colour = median_salary)) +\n  scale_color_viridis_c(\n    trans = \"log\",\n    option = \"plasma\",\n    begin = 0.,\n    end = 0.85,\n    breaks = c(25000, 50000, 100000, 200000, 400000)\n  ) +\n  theme_minimal() +\n  coord_cartesian(xlim = xlim) +\n  coord_flip() +\n  labs(\n    x = \"\",\n    y = \"\"\n  ) +\n  theme(legend.position = \"none\") +\n  scale_y_continuous(labels = scales::comma)\n\n\n\n\n\n\n\n\nFigure 2: Top and bottom cities by median salary"
  },
  {
    "objectID": "posts/20240927-euro-tech-money/index.html#salaries-by-country-and-job-title",
    "href": "posts/20240927-euro-tech-money/index.html#salaries-by-country-and-job-title",
    "title": "European Tech Salaries",
    "section": "Salaries by Country and Job Title",
    "text": "Salaries by Country and Job Title\nLet’s visualize the median salary by country. Since salaries vary by position, it’s important to include job titles on the axis. However, with 249 distinct job titles, we need to group them into broader categories for clarity.\nIn the following plot, the size of the point represents the number of respondents, and the color represents the median salary. The text on the plot shows the median salary for each country and job category.\n\n\nShow the code\ndata1 &lt;- data |&gt; inner_join(cities, by = c(\"City\" = \"name\"))\n\ndata1 &lt;- data1 |&gt; left_join(read.csv(\"data/job-categories.csv\"), by = \"Job.Title\")\n\ncountry_job_title &lt;- data1 |&gt;\n  # Convert to long format\n  as_tibble() |&gt;\n  group_by(Country, Job.Category) |&gt;\n  summarize(\n    resp_count = n(),\n    median_salary = median(Pre.Tax.TC)\n  ) |&gt;\n  ungroup()\n\n\ncountry_job_title |&gt;\n  ggplot(aes(x = Job.Category, y = reorder(Country, desc(Country)))) +\n  geom_point(aes(size = resp_count, colour = median_salary), shape = 21, stroke = T, alpha = 0.9) +\n  scale_color_viridis_c(\n    trans = \"log\",\n    option = \"plasma\",\n    breaks = c(25000, 50000, 100000, 200000)\n  ) +\n  scale_size_area(max_size = 8) +\n  geom_text(\n    aes(\n      y = as.numeric(as.factor(reorder(Country, desc(Country)))) - sqrt(resp_count) / 20,\n      label = format(median_salary, big.mark = \",\"),\n    ),\n    vjust = 1.5,\n    hjust = 0.5,\n    colour = \"grey30\",\n    size = 2.5\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    axis.ticks.y = element_blank(),\n    legend.position = \"none\",\n    # legend.position = \"right\",\n    # legend.key.height = unit(20, \"pt\"),\n    # legend.key.width = unit(5, \"pt\"),\n    # legend.box.margin = margin(0, 0, 0, 10),\n    # legend.title = element_blank()\n  ) +\n  labs(x = NULL, y = NULL)\n\n\n\n\n\n\n\n\nFigure 3: Median salary by country and job title\n\n\n\n\n\nAs we can see, salaries vary significantly by country and job title. Directly comparing salaries across these groups can be misleading due to differences in other influencing factors. To better understand the contribution of each factor, let’s build a linear regression model that accounts for these variables and potentially others, while controlling for confounding factors.\n\n\nShow the code\ndata1 &lt;- data |&gt; inner_join(cities, by = c(\"City\" = \"name\"))\ndata1 &lt;- data1 |&gt; left_join(read.csv(\"data/job-categories.csv\"), by = \"Job.Title\")\nmodel &lt;- lm(Pre.Tax.TC ~ 0 + Country + Job.Category + Seniority, data = data1)"
  },
  {
    "objectID": "posts/20240927-euro-tech-money/index.html#factors-affecting-salary",
    "href": "posts/20240927-euro-tech-money/index.html#factors-affecting-salary",
    "title": "European Tech Salaries",
    "section": "Factors Affecting Salary",
    "text": "Factors Affecting Salary\nThe linear model, built using Country, Job Category, and Seniority, demonstrates decent predictive power, with an adjusted R-squared value of 0.82. The model’s coefficients represent the effect (in USD) of each factor on salary. Let’s visualize these coefficients to gain a clearer understanding of the impact of each factor.\n\n\nShow the code\nlibrary(tibble)\n\nmodel_coef &lt;- summary(model)$coefficients |&gt;\n  data.frame() |&gt;\n  rownames_to_column(\"value\") |&gt;\n  mutate(\n    effect = Estimate,\n    error = `Std..Error`,\n    p.value = `Pr...t..`,\n    significant = p.value &lt; 0.05\n  )\n\nmodel_coef &lt;- model_coef |&gt;\n  mutate(\n    variable = case_when(\n      str_detect(value, \"Country\") ~ \"Country\",\n      str_detect(value, \"Job.Category\") ~ \"Job Category\",\n      str_detect(value, \"Seniority\") ~ \"Seniority\"\n    )\n  )\n\nmodel_coef &lt;- model_coef |&gt; mutate(value = sub(\"Country\", \"\", value))\nmodel_coef &lt;- model_coef |&gt; mutate(value = sub(\"Job.Category\", \"\", value))\nmodel_coef &lt;- model_coef |&gt; mutate(value = sub(\"Seniority\", \"\", value))\n\ncountry_coef &lt;- model_coef |&gt;\n  filter(variable == \"Country\") |&gt;\n  arrange(desc(effect))\ncountry_coef &lt;- country_coef |&gt; mutate(rownumber = 1:nrow(country_coef))\n\nxlim &lt;- c(\n  min(country_coef$effect - country_coef$error),\n  max(country_coef$effect + 1.1 * country_coef$error)\n)\nxscale &lt;- c(0, 50000, 100000, 150000)\n\n\nThe plot below illustrates the effect of each country on salary. The dot represents the estimated effect, while the error bars show the standard error. The color of the plot elements and the accompanying text indicate the significance of the effect. Only a few countries have a statistically significant impact on salary at the 0.05 level. However, it is generally better practice to consider the overall differences in salaries across countries.\n\nShow the code\ncountry_coef[1:17, ] |&gt;\n  ggplot(aes(x = effect, y = reorder(value, effect), colour = significant)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = effect - error, xmax = effect + error),\n    height = .2\n  ) +\n  geom_text(aes(label = paste(\"p =\", format(p.value, digits = 2))),\n    vjust = 1.5,\n    hjust = -0.2,\n    colour = \"grey55\",\n    fill = \"white\",\n    size = 2.5\n  ) +\n  geom_vline(\n    xintercept = 0,\n    linetype = \"dashed\",\n    colour = \"grey55\"\n  ) +\n  theme_minimal() +\n  coord_cartesian(xlim = xlim) +\n  scale_x_discrete(limits = xscale, labels = scales::comma) +\n  theme(legend.position = \"none\") +\n  labs(x = \"\", y = \"\")\ncountry_coef[18:34, ] |&gt;\n  ggplot(aes(x = effect, y = reorder(value, effect), colour = significant)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = effect - error, xmax = effect + error),\n    height = .2\n  ) +\n  geom_text(aes(label = paste(\"p =\", format(p.value, digits = 2))),\n    vjust = 1.5,\n    hjust = -0.2,\n    colour = \"grey55\",\n    fill = \"white\",\n    size = 2.5\n  ) +\n  geom_vline(\n    xintercept = 0,\n    linetype = \"dashed\",\n    colour = \"grey55\"\n  ) +\n  theme_minimal() +\n  coord_cartesian(xlim = xlim) +\n  scale_x_discrete(limits = xscale, labels = scales::comma) +\n  theme(legend.position = \"none\") +\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Effect of country on salary\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext two plots show the effect of job category and seniority on the salary. The interpretation is similar to the previous plot.\n\nShow the code\nmodel_coef |&gt;\n  filter(variable == \"Job Category\") |&gt;\n  ggplot(aes(x = effect, y = reorder(value, effect), colour = significant)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = effect - error, xmax = effect + error),\n    height = .2\n  ) +\n  geom_text(aes(label = paste(\"p =\", format(p.value, digits = 2))),\n    vjust = 1.5,\n    hjust = -0.2,\n    colour = \"grey55\",\n    fill = \"white\",\n    size = 2.5\n  ) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  theme_minimal() +\n  scale_x_discrete(limits = c(-50000, 50000), labels = scales::comma) +\n  theme(legend.position = \"none\") +\n  labs(x = \"\", y = \"\")\nmodel_coef |&gt;\n  filter(variable == \"Seniority\") |&gt;\n  mutate(value = str_replace(value, \"Senior Staff / \", \"\")) |&gt;\n  ggplot(aes(x = effect, y = reorder(value, effect), colour = significant)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = effect - error, xmax = effect + error),\n    height = .2\n  ) +\n  geom_text(aes(label = paste(\"p =\", format(p.value, digits = 2))),\n    vjust = 1.5,\n    hjust = -0.2,\n    colour = \"grey55\",\n    fill = \"white\",\n    size = 2.5\n  ) +\n  geom_vline(\n    xintercept = 0,\n    linetype = \"dashed\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  scale_x_discrete(limits = c(-50000, 50000)) +\n  labs(x = \"\", y = \"\") +\n  scale_x_continuous(labels = scales::comma)\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Effect of job title on salary\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Effect of seniority on salary"
  },
  {
    "objectID": "posts/20240927-euro-tech-money/index.html#taxation",
    "href": "posts/20240927-euro-tech-money/index.html#taxation",
    "title": "European Tech Salaries",
    "section": "Taxation",
    "text": "Taxation\nNet salary is what matters most to employees, but it differs from gross salary. Taxes and social security contributions can significantly reduce take-home pay. Let’s calculate the net salary for each respondent and visualize the distribution of net salaries by country.\n\n\nShow the code\nlibrary(tidyr)\n\nnet_by_country &lt;- select(data, Country, City, Pre.Tax.TC, After.Tax.TC) |&gt;\n  inner_join(cities, by = c(\"City\" = \"name\")) |&gt;\n  mutate(Tax = Pre.Tax.TC - After.Tax.TC) |&gt;\n  group_by(Country) |&gt;\n  summarise(\n    After.Tax.TC = mean(After.Tax.TC, na.rm = TRUE),\n    Tax = mean(Tax, na.rm = TRUE),\n    Net = After.Tax.TC,\n    Tax.Percent = Tax / (Net + Tax)\n  ) |&gt;\n  gather(key = \"variable\", value = \"value\", -Country, -Net, -Tax.Percent)\n\nnet_by_country$variable &lt;-\n  factor(net_by_country$variable, levels = c(\"Tax\", \"After.Tax.TC\"))\n\nnet_by_country$Country &lt;-\n  factor(net_by_country$Country,\n    levels = unique(net_by_country$Country[order(net_by_country$Net)])\n  )\n\nnet_by_country |&gt;\n  ggplot(aes(x = Country, y = value, fill = variable)) +\n  geom_col(width = 0.75, alpha = 0.9) +\n  geom_text(\n    aes(label = ifelse(variable == \"Tax\",\n      scales::percent(Tax.Percent, accuracy = 1), \"\"\n    )),\n    position = position_stack(vjust = 0.5),\n    colour = \"grey30\", size = 2.5\n  ) +\n  coord_flip() +\n  theme_minimal() +\n  labs(x = \"\", y = \"\", fill = \"\") +\n  scale_y_continuous(labels = scales::comma)\n\n\n\n\n\n\n\n\nFigure 7: Net salary by country\n\n\n\n\n\nWe can see that the tax burden varies significantly across countries. While the net salary is the most important factor for employees, it is also important to consider the cost of living in each country."
  },
  {
    "objectID": "posts/20240927-euro-tech-money/index.html#cost-of-living",
    "href": "posts/20240927-euro-tech-money/index.html#cost-of-living",
    "title": "European Tech Salaries",
    "section": "Cost of Living",
    "text": "Cost of Living\nWe have net salary, but how far does it go in each country? Let’s calculate the cost of living for each respondent and visualize the distribution of costs by country.\nWe have the yearly savings for each respondent, which is the difference between the net salary and the cost of living. Having known household size and share of household expenses, we can calculate the cost of living for a household of a certain size. For this, we are going to employ the linear regression.\n\n\nShow the code\ndata1 &lt;- data |&gt; inner_join(cities, by = c(\"City\" = \"name\"))\n# let's exclude unrealistic values\ndata1 &lt;- data1 |&gt; filter(Share.of.Household.Expenses &gt; 10)\ndata1 &lt;- data1 |&gt; filter(Yearly.Savings &lt; After.Tax.TC)\n\ndata1 &lt;- data1 |&gt; mutate(Cost.of.Living = (After.Tax.TC - Yearly.Savings) / Share.of.Household.Expenses * 100)\n# let's convert Household.Size to a factor\ndata1 &lt;- data1 |&gt; mutate(Household.Size = as.factor(round(Household.Size)))\n\nmodel &lt;- lm(Cost.of.Living ~ 0 + Country + Household.Size, data = data1)\n\nmodel_coef &lt;- summary(model)$coefficients |&gt;\n  data.frame() |&gt;\n  rownames_to_column(\"value\") |&gt;\n  mutate(\n    effect = Estimate,\n    error = `Std..Error`,\n    p.value = `Pr...t..`,\n    significant = p.value &lt; 0.05\n  )\n\nmodel_coef &lt;- model_coef |&gt;\n  mutate(\n    variable = case_when(\n      str_detect(value, \"Country\") ~ \"Country\",\n      str_detect(value, \"Household.Size\") ~ \"Household Size\",\n      str_detect(value, \"Lifestyle\") ~ \"Lifestyle\"\n    )\n  )\n\nmodel_coef &lt;- model_coef |&gt; mutate(value = sub(\"Country\", \"\", value))\nmodel_coef &lt;- model_coef |&gt; mutate(value = sub(\"Household.Size\", \"\", value))\nmodel_coef &lt;- model_coef |&gt; mutate(value = sub(\"Lifestyle\", \"\", value))\n\ncountry_coef &lt;- model_coef |&gt;\n  filter(variable == \"Country\") |&gt;\n  arrange(desc(effect))\ncountry_coef &lt;- country_coef |&gt;\n  mutate(rownumber = 1:nrow(country_coef))\n\nxlim &lt;- c(\n  min(country_coef$effect - country_coef$error),\n  max(country_coef$effect + 2 * country_coef$error)\n)\nxscale &lt;- c(0, 25000, 50000, 75000, 100000)\n\n\nThe model Cost.of.Living ~ 0 + Country + Household.Size has an adjusted R-squared value of 0.75. We will not use Lifestyle as a factor in the model, as some of the levels have a small number of observations.\nLet’s look at the coefficients for the country variable. Basically, the effect represents cost of living for a single person.\n\nShow the code\ncountry_coef[1:17, ] |&gt;\n  ggplot(aes(x = effect, y = reorder(value, effect), colour = significant)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = effect - error, xmax = effect + error),\n    height = .2\n  ) +\n  geom_text(aes(label = paste(\"p =\", format(p.value, digits = 2))),\n    vjust = 1.5,\n    hjust = -0.2,\n    colour = \"grey55\",\n    fill = \"white\",\n    size = 2.5\n  ) +\n  geom_vline(\n    xintercept = 0,\n    linetype = \"dashed\",\n    colour = \"grey55\"\n  ) +\n  theme_minimal() +\n  coord_cartesian(xlim = xlim) +\n  scale_x_discrete(limits = xscale, labels = scales::comma) +\n  theme(legend.position = \"none\") +\n  labs(x = \"\", y = \"\")\ncountry_coef[18:34, ] |&gt;\n  ggplot(aes(x = effect, y = reorder(value, effect), colour = significant)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = effect - error, xmax = effect + error),\n    height = .2\n  ) +\n  geom_text(aes(label = paste(\"p =\", format(p.value, digits = 2))),\n    vjust = 1.5,\n    hjust = -0.2,\n    colour = \"grey55\",\n    fill = \"white\",\n    size = 2.5\n  ) +\n  geom_vline(\n    xintercept = 0,\n    linetype = \"dashed\",\n    colour = \"grey55\"\n  ) +\n  theme_minimal() +\n  coord_cartesian(xlim = xlim) +\n  scale_x_discrete(limits = xscale, labels = scales::comma) +\n  theme(legend.position = \"none\") +\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Effect of country on cost of living\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext, we will look at the effect of household size on the cost of living. As expected, total cost of living for the two people is higher than for a single person, but the cost per person is lower for a larger household.\n\n\nShow the code\nxscale &lt;- c(0, 10000, 20000, 30000, 40000, 50000)\nxlim &lt;- c(-10000, 60000)\n\nmodel_coef |&gt;\n  filter(variable == \"Household Size\") |&gt;\n  ggplot(aes(x = effect, y = reorder(value, desc(value)), colour = significant)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = effect - error, xmax = effect + error),\n    height = .2\n  ) +\n  geom_text(aes(label = paste(\"p =\", format(p.value, digits = 2))),\n    vjust = 1.5,\n    hjust = -0.2,\n    colour = \"grey55\",\n    fill = \"white\",\n    size = 2.5\n  ) +\n  geom_vline(\n    xintercept = 0,\n    linetype = \"dashed\",\n    colour = \"grey55\"\n  ) +\n  theme_minimal() +\n  coord_cartesian(xlim = xlim) +\n  scale_x_discrete(limits = xscale, labels = scales::comma) +\n  theme(legend.position = \"none\") +\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nFigure 9: Effect of household size on cost of living"
  },
  {
    "objectID": "posts/20240927-euro-tech-money/index.html#savings",
    "href": "posts/20240927-euro-tech-money/index.html#savings",
    "title": "European Tech Salaries",
    "section": "Savings",
    "text": "Savings\nFinally, let’s look at the distribution of yearly savings by country. We will employ the same approach as before, using a linear regression model to understand the factors affecting savings. We will use Country, Household.Size, Job.Category, Seniority, and Share.of.Household.Expenses as predictors.\n\n\nShow the code\ndata1 &lt;- data |&gt; inner_join(cities, by = c(\"City\" = \"name\"))\n# let's exclude unrealistic values\ndata1 &lt;- data1 |&gt; filter(Share.of.Household.Expenses &gt; 10)\ndata1 &lt;- data1 |&gt; filter(Yearly.Savings &lt; After.Tax.TC)\n\n# let's convert Household.Size to a factor\ndata1 &lt;- data1 |&gt; mutate(Household.Size = as.factor(round(Household.Size)))\ndata1 &lt;- data1 |&gt; mutate(Share.of.Household.Expenses = Share.of.Household.Expenses / 100)\ndata1 &lt;- data1 |&gt; left_join(read.csv(\"data/job-categories.csv\"), by = \"Job.Title\")\n\nmodel &lt;- lm(Yearly.Savings ~ 0 + Country + Household.Size + Job.Category + Seniority + Share.of.Household.Expenses, data = data1)\n\nmodel_coef &lt;- summary(model)$coefficients |&gt;\n  data.frame() |&gt;\n  rownames_to_column(\"value\") |&gt;\n  mutate(\n    effect = Estimate,\n    error = `Std..Error`,\n    p.value = `Pr...t..`,\n    significant = p.value &lt; 0.05\n  )\n\nmodel_coef &lt;- model_coef |&gt;\n  mutate(\n    variable = case_when(\n      str_detect(value, \"Country\") ~ \"Country\",\n      str_detect(value, \"Household.Size\") ~ \"Household Size\",\n      str_detect(value, \"Lifestyle\") ~ \"Lifestyle\"\n    )\n  )\n\nmodel_coef &lt;- model_coef |&gt; mutate(value = sub(\"Country\", \"\", value))\nmodel_coef &lt;- model_coef |&gt; mutate(value = sub(\"Household.Size\", \"\", value))\nmodel_coef &lt;- model_coef |&gt; mutate(value = sub(\"Lifestyle\", \"\", value))\n\ncountry_coef &lt;- model_coef |&gt;\n  filter(variable == \"Country\") |&gt;\n  arrange(desc(effect))\ncountry_coef &lt;- country_coef |&gt; mutate(rownumber = 1:nrow(country_coef))\n\nxlim &lt;- c(\n  min(country_coef$effect - country_coef$error),\n  max(country_coef$effect + 1.2 * country_coef$error)\n)\nxscale &lt;- c(0, 25000, 50000, 75000)\n\n\nThis model performs on a mediocre level, with an adjusted R-squared value of 0.56. The coefficients for the country variable represent the expected yearly savings for a single person.\n\nShow the code\ncountry_coef[1:17, ] |&gt;\n  ggplot(aes(x = effect, y = reorder(value, effect), colour = significant)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = effect - error, xmax = effect + error),\n    height = .2\n  ) +\n  geom_text(aes(label = paste(\"p =\", format(p.value, digits = 2))),\n    vjust = 1.5,\n    hjust = -0.2,\n    colour = \"grey55\",\n    fill = \"white\",\n    size = 2.5\n  ) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", colour = \"grey55\") +\n  theme_minimal() +\n  coord_cartesian(xlim = xlim) +\n  scale_x_discrete(limits = xscale, labels = scales::comma) +\n  theme(legend.position = \"none\") +\n  labs(x = \"\", y = \"\")\ncountry_coef[18:34, ] |&gt;\n  ggplot(aes(x = effect, y = reorder(value, effect), colour = significant)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = effect - error, xmax = effect + error),\n    height = .2\n  ) +\n  geom_text(aes(label = paste(\"p =\", format(p.value, digits = 2))),\n    vjust = 1.5,\n    hjust = -0.2,\n    colour = \"grey55\",\n    fill = \"white\",\n    size = 2.5\n  ) +\n  geom_vline(\n    xintercept = 0,\n    linetype = \"dashed\",\n    colour = \"grey55\"\n  ) +\n  theme_minimal() +\n  coord_cartesian(xlim = xlim) +\n  scale_x_discrete(limits = xscale, labels = scales::comma) +\n  theme(legend.position = \"none\") +\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\n\n\n\nFigure 10: Effect of country on yearly savings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOnly a few countries show a statistically significant impact on yearly savings. While differences in salary and cost of living across countries and roles are concrete factors, individual spending habits and lifestyle choices play an even more significant role in determining yearly savings."
  },
  {
    "objectID": "posts/20240927-euro-tech-money/index.html#conclusion",
    "href": "posts/20240927-euro-tech-money/index.html#conclusion",
    "title": "European Tech Salaries",
    "section": "Conclusion",
    "text": "Conclusion\nThis analysis offers several key insights. While the following conclusions are statistically significant at the 0.05 level, they may not apply to every individual case.\n\nThe top three countries for gross salary are Belgium, Switzerland, and the United Kingdom. These same countries also lead in net salary.\nBelgium has the highest tax burden, followed by Germany and the UK.\nSwitzerland has the highest cost of living, with Belgium and the UK also ranking high. Hungary, Spain, and Italy are among the least expensive countries.\nThe highest yearly savings in the sample are found in Georgia and Romania, followed by the UK, Switzerland, and Denmark."
  },
  {
    "objectID": "posts/20241105-airflow-filesensor/index.html",
    "href": "posts/20241105-airflow-filesensor/index.html",
    "title": "Using Airflow FileSensor for Triggering ETL Process",
    "section": "",
    "text": "In this article, we will discuss how to use the Airflow FileSensor to trigger an ETL process. We will walk through the process of setting up a FileSensor in Airflow and using it to monitor a directory for new files. Once a new file is detected, the ETL process will be triggered automatically. This can be a useful technique for automating data processing tasks that rely on the availability of new files."
  },
  {
    "objectID": "posts/20241105-airflow-filesensor/index.html#abstract",
    "href": "posts/20241105-airflow-filesensor/index.html#abstract",
    "title": "Using Airflow FileSensor for Triggering ETL Process",
    "section": "",
    "text": "In this article, we will discuss how to use the Airflow FileSensor to trigger an ETL process. We will walk through the process of setting up a FileSensor in Airflow and using it to monitor a directory for new files. Once a new file is detected, the ETL process will be triggered automatically. This can be a useful technique for automating data processing tasks that rely on the availability of new files."
  },
  {
    "objectID": "posts/20241105-airflow-filesensor/index.html#introduction",
    "href": "posts/20241105-airflow-filesensor/index.html#introduction",
    "title": "Using Airflow FileSensor for Triggering ETL Process",
    "section": "Introduction",
    "text": "Introduction\nAirflow is a popular open-source platform for orchestrating complex data workflows. It allows users to define, schedule, and monitor workflows as directed acyclic graphs (DAGs). Airflow provides a wide range of operators that can be used to perform various tasks, such as executing SQL queries, transferring files, and sending emails.\n\n\n\nApache Airflow Logo\n\n\nOne common use case for Airflow is to automate ETL (Extract, Transform, Load) processes. ETL processes involve extracting data from various sources, transforming it into a usable format, and loading it into a data warehouse or other storage system. Airflow provides operators that can be used to perform each step of the ETL process, making it easy to build and schedule complex data pipelines.\nIn this article, we will focus on the Extract step of the ETL process and discuss how to use the Airflow FileSensor to trigger an ETL process when new files become available."
  },
  {
    "objectID": "posts/20241105-airflow-filesensor/index.html#example-use-case",
    "href": "posts/20241105-airflow-filesensor/index.html#example-use-case",
    "title": "Using Airflow FileSensor for Triggering ETL Process",
    "section": "Example Use Case",
    "text": "Example Use Case\nSuppose we have a source system that generates CSV files on a scheduled basis. We want to build an ETL process that reads these CSV files, transforms the data, and loads it into a database. To automate this process, we can use the Airflow FileSensor to monitor a directory for changes and trigger the ETL process.\nTo signal the availability of new files, we can program source system to add a new file to the directory after new CSV files are created. The FileSensor will detect the presence of the new file and trigger the ETL process automatically."
  },
  {
    "objectID": "posts/20241105-airflow-filesensor/index.html#setting-up-the-dag",
    "href": "posts/20241105-airflow-filesensor/index.html#setting-up-the-dag",
    "title": "Using Airflow FileSensor for Triggering ETL Process",
    "section": "Setting up the DAG",
    "text": "Setting up the DAG\nTrigger DAG can be set by creating a new Python file in the dags directory of your Airflow installation. For example, you can create a new file called reports_trigger.py with the following content:\nimport pendulum\n\nfrom Airflow.models.dag import DAG\nfrom Airflow.operators.bash import BashOperator\n\nfrom Airflow.sensors.filesystem import FileSensor\nfrom Airflow.operators.trigger_dagrun import TriggerDagRunOperator\n\n# Where to look for the file\nFILEPATH = \"/mnt/Reports/READY\"\n# The DAG to trigger\nDAG_ID = \"reports_uploader\"\n# Central European Time\nTZ = \"CET\"\n\nwith DAG(\n    dag_id=\"reports_trigger\",\n    description=\"Monitors the appearance of a file and starts a DAG\",\n    # Every 2 hours\n    schedule=\"5 */2 * * *\",\n    start_date=pendulum.datetime(2024, 11, 5, 0, 0, 0, tz=TZ),\n    # Don't run backfill\n    catchup=False,\n    tags=[\"trigger\"],\n    # Only one run at a time\n    max_active_runs=1,\n) as dag:\n\n    check_flag = FileSensor(\n        task_id=\"check_flag\",\n        filepath=FILEPATH,\n    )\n\n    remove_flag = BashOperator(\n        task_id=\"remove_flag\",\n        bash_command=f\"rm -f {FILEPATH}\",\n    )\n\n    trigger_dag = TriggerDagRunOperator(\n        task_id=\"trigger_dag\",\n        trigger_dag_id=DAG_ID,\n        logical_date=pendulum.now().add(seconds=5),\n    )\n    \n    # Set the order of the tasks\n    check_flag &gt;&gt; remove_flag &gt;&gt; trigger_dag\nIn this example, we define a new DAG called reports_trigger that monitors the appearance of a file READY in the /mnt/Reports directory. When a new file is detected, the next task remove_flag deletes it, and another DAG called reports_uploader is triggered for execution.\n\n\n\nDAG in Graph View"
  },
  {
    "objectID": "posts/20241105-airflow-filesensor/index.html#setting-up-the-source-system",
    "href": "posts/20241105-airflow-filesensor/index.html#setting-up-the-source-system",
    "title": "Using Airflow FileSensor for Triggering ETL Process",
    "section": "Setting up the source system",
    "text": "Setting up the source system\nThe source system should be programmed to create a new file named READY in the /mnt/Reports directory after new CSV files are created. There is a variety of ways to achieve this, depending on the source system and the tools available.\nIf the source system is a Windows machine, you can create a batch file that creates the READY file and schedule it to run after the CSV files are generated. For example, you can create a batch file called create_flag.cmd with the following content:\necho &gt; C:\\Users\\admin\\Reports\\%1\nThe argument would be the name of the file to create, READY in this case. If you use the Windows Task Scheduler, add new action to run the batch file with the argument READY.\nIf the source system is a Linux machine, you can create file using the touch command:\ntouch /mnt/Reports/READY"
  },
  {
    "objectID": "posts/20241105-airflow-filesensor/index.html#setting-up-the-etl-process",
    "href": "posts/20241105-airflow-filesensor/index.html#setting-up-the-etl-process",
    "title": "Using Airflow FileSensor for Triggering ETL Process",
    "section": "Setting up the ETL Process",
    "text": "Setting up the ETL Process\nThe ETL process is defined within a distinct DAG named reports_uploader, located in a separate Python file in the dags directory of the Airflow installation."
  },
  {
    "objectID": "posts/20241105-airflow-filesensor/index.html#conclusion",
    "href": "posts/20241105-airflow-filesensor/index.html#conclusion",
    "title": "Using Airflow FileSensor for Triggering ETL Process",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, we discussed how to use the Airflow FileSensor to trigger an ETL process when new files become available and how to set up a DAG to monitor a directory for changes. We also discussed how to program the source system to create a new file to signal the availability of new data.\nI hope this article has been helpful in understanding how to use the Airflow FileSensor for triggering ETL processes. If you have any further questions or comments, please feel free to leave them int the comments section."
  },
  {
    "objectID": "posts/20240821-sophisthse/index.html",
    "href": "posts/20240821-sophisthse/index.html",
    "title": "Python Library for Russian Macroeconomics Data",
    "section": "",
    "text": "The article introduces a Python library called sophisthse that provides access to Russian macroeconomic statistics time series published by the National Research University Higher School of Economics (HSE). The library aims to simplify the process of working with Russian macroeconomic data and facilitate the analysis of economic trends in the country. The article demonstrates how to use the library to list available time series, download data, and visualize the time series data."
  },
  {
    "objectID": "posts/20240821-sophisthse/index.html#acknowledgments",
    "href": "posts/20240821-sophisthse/index.html#acknowledgments",
    "title": "Python Library for Russian Macroeconomics Data",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThe library is a port of the sophisthse R package developed by Boris Demeshev. The original package provides similar functionality for working with Russian macroeconomic data in R. The Python version of the library is designed to offer the same features to Python users interested in analyzing Russian economic indicators."
  },
  {
    "objectID": "posts/20240821-sophisthse/index.html#source-data",
    "href": "posts/20240821-sophisthse/index.html#source-data",
    "title": "Python Library for Russian Macroeconomics Data",
    "section": "Source Data",
    "text": "Source Data\nThe sophist.hse.ru website contains wide range of Russian macroeconomic statistics time series, including data on GDP, inflation, unemployment, industrial production, retail sales, and more. The data is originally sourced from the Federal State Statistics Service of the Russian Federation (Rosstat) and other official sources."
  },
  {
    "objectID": "posts/20240821-sophisthse/index.html#features",
    "href": "posts/20240821-sophisthse/index.html#features",
    "title": "Python Library for Russian Macroeconomics Data",
    "section": "Features",
    "text": "Features\nThe sophisthse library provides the following features:\n\nListing available tables with time series data\nDownloading and caching time series data"
  },
  {
    "objectID": "posts/20240821-sophisthse/index.html#installation",
    "href": "posts/20240821-sophisthse/index.html#installation",
    "title": "Python Library for Russian Macroeconomics Data",
    "section": "Installation",
    "text": "Installation\nYou can install the library using pip:\npip install sophisthse"
  },
  {
    "objectID": "posts/20240821-sophisthse/index.html#usage",
    "href": "posts/20240821-sophisthse/index.html#usage",
    "title": "Python Library for Russian Macroeconomics Data",
    "section": "Usage",
    "text": "Usage\nHere is an example of how to use the library to download and plot the time series data.\n\nList Available Time Series\nTime series are published on the sophist.hse.ru website. Initializing sophisthse class will download the list of available tables. You can list the available tables using the list_tables method:\n\nfrom sophisthse import sophisthse\n\nsph = sophisthse()\ntables = sph.list_tables()\ntables\n\n\n\n\n\n\n\n\ndate\nname\n\n\n\n\n0\n2020-05-27 15:46:00\nAGR_M_I - Copy\n\n\n1\n2024-08-08 09:10:00\nAGR_M_I\n\n\n2\n2024-08-08 09:10:00\nAGR_Q_I\n\n\n3\n2024-02-11 19:10:00\nAGR_Y_DIRI\n\n\n4\n2024-07-26 20:04:00\nAPCI3N\n\n\n...\n...\n...\n\n\n284\n2010-03-29 12:57:00\nvrp\n\n\n285\n2010-03-30 12:41:00\nvrp_r\n\n\n286\n2024-08-08 09:12:00\nWAG_M\n\n\n287\n2024-06-07 15:03:00\nWAG_Q\n\n\n288\n2024-03-13 19:36:00\nWAG_Y\n\n\n\n\n289 rows × 2 columns\n\n\n\nThe column named date contains timestamp of the latest update. The dates in this column hint that some tables may be out of date. Let’s take a look at the date of the latest update of the tables.\n\ntables[\"year\"] = tables[\"date\"].dt.year\ntables.groupby(\"year\").size().plot(kind=\"bar\")\n\n\n\n\n\n\n\n\nThe 116 out of 289 are being updated through 2024. Let’s take a look at the most recently updated tables.\n\ntables[\"date\"] = tables[\"date\"].dt.date\n(\n    tables.groupby(\"date\")\n    .agg({\"name\": lambda x: \", \".join(x)})\n    .reset_index()\n    .sort_values(\"date\", ascending=False)\n    .head()\n)\n\n\n\n\n\n\n\n\ndate\nname\n\n\n\n\n38\n2024-08-08\nAGR_M_I, AGR_Q_I, BBR_EA2_M_I, BBR_EA2_Q_I, CN...\n\n\n37\n2024-07-26\nAPCI3N, BMPM3N, CCPM3N, CPPNF3N, ECOG3N, EMAM3...\n\n\n36\n2024-06-07\nWAG_Q\n\n\n35\n2024-06-06\nGOV_Q, M_Q\n\n\n34\n2024-04-05\nINVFC_Q, INVFC_Q_I\n\n\n\n\n\n\n\n\n\nDownload and Plot Time Series\nAccording to the sophist.hse.ru website table AGR_M_I provides the index of real agricultural production in column AGR_M_DIRI, and seasonally adjusted one in the AGR_M_DIRI_SA. Let’s download it and take a look at the data.\n\nagr_m_i = sph.get_table(\"AGR_M_I\")\nagr_m_i.tail()\n\n\n\n\n\n\n\n\nAGR_M_DIRI\nAGR_M_DIRI_SA\n\n\nT\n\n\n\n\n\n\n2024-02\n110.0\n353.3\n\n\n2024-03\n169.6\n355.7\n\n\n2024-04\n190.3\n357.2\n\n\n2024-05\n208.4\n358.0\n\n\n2024-06\n213.2\n358.5\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAfter calling get_table, the data is cached locally. The next time get_table is called, the data will be loaded from the cache. However, if the table is updated on the server, which is checked when the sophisthse class is initialized, the new data will be loaded from the website.\n\n\nThe method returns a pandas DataFrame, so we can plot the data using the plot method.\n\nagr_m_i.plot()\n\n\n\n\n\n\n\n\n\n\nAdditional Example\nAccording to the sophist.hse.ru website table FINMAR_M contains data on the financial market.\n\nfinmar = sph.get_table(\"FINMAR_M\")\nfinmar.tail()\n\n\n\n\n\n\n\n\nRDEXRO_M\nRDEXRM_M\nRTS_M\nIB_M\nGKO_M\nDEP_M\nCR_M\n\n\nT\n\n\n\n\n\n\n\n\n\n\n\n2024-03\n92.37\n92.35\n1131.21\nNaN\nNaN\nNaN\nNaN\n\n\n2024-04\n91.78\n93.05\n1165.23\nNaN\nNaN\nNaN\nNaN\n\n\n2024-05\n89.79\n90.05\n1185.57\nNaN\nNaN\nNaN\nNaN\n\n\n2024-06\n85.75\n85.75\n1134.30\nNaN\nNaN\nNaN\nNaN\n\n\n2024-07\n86.33\n86.11\n1092.61\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nThe first columns are:\n\nRDEXRO_M - Official exchange rate of the ruble against the US dollar.\nRDEXRM_M - Exchange rate of the US dollar on the Moscow Exchange.\nRTS_M - Average RTS index.\n\nThe last four columns show NaNs for the most recent periods. Let’s take a look at the missing values.\n\nsns.heatmap(~finmar.isnull(), cbar=False, cmap=\"Blues\")\n\n\n\n\n\n\n\n\nThe heatmap shows that the data in the IB_M, GKO_M, DEP_M, and CR_M columns is not available for the recent years, while the RDEXRO_M, RDEXRM_M, and RTS_M columns have complete data.\nLet’s plot the RTS_M column. Due to the economic fluctuations, the RTS index has a lot of volatility. We will add the moving average to the plot.\n\nfinmar[\"RTS_M\"].plot(label=\"RTS index\")\nfinmar[\"RTS_M\"].rolling(12, center=True).mean().plot(label=\"12-months MA\")\nplt.legend()"
  },
  {
    "objectID": "posts/20240821-sophisthse/index.html#further-investigation",
    "href": "posts/20240821-sophisthse/index.html#further-investigation",
    "title": "Python Library for Russian Macroeconomics Data",
    "section": "Further Investigation",
    "text": "Further Investigation\nLooking at the tables, one can assume that the suffixes _M, _Q and _Y in the names mean that the data was sampled monthly, quarterly and annually, respectively. Let’s check this assumption.\n\ndef guess_period(name: str):\n    if \"_M\" in name:\n        return \"month\"\n    if \"_Q\" in name:\n        return \"quarter\"\n    if \"_Y\" in name:\n        return \"year\"\n    return \"unknown\"\n\n\ntables[\"period\"] = tables[\"name\"].apply(guess_period)\n\ntables.groupby(\"period\").size().plot(kind=\"bar\")\n\n\n\n\n\n\n\n\nThe bar plot shows that the assumption is partially correct. There are tables with unknown sampling frequency. This information can be drawn from the actual series after downloading data. It’s not necessary for demonstrative purposes, so we will skip this step.\nLet’s calculate the correlation matrix for the monthly data.\n\nfrom time import sleep\nfrom random import randint\nimport pandas as pd\n\n\nmonthly_tables = tables.query(\"period == 'month' & year &gt;= 2023\")\n\ndf = pd.DataFrame(index=pd.period_range(start=\"2014-01-01\", end=\"2025-01-01\", freq=\"M\"))\nfor table in monthly_tables[\"name\"]:\n    df_add = sph.get_table(table)\n    # We don't want to overload the server\n    # sleep(randint(1, 3))\n    df = df.merge(df_add, left_index=True, right_index=True)\n\n# We do not want to deal with NaNs, at least at this time :)\ndf = df.dropna(how=\"any\", axis=1)\n\n# Keep only columns with \"_M\" suffix to skip derived columns\ncolumns = [col for col in df.columns if col.endswith(\"_M\")]\n\ncorr = df[columns].corr()\n\nsns.heatmap(\n    corr,\n    vmin=-1.0,\n    vmax=1.0,\n    xticklabels=corr.columns.values,  # type: ignore\n    yticklabels=corr.columns.values,  # type: ignore\n    cmap=\"coolwarm\",\n)\n\n\n\n\n\n\n\n\nThe last column (or row) is the WAG_C_M. This is a monthly series of the average nominal wage. It correlates with many economics indicators, and it worth noting that CBEX_M (Consolidated Budget Expenditures) explains almost 84% of WAG_C_M variability. Let’s take a look at these two series.\n\ndf_norm = df[columns].apply(lambda x: (x - x.mean()) / x.std())\ndf_norm[[\"WAG_C_M\", \"CBEX_M\"]].plot()\n\n\n\n\n\n\n\n\nThe plot shows that the WAG_C_M series follows the CBEX_M series with great accuracy. This is an interesting observation that can be used in further analysis."
  },
  {
    "objectID": "posts/20240821-sophisthse/index.html#conclusion",
    "href": "posts/20240821-sophisthse/index.html#conclusion",
    "title": "Python Library for Russian Macroeconomics Data",
    "section": "Conclusion",
    "text": "Conclusion\nThe sophisthse library provides a convenient way to access and work with Russian macroeconomic time series data. The library simplifies the process of downloading and caching data, making it easier to analyze economic trends in Russia. The library is a valuable resource for researchers, analysts, and anyone interested in studying the Russian economy."
  },
  {
    "objectID": "posts/20240721-welcome/index.html",
    "href": "posts/20240721-welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in my blog. Welcome!"
  },
  {
    "objectID": "posts/20250704-animation/index.html",
    "href": "posts/20250704-animation/index.html",
    "title": "Animation of Spatial Data",
    "section": "",
    "text": "This is an example of how to create an animated visualization of spatial data using R. The data is sourced from the German Weather Service (Deutscher Wetterdienst, DWD) and includes cloud coverage and density observations from various weather stations across Germany."
  },
  {
    "objectID": "posts/20250704-animation/index.html#load-the-stations-data",
    "href": "posts/20250704-animation/index.html#load-the-stations-data",
    "title": "Animation of Spatial Data",
    "section": "Load the stations data",
    "text": "Load the stations data\nHere we will download the stations data from the DWD website. The data contains information about weather stations, including their IDs, names, locations, and the time period they were active.\n\nurl &lt;- \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/subdaily/cloudiness/historical/\"\nstations_file &lt;- \"N_Terminwerte_Beschreibung_Stationen.txt\"\nif (!file.exists(\"data\")) {\n  dir.create(\"data\")\n}\nif (!file.exists(paste0(\"data/\", stations_file))) {\n  download.file(paste0(url, stations_file), here(\n    \"data\", stations_file\n  ), mode = \"wb\")\n}\n\nLet’s read the stations data.\n\ncol_names &lt;- c(\n  \"STATIONS_ID\", \"von_datum\", \"bis_datum\", \"Stationshoehe\", \"lat\",\n  \"lon\", \"Stationsname\", \"Bundesland\", \"Abgabe\"\n)\n\nstations &lt;- read.fwf(\n  here(\n    \"posts\", \"20250704-animation\", \"data\",\n    \"N_Terminwerte_Beschreibung_Stationen.txt\"\n  ),\n  widths = c(6, 9, 9, 15, 12, 10, 41, 41, 5), skip = 2,\n  fileEncoding = \"Windows-1252\", col.names = col_names\n) |&gt; as.data.table()\n\n\nstations[, von_datum := as.Date(str_trim(von_datum), format = \"%Y%m%d\")]\nstations[, bis_datum := as.Date(str_trim(bis_datum), format = \"%Y%m%d\")]\nstations[, lon := as.numeric(lon)]\nstations[, lat := as.numeric(lat)]"
  },
  {
    "objectID": "posts/20250704-animation/index.html#read-the-links-to-the-data-files",
    "href": "posts/20250704-animation/index.html#read-the-links-to-the-data-files",
    "title": "Animation of Spatial Data",
    "section": "Read the links to the data files",
    "text": "Read the links to the data files\nWe will read the HTML content of the DWD website to extract the links to the cloudiness data files. The links will be filtered to include only those that contain the term “terminwerte”.\n\npage_content &lt;- read_html(url)\n\nlinks &lt;- page_content |&gt;\n  html_nodes(\"a\") |&gt;\n  html_attr(\"href\")\n\nlinks &lt;- links[!is.na(links) & str_detect(links, \"terminwerte\")]\nlinks &lt;- links |&gt; data.table()\nlinks |&gt; head()\n\n                                            links\n                                           &lt;char&gt;\n1: terminwerte_N_00001_19370101_19860630_hist.zip\n2: terminwerte_N_00003_18910101_20110331_hist.zip\n3: terminwerte_N_00044_19710301_20111231_hist.zip\n4: terminwerte_N_00052_19730101_20011231_hist.zip\n5: terminwerte_N_00061_19750701_19780831_hist.zip\n6: terminwerte_N_00070_19730601_19860930_hist.zip\n\n\nExtract the station IDs from the links. The station IDs are 5-digit numbers that are part of the file names.\n\nlinks[, STATIONS_ID := str_extract(links, \"[0-9]{5}\")]\nlinks[, STATIONS_ID := as.integer(STATIONS_ID)]"
  },
  {
    "objectID": "posts/20250704-animation/index.html#filter-the-stations-data",
    "href": "posts/20250704-animation/index.html#filter-the-stations-data",
    "title": "Animation of Spatial Data",
    "section": "Filter the stations data",
    "text": "Filter the stations data\nWe will download only those stations data that were active during the period from 2015 to 2025.\n\nstations &lt;- stations[von_datum &lt;= \"2023-12-01\" & bis_datum &gt;= \"2025-01-01\"]\n\nlinks &lt;- links[stations, on = \"STATIONS_ID\"]"
  },
  {
    "objectID": "posts/20250704-animation/index.html#download-and-process-the-data-files",
    "href": "posts/20250704-animation/index.html#download-and-process-the-data-files",
    "title": "Animation of Spatial Data",
    "section": "Download and process the data files",
    "text": "Download and process the data files\nIn this section, we will download the data files from the DWD website and process them to extract the cloud coverage and density observations. The data will be stored in a DuckDB database which is useful if we need to reuse the data later without downloading and parsing it again.\n\n# check if the files exist\nfiles &lt;- list.files(\"data\", full.names = TRUE)\nfiles &lt;- files[str_detect(files, \"produkt_n_termin\")]\n\nif (length(files) == 0) {\n  for (link in links$links) {\n    download.file(paste0(url, link), here(\n      \"posts\", \"20250704-animation\", \"data\", link\n    ), mode = \"wb\")\n    unzip(paste0(\"data/\", link), exdir = \"data\")\n    unlink(\"data/Metadaten*\")\n    unlink(\"data/*.html\")\n  }\n}\nunlink(\"data/*.zip\")\n\nHere is where parsing is done.\n\ncon &lt;- dbConnect(duckdb(),\n  dbdir = here(\"posts\", \"20250704-animation\", \"db\", \"weather.duckdb\")\n)\ntables &lt;- dbGetQuery(con, \"SHOW ALL TABLES;\")[\"name\"]\n\nif (!(\"cloudiness\" %in% tables)) {\n  files &lt;- list.files(\"data\", full.names = TRUE)\n  files &lt;- files[str_detect(files, \"produkt_n_termin\")]\n\n  start_date &lt;- \"2023-12-01\"\n\n  observations &lt;- data.table()\n\n  for (file in files) {\n    temp_data &lt;- read.csv(file, sep = \";\") |&gt; as.data.table()\n    temp_data[, MESS_DATUM := as.Date(str_trim(MESS_DATUM), format = \"%Y%m%d\")]\n    temp_data &lt;- temp_data[MESS_DATUM &gt;= start_date]\n    observations &lt;- rbind(\n      observations,\n      temp_data\n    )\n  }\n\n  observations &lt;- observations[N_TER != -999]\n  observations[, CD_TER := ifelse(CD_TER == -999, NA, CD_TER)]\n\n  if (!file.exists(\"db\")) {\n    dir.create(\"db\")\n  }\n\n  con &lt;- dbConnect(duckdb(),\n    dbdir = here(\n      \"posts\", \"20250704-animation\", \"db\", \"weather.duckdb\"\n    )\n  )\n  dbWriteTable(con, \"cloudiness\", observations, overwrite = TRUE)\n}\n\nobservations &lt;- dbGetQuery(con, \"SELECT * FROM cloudiness\") |&gt; as.data.table()\n\ndbDisconnect(con)\n\nobservations |&gt; glimpse()\n\nRows: 228,731\nColumns: 6\n$ STATIONS_ID &lt;int&gt; 4024, 4024, 4024, 4024, 4024, 4024, 4024, 4024, 4024, 4024…\n$ MESS_DATUM  &lt;date&gt; 2023-12-01, 2023-12-01, 2023-12-01, 2023-12-02, 2023-12-0…\n$ QN_4        &lt;int&gt; 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9…\n$ N_TER       &lt;int&gt; 4, 5, 8, 8, 8, 8, 8, 7, 8, 8, 8, 8, 7, 8, 8, 8, 7, 8, 8, 8…\n$ CD_TER      &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ eor         &lt;chr&gt; \"eor\", \"eor\", \"eor\", \"eor\", \"eor\", \"eor\", \"eor\", \"eor\", \"e…\n\n\nAs the data contains multiple observations per day for each station, we will aggregate the data to get the average cloud coverage and density per day per station.\n\nobservations &lt;- observations[, .(\n  .N,\n  cloud_coverage = mean(N_TER, na.rm = TRUE),\n  cloud_density = mean(CD_TER, na.rm = TRUE)\n), by = c(\"STATIONS_ID\", \"MESS_DATUM\")]\n\nLet’s plot the cloud coverage for a specific station as a time series to visualize the data.\n\nobservations[MESS_DATUM &gt; \"2024-01-01\" & STATIONS_ID == 433] |&gt;\n  ggplot(aes(x = MESS_DATUM, y = cloud_coverage)) +\n  # geom_line() +\n  geom_line(aes(y = rollapply(cloud_coverage,\n    width = 7, FUN = mean, align = \"center\", partial = TRUE\n  )), color = \"blue\") +\n  labs(title = NULL, x = NULL, y = \"Cloud Coverage\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/20250704-animation/index.html#add-h3-addresses",
    "href": "posts/20250704-animation/index.html#add-h3-addresses",
    "title": "Animation of Spatial Data",
    "section": "Add H3 addresses",
    "text": "Add H3 addresses\nTo visualize the data on a map, we will convert the latitude and longitude coordinates of the stations into H3 addresses. H3 is a geospatial indexing system that allows us to represent geographic locations as hexagonal cells.\n\npoints &lt;- stations[, .(lon, lat)] |&gt; unique()\n\npoints[, h3_address := point_to_cell(points, res = 4)]\n\nAssuming columns 1 and 2 contain x, y coordinates in EPSG:4326\n\nstations &lt;- stations[, c(\n  \"STATIONS_ID\", \"von_datum\", \"bis_datum\", \"Stationshoehe\",\n  \"lat\", \"lon\", \"Stationsname\", \"Bundesland\", \"Abgabe\"\n)]\n\nstations[points, on = .(lon, lat), h3_address := h3_address]\n\nstations &lt;- stations[, geometry := cell_to_polygon(h3_address, simple = F)[2]]\n\nLoad the boundaries of Germany to use as a background for the map.\n\nboundaries &lt;- geoboundaries(\"Germany\", release_type = \"gbOpen\", adm_lvl = \"adm1\")\n\nJoin the stations data with the observations data to have the geometry of the stations in the observations data. Calculate mean for each H3 address and drop duplicates.\n\nobservations &lt;- observations[stations, on = \"STATIONS_ID\"]\nobservations[, coud_coverage := mean(cloud_coverage, rm = T), by = h3_address]\n\nWarning in `[.data.table`(observations, , `:=`(coud_coverage,\nmean(cloud_coverage, : Unable to optimize call to mean() and could be very\nslow. You must name 'na.rm' like that otherwise if you do mean(x,TRUE) the TRUE\nis taken to mean 'trim' which is the 2nd argument of mean. 'trim' is not yet\noptimized.\n\nobservations &lt;- observations |&gt; unique(by = c(\"h3_address\", \"MESS_DATUM\"))"
  },
  {
    "objectID": "posts/20250704-animation/index.html#create-animations-of-cloud-coverage-in-germany",
    "href": "posts/20250704-animation/index.html#create-animations-of-cloud-coverage-in-germany",
    "title": "Animation of Spatial Data",
    "section": "Create animations of cloud coverage in Germany",
    "text": "Create animations of cloud coverage in Germany\nIn the following section, we will create animations of cloud coverage in Germany using the observations data in 2024. This code creates a series of maps showing the average cloud coverage for each day in 2024, with a rolling average of 7 days to smooth the data.\n\nmin_date &lt;- observations[, MESS_DATUM] |&gt; min(na.rm = TRUE)\nmax_date &lt;- observations[, MESS_DATUM] |&gt; max(na.rm = TRUE)\n\nmin_date &lt;- max(c(min_date, as.Date(\"2024-01-01\")))\nmax_date &lt;- min(c(max_date, as.Date(\"2024-12-31\")))\n\nmax_coverage &lt;- observations[(MESS_DATUM &gt;= min_date) &\n  (MESS_DATUM &lt;= max_date), cloud_coverage] |&gt; max(na.rm = TRUE)\nmin_coverage &lt;- observations[(MESS_DATUM &gt;= min_date) &\n  (MESS_DATUM &lt;= max_date), cloud_coverage] |&gt; min(na.rm = TRUE)\n\ntable_dates &lt;- seq(min_date - 7, max_date + 7, by = 1) |&gt;\n  as.data.table() |&gt;\n  rename(MESS_DATUM = V1)\nobservations &lt;- table_dates[observations, on = \"MESS_DATUM\"]\n\nobservations[, cloud_coverage_r7 := rollapply(cloud_coverage,\n  width = 7, FUN = mean, align = \"center\", partial = TRUE\n),\nby = STATIONS_ID\n]\n\ndates &lt;- seq(min_date, max_date, by = 1)\n\nif (!file.exists(\"figures\")) {\n  dir.create(\"figures\")\n}\n\nfor (d in as.character(dates)) {\n  p &lt;- ggplot(aes(fill = cloud_coverage_r7),\n    data = observations[MESS_DATUM == d] |&gt; as.data.frame()\n  ) +\n    geom_sf(data = boundaries, fill = \"gray78\", color = \"gray54\") +\n    geom_sf(aes(geometry = geometry), color = \"gray78\") +\n    scale_fill_whitebox_c(\n      palette = \"deep\",\n      direction = 1,\n      limits = c(min_coverage, max_coverage)\n    ) +\n    coord_sf(default_crs = sf::st_crs(4326)) +\n    theme_void() +\n    theme(\n      legend.position = \"bottom\",\n      legend.key.height = unit(4, \"pt\"),\n      legend.key.width = unit(20, \"pt\"),\n      legend.title.position = \"top\",\n      panel.grid.minor = element_blank(),\n      panel.grid.major = element_blank(),\n      plot.background = element_rect(fill = NA, color = NA),\n      title = element_text(size = 8, color = \"gray35\")\n    ) +\n    labs(title = d, fill = \"Cloud Coverage\")\n\n  ggsave(\n    here(\n      \"posts\", \"20250704-animation\", \"figures\",\n      paste0(\"cloudiness-\", d, \".png\")\n    ),\n    plot = p,\n    units = \"px\",\n    width = 1200,\n    height = 1200,\n    dpi = 300\n  )\n}"
  },
  {
    "objectID": "posts/20250704-animation/index.html#create-a-gif-animation",
    "href": "posts/20250704-animation/index.html#create-a-gif-animation",
    "title": "Animation of Spatial Data",
    "section": "Create a GIF animation",
    "text": "Create a GIF animation\nFinally, we will create a GIF animation from the generated PNG files. The GIF will show the cloud coverage in Germany over the course of 2024, with each frame representing a day.\n\npng_files &lt;- list.files(\n  here(\"posts\", \"20250704-animation\", \"figures\"),\n  full.names = TRUE, pattern = \"cloudiness.+\\\\.png\"\n) |&gt;\n  sort() |&gt;\n  as.character()\n\ngif_file &lt;- here(\n  \"posts\", \"20250704-animation\", \"animation-cloudiness.gif\"\n)\n\ngifski(png_files, gif_file, width = 1200, height = 1200, delay = 0.1, loop = TRUE)\n\n[1] \"/mnt/Projects/Blog/posts/20250704-animation/animation-cloudiness.gif\"\n\nunlink(here(\"posts\", \"20250704-animation\", \"figures\", \"cloudiness*\"))\n\n\nThe resulting GIF animation shows the cloud coverage in Germany for each day in 2024, with a rolling average of 7 days to smooth the data. The animation provides a clear visual representation of how cloud coverage changed over time across different regions in Germany."
  },
  {
    "objectID": "posts/20241218-cv-week/index.html",
    "href": "posts/20241218-cv-week/index.html",
    "title": "CV Week 2024",
    "section": "",
    "text": "In November 2024, the Yandex School of Data Analysis (YSDA) and Yandex conducted an open online intensive course on computer vision, focusing on generative diffusion models that underpin many visual services."
  },
  {
    "objectID": "posts/20241218-cv-week/index.html#introduction",
    "href": "posts/20241218-cv-week/index.html#introduction",
    "title": "CV Week 2024",
    "section": "",
    "text": "In November 2024, the Yandex School of Data Analysis (YSDA) and Yandex conducted an open online intensive course on computer vision, focusing on generative diffusion models that underpin many visual services."
  },
  {
    "objectID": "posts/20241218-cv-week/index.html#format-and-structure",
    "href": "posts/20241218-cv-week/index.html#format-and-structure",
    "title": "CV Week 2024",
    "section": "Format and Structure",
    "text": "Format and Structure\nDates: November 25–29, 2024.\nFormat: All lectures and seminars were pre-recorded and broadcast on YouTube. During the broadcasts, participants could ask questions in the comments and interact with instructors and fellow participants via a dedicated Telegram channel.\nParticipation and Certification: The broadcasts were open to all without selection. However, to receive a certificate, participants needed to complete Qualifying assignments on the Yandex Contest platform and submit a final project."
  },
  {
    "objectID": "posts/20241218-cv-week/index.html#lectures-and-seminars",
    "href": "posts/20241218-cv-week/index.html#lectures-and-seminars",
    "title": "CV Week 2024",
    "section": "Lectures and Seminars",
    "text": "Lectures and Seminars\nThe intensive course’s schedule consisted of lectures and seminars. Five days from Monday to Friday, in the evenings, there were a lecture for approximately an hour and then a seminar where the instructor provided with the implementation of the different aspects of image generation approaches.\nYouTube Playlist\nNovember 25\n\nLecture 1: Introduction to Diffusion Models (Lecturer: Dmitry Baranchuk, Researcher, Yandex Research).\nSeminar 1: Basic Implementation of Diffusion Models (Instructor: Sergey Kastrulin, Researcher, Yandex Research).\n\nNovember 26\n\nLecture 2: Formulating Diffusion Models through Stochastic and Ordinary Differential Equations (Lecturer: Mikhail Romanov, Developer, Computer Vision Service).\nSeminar 2: Implementing an Efficient Sampler (Instructor: Mikhail Romanov).\n\nNovember 27\n\nLecture 3: Architectures of Diffusion Models, Training and Sampling Methods, and Text-to-Image Generation (Lecturer: Denis Kuznedelev, Researcher, Yandex Research).\nSeminar 3: Generating Images from Text Descriptions (Instructor: Denis Kuznedelev).\n\nNovember 28\n\nLecture 4: Distillation of Diffusion Models Using ODE-Based Methods (Lecturer: Nikita Starodubtsev, Researcher, Yandex Research ML Residency).\nLecture 5: Distillation of Diffusion Models Without ODEs (Lecturer: Dmitry Baranchuk).\nSeminar 4: Implementing Consistent Models for Text-to-Image Generation (Instructor: Nikita Starodubtsev).\n\nNovember 29\n\nLecture 6: Fine-Tuning Diffusion Models Using Reinforcement Learning Methods (Lecturer: Alexander Shishenya, Developer, Computer Vision Service).\nLecture 7: YandexART — Industrial Diffusion Model (Lecturer: Sergey Kastrulin)."
  },
  {
    "objectID": "posts/20241218-cv-week/index.html#qualifying-assignment",
    "href": "posts/20241218-cv-week/index.html#qualifying-assignment",
    "title": "CV Week 2024",
    "section": "Qualifying Assignment",
    "text": "Qualifying Assignment\nThe Qualifying Assignment was a prerequisite for accessing the final project and comprised three programming tasks designed to assess participants’ proficiency in fundamental machine learning concepts and PyTorch implementation.\n\n1. Reshape a Tensor\nThe first task required reshaping a list or tensor by swapping its first two dimensions without utilizing PyTorch or NumPy methods. For example:\n\\(\\begin{bmatrix}1 & 2 & 3\\\\4 & 5 & 6\\end{bmatrix}\\) should be transformed into \\(\\begin{bmatrix}1 & 4 \\\\ 2 & 5 \\\\ 3 & 6\\end{bmatrix}\\).\nThis task tested participants’ understanding of tensor manipulation at a fundamental level, emphasizing the importance of grasping underlying data structures without relying on high-level library functions.\n\n\n2. Encoder and Decoder Architecture\nThe second task involved implementing the encoder and decoder components of a Variational Autoencoder (VAE) architecture. VAEs are generative models that learn to represent data in a latent space, enabling the generation of new, similar data points. This task assessed participants’ ability to construct neural network architectures.\n\n\n\nVariational autoencoder architecture\n\n\n\n\n3. VAE Loss Function\nThe third task required identifying and correcting errors in the VAE loss function implementation. To validate the correctness of the function, participants trained the VAE and performed inference on the MNIST digits dataset. The VAE loss function typically comprises two components:\n\nReconstruction Loss: Measures how well the decoder reconstructs the input data.\nKullback-Leibler (KL) Divergence: Regularizes the learned latent space to align with a predefined distribution, often a standard normal distribution.\n\nAccurate implementation of this loss function is crucial for the VAE to learn meaningful latent representations and generate coherent outputs.\n\n\n\nReconstructed digits using VAE\n\n\nEach of the first two tasks was worth up to 2 points, while the third task could earn up to 4 points. A minimum of 6 points was required to pass, ensuring that participants had a solid grasp of the necessary concepts to proceed to the final project."
  },
  {
    "objectID": "posts/20241218-cv-week/index.html#final-project",
    "href": "posts/20241218-cv-week/index.html#final-project",
    "title": "CV Week 2024",
    "section": "Final Project",
    "text": "Final Project\nIn the final project, participants had to distill a multi-step diffusion model into a more efficient, few-step student model, significantly enhancing generation speed.\nThe project focused on implementing the Consistency Distillation framework, a method that accelerates diffusion models by enforcing self-consistency along learned trajectories.\nParticipants would apply this technique to distill the Stable Diffusion 1.5 (SD 1.5) model, a latent text-to-image diffusion model capable of generating photo-realistic images from textual descriptions.\nThe project comprised eight tasks, each building upon the previous, guiding participants toward developing a proficient model capable of generating images in just four steps.\n\nTeacher Model\nThe initial model for our experiments is Stable Diffusion 1.5, a pre-trained latent text-to-image diffusion model. This serves as the “teacher” model in our distillation process.\nFor the prompt: “A sad puppy with large eyes”, running the model with 50 steps and a guidance_scale of 7.5 produces the following high-quality images:\n\nHowever, when the number of steps is reduced to 4, the output becomes blurry and lacks detail:\n\nThis demonstrates the trade-off between the number of sampling steps and image quality in diffusion models. Our goal is to bridge this gap by using techniques like Consistency Distillation to achieve similar quality with fewer steps.\n\n\nConsistency Training\nThe model will be trained on a subset of the COCO Dataset comprising 5,000 images. To reduce memory consumption, we will train LoRA (Low-Rank Adaptation) adapters for the U-Net convolutional neural network instead of fine-tuning the entire model.\nThis approach significantly decreases the number of trainable parameters and activation memory, enhancing efficiency during training.\nAdditionally, implementing techniques such as gradient checkpointing can further reduce memory usage, albeit with a potential increase in training time.\nBy employing these strategies, we aim to achieve effective model performance while operating within the memory constraints of our training environment.\nThe result with guidance_scale = 2 looks sharper, but the quality is still far from desired.\n\n\n\nConsistency Distillation\nIn the Consistency Distillation phase, we enhance the model’s quality by incorporating a teacher model within the U-Net architecture, utilizing LoRA adapters trained in the previous step. This integration refines the student’s learning process, leading to significantly improved image generation results.\nConsistency Distillation is a technique that accelerates diffusion models by enforcing self-consistency along learned trajectories. By aligning the student model’s outputs with those of the teacher model, the student learns to produce high-quality images in fewer steps. Implementing this method with LoRA adapters allows for efficient training, as LoRA reduces the number of trainable parameters, thereby decreasing memory consumption and computational load.\n\n\n\nMulti-boundary Consistency Distillation\nIn the Multi-boundary Consistency Distillation phase, we draw inspiration from recent advancements in generative modeling, particularly the work by researchers from Google DeepMind. In their paper “Multistep Consistency Models” (arXiv:2403.06807v3), they propose a method that interpolates between consistency models and diffusion models, allowing for a trade-off between sampling speed and quality.\nBy employing 2 to 8 sampling steps, this approach achieves performance comparable to traditional diffusion models but with significantly reduced computational resources. This reduction in sampling steps leads to decreased memory usage and computational load, making the models more efficient without compromising output quality.\nImplementing this technique involves training the model to maintain consistency across multiple steps, effectively enabling it to generate high-quality samples in fewer iterations. This advancement is particularly beneficial for applications requiring rapid generation or operating under resource constraints.\n\n\n\nGraded Assignments\nThere were 4 automatically graded assignments and one teacher graded assignment.\n\n1. Implementation of DDIM Solver Step\nIn diffusion models, the forward diffusion process gradually transforms images into noise, following the distribution:\n\\[ q(\\mathbf{x}_t | \\mathbf{x}_0)= {N}(\\mathbf{x}_t | \\alpha_t \\mathbf{x}_0, \\sigma^2_t I)\\]\nAt time step \\(t\\), the noisy image \\(\\mathbf{x}_t\\) can be represented as: \\(\\mathbf{x}_t = \\alpha_t \\mathbf{x}_0 + \\sigma_t \\epsilon\\), where \\(\\epsilon{\\sim} {N}(0, I)\\).\nThe goal of the diffusion model is to solve the inverse problem, reconstructing an image from noise. This reverse process is formulated as an ordinary differential equation (ODE):\n\\(dx = \\left[ f(\\mathbf{x}, t) - \\frac{1}{2} \\nabla_{\\mathbf{x}_t} \\log p_t(\\mathbf{x}) \\right] dt\\), where \\(f(\\mathbf{x}, t)\\) is known from the given noise process, and \\(\\nabla_{\\mathbf{x}_t} \\log p_t(\\mathbf{x}_t)\\) (score function) is estimated using a neural network: \\(s_\\theta(\\mathbf{x}_t, t) \\approx \\nabla_{\\mathbf{x}_t} \\log p_t(\\mathbf{x}_t)\\).\nThus, having an estimate for \\(\\nabla_{\\mathbf{x}_t} \\log p_t(\\mathbf{x})\\), we can solve this ODE starting from random noise and generate a picture.\nFor this assignment, participants implemented the step from \\(\\mathbf{x}_t\\) to \\(\\mathbf{x}_s\\) using the Denoising Diffusion Implicit Models (DDIM) framework:\n\\[\\mathbf{x}_s = DDIM(\\epsilon_\\theta, \\mathbf{x}_t, t, s) = \\alpha_s \\cdot \\left(\\frac{\\mathbf{x}_t - \\sigma_t \\epsilon_\\theta}{\\alpha_t} \\right) + \\sigma_s \\epsilon_\\theta\\]\nMost of the function was pre-written by the instructors, but participants were required to correctly set \\(\\alpha_t\\) and \\(\\sigma_t\\) using the DDIMScheduler. This task tested participants’ understanding of the underlying mathematical concepts and their ability to implement them in code.\n\n\n2. Implementation of Noise Process\nThe second task involved implementing the function q_sample(x, t, scheduler, noise) to simulate the forward diffusion process. The function follows the mathematical formulation:\n\\(q(\\mathbf{x}_t | \\mathbf{x}_0)= {N}(\\mathbf{x}_t | \\alpha_t \\mathbf{x}_0, \\sigma^2_t I)\\) where the noisy image \\(x_t\\) at time \\(t\\) is calculated as:\n\\(\\mathbf{x}_t = \\alpha_t \\mathbf{x}_0 + \\sigma_t \\epsilon\\), where \\(\\epsilon{\\sim} {N}(0, I)\\).\nParticipants were required to:\n\nUse the scheduler to retrieve \\(\\alpha_t\\) and \\(\\sigma_t\\) values for the given timestep \\(t\\).\nGenerate random noise \\(\\epsilon \\sim \\mathcal{N}(0, I)\\) and apply it to the formula above to simulate the noisy image \\(\\mathbf{x}_t\\).\nHandle edge cases for boundary points, ensuring the process remains valid when \\(t=0\\).\n\nThis assignment focused on implementing the forward diffusion process accurately while accounting for the nuances of boundary conditions, which added complexity to the task.\n\n\n3. Consistency Training\nConsistency distillation leverages the teacher model to obtain the second point on the ODE trajectory, which can also be computed using the DDIM formula. The task required participants to derive the function for this computation based on the definitions of the noise process and the score function.\n\\(\\epsilon_\\theta(x_t, t) = - \\sigma_t s_\\theta(x_t, t)\\)\n\\(s_\\theta(x_t, t) \\approx \\nabla_{x_t} \\log q(x_t) = \\mathop{\\mathbb{E}}_{\\mathbf{x}\\sim p_{data}}\\left [ \\nabla_{\\mathbf{x}_t} \\log q(\\mathbf{x}_t | \\mathbf{x}) \\vert \\mathbf{x}_t \\right ] \\approx \\nabla_{\\mathbf{x}_t} \\log q(\\mathbf{x}_t \\vert \\mathbf{x})\\)\n\n\n4. Multi-boundary Timesteps\nThe objective of this task was to implement the get_multi_boundary_timesteps function, which generates boundary points for multi-step consistency training. These boundary points define the timesteps used in sampling and play a critical role in the effectiveness of the distillation process.\nThe task itself was somewhat ambiguous, leaving room for interpretation. Participants were expected to:\n\nUnderstand how multi-boundary timesteps are utilized in multi-step consistency models. Implement the function to output the appropriate boundary points.\nEnsure that the timesteps are well-distributed and align with the requirements of the consistency distillation framework.\n\n\n\n5. Generated Images\nOnce all the graded and non-graded tasks were completed, the next step was to train the Multi-boundary Consistency Model. This involved applying the techniques and frameworks developed in earlier tasks to produce high-quality generated images.\nAfter training, participants were required to upload the trained model along with generated results for assessment. This provided an opportunity to showcase the practical implementation of multi-step consistency techniques.\nThe trained model was evaluated based on its ability to generate images that were consistent and high-quality, even with a reduced number of sampling steps.\n\n\n\n\n\n\n\n\n\nConsistency Distillation\n\n\n\n\n\n\n\nMulti-boundary Consistency Distillation\n\n\n\n\n\n\nSampling Prompts\n\nA sad puppy with large eyes.\nAstronaut in a jungle, cold color palette, muted colors, detailed, 8k.\nA photo of beautiful mountain with realistic sunset and blue lake, highly detailed, masterpiece.\nA girl with pale blue hair and a cami tank top.\nA lighthouse in a giant wave, origami style.\nBelle epoque, christmas, red house in the forest, photo realistic, 8k.\nA small cactus with a happy face in the Sahara desert.\nGreen commercial building with refrigerator and refrigeration units outside."
  },
  {
    "objectID": "posts/20241218-cv-week/index.html#conclusion",
    "href": "posts/20241218-cv-week/index.html#conclusion",
    "title": "CV Week 2024",
    "section": "Conclusion",
    "text": "Conclusion\nParticipating in YSDA’s intensive training programs has been both challenging and rewarding. My first experience was during GPT Week in 2023, which did not require a qualifying assignment. The final project involved training a model to summarize articles, with flexibility regarding the architecture and implementation details.\nIn contrast, CV Week 2024 presented a more rigorous experience. Both the qualifying assignment and the final project demanded significant effort, especially given my limited background in computer vision. This intensive pushed me to expand my knowledge and skills, making the experience both demanding and educational.\nFor those interested in exploring similar topics, YSDA offers a range of online courses. These resources can provide a solid foundation for tackling advanced subjects in data analysis and machine learning.\nSource code and presentations can be found in my GitHub repository: CV_Week."
  },
  {
    "objectID": "posts/20240731-customers-graphs/index.html",
    "href": "posts/20240731-customers-graphs/index.html",
    "title": "Merging Customers Records Using Graphs in Python",
    "section": "",
    "text": "Traditional relational databases and spreadsheets fall short in capturing complex relationships among customers. Enter graph theory – a powerful framework for representing and analyzing interconnected data. By visualizing customer relationships as a graph, we can uncover hidden patterns, identify clusters, and improve data quality."
  },
  {
    "objectID": "posts/20240731-customers-graphs/index.html#introduction",
    "href": "posts/20240731-customers-graphs/index.html#introduction",
    "title": "Merging Customers Records Using Graphs in Python",
    "section": "",
    "text": "Traditional relational databases and spreadsheets fall short in capturing complex relationships among customers. Enter graph theory – a powerful framework for representing and analyzing interconnected data. By visualizing customer relationships as a graph, we can uncover hidden patterns, identify clusters, and improve data quality."
  },
  {
    "objectID": "posts/20240731-customers-graphs/index.html#motivation",
    "href": "posts/20240731-customers-graphs/index.html#motivation",
    "title": "Merging Customers Records Using Graphs in Python",
    "section": "Motivation",
    "text": "Motivation\nOver time, customers records can become fragmented and duplicated. For example, a customer may use multiple email addresses or phone numbers when interacting with a company. Creating a new record for each email or phone can lead to duplicate records for the same customer. This is especially common in B2B scenarios, where customers often have multiple representatives. Furthermore, some customers represent multiple companies, and their records may be duplicated across different companies.\nDoing any type of marketing analysis on such dataset can lead to incorrect results. We cannot be sure about the latest purchase, the total amount spent, or the number of orders. Is this customer a loyal one or not? Is that customer a new one or not? Is this customer going to leave us or they just started buying from another company? Do we need to send a discount to this customer or not? To answer these questions, we need to have customers database defragmented and deduplicated.\nMerging records manually can be time-consuming and error-prone. By using graphs, we can represent the relationships between customers, emails, and phones and find groups of connected customers. This can help us identify duplicate records and perform actions depending on our business logic."
  },
  {
    "objectID": "posts/20240731-customers-graphs/index.html#sample-data",
    "href": "posts/20240731-customers-graphs/index.html#sample-data",
    "title": "Merging Customers Records Using Graphs in Python",
    "section": "Sample Data",
    "text": "Sample Data\nWe have three datasets: customers, emails, and phones. Each customer can have multiple emails and phones. The names, emails, and phones are generated randomly and do not correspond to real people, though the structure of the data is similar to what you might find in a real-world scenario. In fact, it is the sample taken from the real data, but the names and other personal information are generated randomly to replace the actual ones.\n\ncustomers = pd.read_csv(\"data/customers.csv\")\nemails = pd.read_csv(\"data/emails.csv\")\nphones = pd.read_csv(\"data/phones.csv\")\n\nTake a look at the data.\n\ncustomers.head(3)\n\n\n\n\n\n\n\n\ncustomer_id\nname\n\n\n\n\n0\n330087\nWilliam Sparks\n\n\n1\n443237\nJoseph Williams\n\n\n2\n329867\nEddie Porter\n\n\n\n\n\n\n\n\n\nLength: 1000 Unique: 1000\n\n\n\nemails.head(3)\n\n\n\n\n\n\n\n\ncustomer_id\nemail\n\n\n\n\n0\n599100\nbrian12@example.net\n\n\n1\n330087\nemyers@example.com\n\n\n2\n25494\ncindymurphy@example.net\n\n\n\n\n\n\n\n\n\nLength: 957 Unique: 626 Duplicated: 331 Empty: 0\n\n\n\nphones.head(3)\n\n\n\n\n\n\n\n\ncustomer_id\nphone\n\n\n\n\n0\n15962\n876.997.0254\n\n\n1\n99723\n001-706-213-0362\n\n\n2\n99723\n886.527.4420x90003\n\n\n\n\n\n\n\n\n\nLength: 855 Unique: 524 Duplicated: 331 Empty: 0"
  },
  {
    "objectID": "posts/20240731-customers-graphs/index.html#icons-for-nodes",
    "href": "posts/20240731-customers-graphs/index.html#icons-for-nodes",
    "title": "Merging Customers Records Using Graphs in Python",
    "section": "Icons for Nodes",
    "text": "Icons for Nodes\nNext chunk of code creates a dictionary of icons for different types of nodes in the graph. It will be used later to visualize the subgraphs.\n\nimport PIL\n\nicons = {\n    \"customer\": \"icons/customer.png\",\n    \"phone\": \"icons/phone.png\",\n    \"email\": \"icons/email.png\",\n}\n\nimages = {k: PIL.Image.open(fname) for k, fname in icons.items()}"
  },
  {
    "objectID": "posts/20240731-customers-graphs/index.html#creating-a-graph",
    "href": "posts/20240731-customers-graphs/index.html#creating-a-graph",
    "title": "Merging Customers Records Using Graphs in Python",
    "section": "Creating a Graph",
    "text": "Creating a Graph\nLet’s create graph and add nodes. Each node will represent a customer, email, or phone. We will use the images dictionary to assign an image to each node, but it’s not necessary for the procedure, as well as setting the type of the node.\n\nG = nx.Graph()\n\nnodes = []\n\nfor x in emails[\"email\"].dropna().unique():\n    G.add_node(x, image=images[\"email\"], type=\"email\")\n\nfor x in phones[\"phone\"].dropna().unique():\n    G.add_node(x, image=images[\"phone\"], type=\"phone\")\n\nfor x in customers[\"customer_id\"].unique():\n    G.add_node(x, image=images[\"customer\"], type=\"customer\")\n\nNext, we will add edges to the graph. The edges will connect customers with their emails and phones.\n\nedges = []\n\nfor x in customers[[\"customer_id\"]].merge(emails).values:\n    edges.append(x)\n\nfor x in customers[[\"customer_id\"]].merge(phones).values:\n    edges.append(x)\n\nG.add_edges_from(edges)"
  },
  {
    "objectID": "posts/20240731-customers-graphs/index.html#finding-groups-of-connected-customers",
    "href": "posts/20240731-customers-graphs/index.html#finding-groups-of-connected-customers",
    "title": "Merging Customers Records Using Graphs in Python",
    "section": "Finding Groups of Connected Customers",
    "text": "Finding Groups of Connected Customers\nCustomers that share the same email or phone will be connected by the edges. Let’s find groups of connected customers.\n\ngroups = list(nx.connected_components(G))\nprint(\"Groups:\", len(groups))\n\nGroups: 559"
  },
  {
    "objectID": "posts/20240731-customers-graphs/index.html#visualizing-the-graph",
    "href": "posts/20240731-customers-graphs/index.html#visualizing-the-graph",
    "title": "Merging Customers Records Using Graphs in Python",
    "section": "Visualizing the Graph",
    "text": "Visualizing the Graph\nThe number of connected components is rather large to visualize all of them, and most of the groups will contain only a few nodes. Let’s find the groups with the largest number of nodes and visualize them.\n\ndf = pd.DataFrame([groups]).T\ndf.columns = [\"group\"]\ndf[\"size\"] = df[\"group\"].apply(len)\ndf[\"size\"].hist(bins=20, log=True)\nplt.title(\"Group Size Distribution\")\nplt.show();\nplt.close()\n\n\n\n\n\n\n\n\nThe simplest way to visualize the graph is to use the draw function from the networkx library. We will use the nx.draw function to visualize the graph. We will create a grid of subplots and visualize the top groups. Parameter seed is set to 42 to make the layout reproducible.\n\nfig, axes = plt.subplots(3, 4, figsize=(8, 6))\n\ntop_groups = list(\n    df.sort_values(\n        \"size\",\n        ascending=False,\n    )\n    .head(len(axes.flatten()))\n    .index\n)\n\nfor i, g in enumerate(top_groups):\n    ax = axes.flatten()[i]\n    subgraph = G.subgraph(groups[g])\n    pos = nx.spring_layout(subgraph, seed=42)\n    nx.draw(\n        subgraph,\n        pos=pos,\n        with_labels=False,\n        node_size=25,\n        ax=ax,\n    )\n    ax.set_title(f\"Group {g}\");\n\nplt.tight_layout()\nplt.show();\nplt.close()\n\n\n\n\n\n\n\n\nThere are literally constellations of different shapes and sizes. Let’s visualize some of them in more detail."
  },
  {
    "objectID": "posts/20240731-customers-graphs/index.html#visualizing-subgraphs",
    "href": "posts/20240731-customers-graphs/index.html#visualizing-subgraphs",
    "title": "Merging Customers Records Using Graphs in Python",
    "section": "Visualizing Subgraphs",
    "text": "Visualizing Subgraphs\nLet’s visualize one of the largest group in more detail. We will use the nx.draw_networkx_edges function to draw the edges and the imshow function to display the icons of the nodes. We will also add the customer id to the customers’ nodes. The value of parameter seed is set to the same value as in the previous chunk to keep the layout. You can change it to see different layouts.\n\nsubgraph = G.subgraph(groups[91])\nfig, ax = plt.subplots(figsize=(8, 8))\n\npos = nx.spring_layout(subgraph, seed=42)\n\nnx.draw_networkx_edges(\n    subgraph,\n    pos=pos,\n    ax=ax,\n    arrows=True,\n    arrowstyle=\"-\",\n    min_source_margin=10,\n    min_target_margin=10,\n)\n\n\ntr_figure = ax.transData.transform\ntr_axes = fig.transFigure.inverted().transform\n\n\nicon_size = (ax.get_xlim()[1] - ax.get_xlim()[0]) * 0.015\nicon_center = icon_size / 2.0\n\nfor n in subgraph.nodes:\n    xf, yf = tr_figure(pos[n])\n    xa, ya = tr_axes((xf, yf))\n    a = plt.axes([xa - icon_center, ya - icon_center, icon_size, icon_size])\n    a.imshow(subgraph.nodes[n][\"image\"])\n    if G.nodes[n][\"type\"] == \"customer\":\n        a.text(\n            0.5,\n            0.5,\n            n,\n            ha=\"center\",\n            va=\"center\",\n            fontsize=8,\n            color=\"red\",\n            backgroundcolor=\"white\",\n            bbox=dict(color=\"white\", facecolor=\"white\", alpha=0.5),\n        )\n    a.axis(\"off\")\n\nsns.despine(left=True, bottom=True)\n\nplt.show();\nplt.close()"
  },
  {
    "objectID": "posts/20240731-customers-graphs/index.html#why-graphs-and-not-sql",
    "href": "posts/20240731-customers-graphs/index.html#why-graphs-and-not-sql",
    "title": "Merging Customers Records Using Graphs in Python",
    "section": "Why Graphs and Not SQL?",
    "text": "Why Graphs and Not SQL?\nWe can see here that the customers in this group form pretty complex relationships. A customer may be connected to another one by the phone numbers, and the other one may be connected to the third one by the email, forming a chain of connections. I believe that it is nearly impossible to find this kind of relationship using SQL. The more complex the relationships are, the more time and effort it will take to find them using SQL. For example, if we have a chain of 10 customers, where each customer is connected to the next one by the phone number, it will take 10 joins to find this chain using SQL. If we have 100 customers in the chain, it will take 100 joins to find it using SQL, and the query will probably never complete. But it takes fractions of a second to find it using the graph."
  },
  {
    "objectID": "posts/20240731-customers-graphs/index.html#assigning-groups-to-customers",
    "href": "posts/20240731-customers-graphs/index.html#assigning-groups-to-customers",
    "title": "Merging Customers Records Using Graphs in Python",
    "section": "Assigning Groups to Customers",
    "text": "Assigning Groups to Customers\nFinally, we will assign a group to each customer. For that, we will expand the groups list and create a new DataFrame with the group_id and customer_id columns.\n\ndf = pd.DataFrame([groups]).T\ndf.columns = [\"customer_id\"]\ndf = df.explode(\"customer_id\")\ndf[\"group_id\"] = df.index\ndf.tail(3)\n\n\n\n\n\n\n\n\ncustomer_id\ngroup_id\n\n\n\n\n557\n601053\n557\n\n\n558\n571.212.7377x69843\n558\n\n\n558\n590385\n558\n\n\n\n\n\n\n\nNote that customer_id column contains phone numbers and emails as well as customer ids, but when we merge the data, there will remain only the customer ids.\n\ncustomers = customers.merge(df)\ncustomers.head(3)\n\n\n\n\n\n\n\n\ncustomer_id\nname\ngroup_id\n\n\n\n\n0\n330087\nWilliam Sparks\n1\n\n\n1\n443237\nJoseph Williams\n4\n\n\n2\n329867\nEddie Porter\n6\n\n\n\n\n\n\n\nLet’s check the number of customers and unique customer ids to make sure that we didn’t lose any customers neither we added duplicates.\n\nlen(customers), len(customers[\"customer_id\"].unique())\n\n(1000, 1000)\n\n\nLooks good. Now we can save the data to the file.\n\ncustomers.to_csv(\"data/customers_grouped.csv\", index=False)"
  },
  {
    "objectID": "posts/20240731-customers-graphs/index.html#conclusion",
    "href": "posts/20240731-customers-graphs/index.html#conclusion",
    "title": "Merging Customers Records Using Graphs in Python",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, we explored how to merge customer records using graphs. We created a graph of customers, emails, and phones and found groups of connected customers. We assigned a group to each customer and saved the data to a file. This approach can help us identify duplicate records and perform actions depending on our business logic. We also visualized the graph and subgraphs to better understand the relationships between customers. This can be useful for marketing analysis, customer segmentation, and other tasks that require a deep understanding of customer relationships."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I’m an entrepreneur and data enthusiast with a strong focus on using analytics to drive smarter business decisions. After years of running my own company, I transitioned into the data space to turn intuition and experience into measurable insights."
  },
  {
    "objectID": "about.html#from-business-to-data",
    "href": "about.html#from-business-to-data",
    "title": "About Me",
    "section": "From Business to Data",
    "text": "From Business to Data\nAs a business owner, I managed operations, strategy, and performance using data long before I wrote my first line of Python. That hands-on experience gave me a practical understanding of what metrics matter and how to translate numbers into action. Today, I apply data science tools to solve business problems at scale."
  },
  {
    "objectID": "about.html#what-i-do",
    "href": "about.html#what-i-do",
    "title": "About Me",
    "section": "What I Do",
    "text": "What I Do\nMy work centers around solving business challenges with data. Current focus areas include:\n\nAutomating reporting and KPIs using SQL, Python, and Power BI\n\nForecasting finance using statistical and ML models\n\nDesigning ETL pipelines with Docker and Airflow\n\nMaking sense of real-world data to support business growth"
  },
  {
    "objectID": "about.html#about-this-blog",
    "href": "about.html#about-this-blog",
    "title": "About Me",
    "section": "About This Blog",
    "text": "About This Blog\nThis blog is my portfolio and a space to explore ideas, document projects, and share solutions. It’s built to demonstrate how I approach analytics from a business-first perspective."
  },
  {
    "objectID": "about.html#featured-projects",
    "href": "about.html#featured-projects",
    "title": "About Me",
    "section": "Featured Projects",
    "text": "Featured Projects\n\n\n\nProject\nDescription\nTopics\n\n\n\n\nBusiness in Russia 2011-2024\nResearch project in progress\nFinancial Analysis · Linear Models · Spatialization · Data Visualization\n\n\nBusiness Intelligence System\nA comprehensive BI system that consolidates data from multiple sources, including ERP systems, Google Analytics, PBX, Help Desk, task trackers, and e-commerce platform. The system delivers insights through Microsoft Power BI dashboards and reports in four key sections: Finance, Supply, Marketing, Sales.\nKey Performance Indicators · Dashboards · Microsoft Fabric · Google BigQuery · Apache Airflow · Web Analytics · KPI Dashboards · Business Intelligence (BI) · Microsoft Power BI · Data Engineering · Business Analytics · ETL Tools\n\n\nForecash\nAn application that generates income and expense forecasts using time series analysis in R and a Power BI report for intuitive, interactive exploration of financial trends. The app reduces risks of cash deficiency by providing real-time forecasts.\nTime Series Analysis · Time Series Forecasting · Financial Forecasting · R (Programming Language) · Microsoft Power BI · Docker\n\n\nContent Mate\nWeb application that consolidates dozens of fragmented operations into a single interface. These include searching for product descriptions online, selecting and processing images, and generating product names and descriptions using LLMs.\nWeb Application · Flask · Gunicorn · Celery · Redis · SQL · Google BigQuery · LLM · Langchain · Natural Language Processing (NLP) · HTML · JavaScript\n\n\nRetrieval-Augmented Generation\nFunctional prototype of a Retrieval-Augmented Generation system.\nRetrieval-Augmented Generation (RAG) · Langchain · Natural Language Processing (NLP) · Web Scraping\n\n\nIncentiWise\nAn analytical system (Python, BigQuery, Power BI) designed to enhance workplace transparency by providing employees with a clear view of their performance metrics and compensation details.\nHR Management · Performance Management · KPI Dashboards · Employee Benefits Design\n\n\nSupStock\nData integration service for an e-commerce company that extracts supplier stock data from diverse sources, matches it to internal SKUs, and uploads the results to a centralized data warehouse for real-time inventory updates.\nData Integration · ETL · Automation · Inventory Management · Data Matching\n\n\nPrice Intel\nWeb scraping project that gathers pricing information.\nWeb Scraping · Asynchronous Programming · Competitive Intelligence · Price Monitoring\n\n\nSpeechKitty\nPython package designed for transcribing call records. It leverages two speech-to-text APIs, supports both single and dual-channel records, and performs diarization to partition fragments for different speakers.\nPython Packaging · API Integration · Audio Processing · Speech-to-Text"
  },
  {
    "objectID": "about.html#outside-of-work",
    "href": "about.html#outside-of-work",
    "title": "About Me",
    "section": "Outside of Work",
    "text": "Outside of Work\n  Instagram     Facebook \nWhen I’m not working with data, you can find me behind the camera. Photography is my creative outlet—it helps me slow down, observe the world, and enjoy the process of framing and storytelling in a different medium."
  },
  {
    "objectID": "about.html#lets-connect",
    "href": "about.html#lets-connect",
    "title": "About Me",
    "section": "Let’s Connect",
    "text": "Let’s Connect"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Systems, Data, and a Dash of Curiosity",
    "section": "",
    "text": "Animation of Spatial Data\n\n\n \n\n\nVisualization\n\nSpatial\n\nAnimation\n\nR\n\n\n \n\nCreating an animated visualization of spatial data using R\n\n\n\nJul 4, 2025\nAleksei \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProduct Cards Creation Application\n\n\n \n\n\nApp\n\nLLM\n\nPython\n\n\n \n\n\n\n\n\nMay 31, 2025\nAleksei \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Anki Flashcards From List of Words\n\n\n \n\n\nNLP\n\nPython\n\n\n \n\n\n\n\n\nMay 3, 2025\nAleksei \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing a Local Retrieval-Augmented Generation System\n\n\n \n\n\nRAG\n\nNLP\n\nLLM\n\nPython\n\n\n \n\n\n\n\n\nMar 21, 2025\nAleksei \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNerdy Valentine's in Python, R, and Matlab\n\n\n \n\n\nPython\n\nR\n\nMatlab\n\n\n \n\n\n\n\n\nFeb 14, 2025\nAleksei \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun Docker Containers Remotely with Airflow\n\n\n \n\n\nAirflow\n\nDocker\n\n\n \n\n\n\n\n\nJan 8, 2025\nAleksei \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBI System Blueprint\n\n\n \n\n\nBI\n\nETL\n\n\n \n\n\n\n\n\nJan 6, 2025\nAleksei \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCV Week 2024\n\n\n \n\n\nCompVis\n\nML\n\n\n \n\n\n\n\n\nDec 18, 2024\nAleksei \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Airflow FileSensor for Triggering ETL Process\n\n\n \n\n\nETL\n\nAirflow\n\n\n \n\n\n\n\n\nNov 5, 2024\nAleksei \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEuropean Tech Salaries\n\n\n \n\n\nMoney\n\nR\n\n\n \n\n\n\n\n\nSep 27, 2024\nAleksei \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython Library for Russian Macroeconomics Data\n\n\n \n\n\nTimeseries\n\nMacroeconomics\n\nPython\n\n\n \n\n\n\n\n\nAug 22, 2024\nAleksei \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKano Method for Prioritization of Features\n\n\n \n\n\nMarketing\n\nProduct\n\nPython\n\n\n \n\n\n\n\n\nAug 5, 2024\nAleksei \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMerging Customers Records Using Graphs in Python\n\n\n \n\n\nPython\n\nGraphs\n\n\n \n\n\n\n\n\nJul 31, 2024\nAleksei \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Geospatial Insights with R and rnaturalearth\n\n\n \n\n\nR\n\nGeo\n\n\n \n\n\n\n\n\nJul 25, 2024\nAleksei \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n \n\n\nNews\n\n\n \n\n\n\n\n\nJul 21, 2024\nAleksei \n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/20250503-anki-part-1/index.html",
    "href": "posts/20250503-anki-part-1/index.html",
    "title": "Creating Anki Flashcards From List of Words",
    "section": "",
    "text": "This notebook demonstrates how to create Anki flashcards from a list of words. The example uses a list of German words related to “Die Stadt” (The City) and translates them into English. It also generates audio files for the words using Google Text-to-Speech.\n\n\n\n\n\n\nFigure 1"
  },
  {
    "objectID": "posts/20250503-anki-part-1/index.html#introduction",
    "href": "posts/20250503-anki-part-1/index.html#introduction",
    "title": "Creating Anki Flashcards From List of Words",
    "section": "",
    "text": "This notebook demonstrates how to create Anki flashcards from a list of words. The example uses a list of German words related to “Die Stadt” (The City) and translates them into English. It also generates audio files for the words using Google Text-to-Speech.\n\n\n\n\n\n\nFigure 1"
  },
  {
    "objectID": "posts/20250503-anki-part-1/index.html#install-required-libraries",
    "href": "posts/20250503-anki-part-1/index.html#install-required-libraries",
    "title": "Creating Anki Flashcards From List of Words",
    "section": "Install Required Libraries",
    "text": "Install Required Libraries\n!pip install pandas googletrans gtts genanki"
  },
  {
    "objectID": "posts/20250503-anki-part-1/index.html#set-parameters",
    "href": "posts/20250503-anki-part-1/index.html#set-parameters",
    "title": "Creating Anki Flashcards From List of Words",
    "section": "Set Parameters",
    "text": "Set Parameters\n\nSRC_LANG = \"de\"\nDST_LANG = \"en\"\nDATA_DIR = \"data\"\nSRC_FILE = \"die Stadt.txt\"\nAUDIO_DIR = \"data/audio\""
  },
  {
    "objectID": "posts/20250503-anki-part-1/index.html#load-words-list",
    "href": "posts/20250503-anki-part-1/index.html#load-words-list",
    "title": "Creating Anki Flashcards From List of Words",
    "section": "Load Words List",
    "text": "Load Words List\nHere we load the list of words from a text file. The file should contain one word per line, and we will remove any empty lines.\n\nimport pandas as pd\n\nwith open(f\"{DATA_DIR}/{SRC_FILE}\", \"r\") as f:\n    lines = f.readlines()\nlines = [line.strip() for line in lines if line.strip()]  # Remove empty lines\n\ndf = pd.DataFrame(lines, columns=[\"Word\"])\ndf.tail(5)\n\n\n\n\n\n\n\n\nWord\n\n\n\n\n113\ndas Kreuzfahrtschiff / die Kreuzfahrtschiffe\n\n\n114\nzu Fuß\n\n\n115\ndie Fahrkarte / die Fahrkarten\n\n\n116\nder Fahrplan / die Fahrpläne\n\n\n117\ndie Endstation / die Endstationen\n\n\n\n\n\n\n\nIn the particular example, the words are in the format “Word / Plural”. For example, “die Stadt / die Städte” means “the city / the cities (plural)”. Let’s split singular and plural forms into separate rows.\n\ndf[\"Word\"] = df[\"Word\"].str.split(\" / \")\ndf = df.explode(\"Word\").reset_index(drop=True)\ndf.tail(5)\n\n\n\n\n\n\n\n\nWord\n\n\n\n\n228\ndie Fahrkarten\n\n\n229\nder Fahrplan\n\n\n230\ndie Fahrpläne\n\n\n231\ndie Endstation\n\n\n232\ndie Endstationen"
  },
  {
    "objectID": "posts/20250503-anki-part-1/index.html#translate-words",
    "href": "posts/20250503-anki-part-1/index.html#translate-words",
    "title": "Creating Anki Flashcards From List of Words",
    "section": "Translate Words",
    "text": "Translate Words\nWe will use the googletrans library to translate the words from German to English. The library provides an asynchronous interface for translation, which is useful for bulk processing.\n\nfrom googletrans import Translator\n\nasync def translate_bulk(texts: list):\n    async with Translator() as translator:\n        translations = await translator.translate(texts, src=SRC_LANG, dest=DST_LANG)\n        return [translation.text for translation in translations]\n\n\nawait translate_bulk(\n    [\"die Polizei\", \"die Motorräder\", \"die Krankenhäuser\", \"die Bahnhöfe\", \"die Flugzeuge\"]\n)\n\n['the police',\n 'The motorcycles',\n 'The hospitals',\n 'The train stations',\n 'The aircraft']\n\n\nIn the next chunk of code, we will create list of words to translate, then will apply the translate_bulk function to this list.\n\ntexts = df[\"Word\"].to_list()\n\ntranslations = await translate_bulk(texts)\n\ntranslations[:5]\n\n['the city', 'the cities', 'the village', 'The villages', 'the street']\n\n\nThe googletrans library employs undocumented Google Translate API, which may lead to rate limiting or blocking. We will hope that it will work for our case. If you encounter issues, consider using a paid translation service or API.\nNext, we will add the translations to the DataFrame.\n\ndf[\"Translation\"] = translations"
  },
  {
    "objectID": "posts/20250503-anki-part-1/index.html#generate-audio-files",
    "href": "posts/20250503-anki-part-1/index.html#generate-audio-files",
    "title": "Creating Anki Flashcards From List of Words",
    "section": "Generate Audio Files",
    "text": "Generate Audio Files\nWe will use the gtts library to generate audio files for the words. This library uses Google Text-to-Speech API and it’s free. The generated audio files will be saved in the AUDIO_DIR directory. The filenames will be generated using a hash of the word.\n\nfrom gtts import gTTS\nimport os\nimport hashlib\n\ndef gen_audio(sentence):\n    h = hashlib.shake_128(sentence.encode()).hexdigest(6)\n    filename = f\"{h}.mp3\"\n    if filename in [f for f in os.listdir(AUDIO_DIR)]:\n        return filename\n    try:\n        gTTS(text=sentence, lang=SRC_LANG, slow=True).save(f\"{AUDIO_DIR}/{filename}\")\n    except Exception as e:\n        return None\n    return filename\n\n\nsample_audio = gen_audio(\"die Krankenhäuser\")\nprint(f\"Audio file saved as: {sample_audio}\")\n\nAudio file saved as: 308d817b87e2.mp3\n\n\nThe gen_audio function generates a hash of the word and checks if the audio file already exists in the AUDIO_DIR. If it does, it returns the filename. If not, it generates the audio file and saves it."
  },
  {
    "objectID": "posts/20250503-anki-part-1/index.html#add-audio-file-paths-to-dataframe",
    "href": "posts/20250503-anki-part-1/index.html#add-audio-file-paths-to-dataframe",
    "title": "Creating Anki Flashcards From List of Words",
    "section": "Add Audio File Paths to DataFrame",
    "text": "Add Audio File Paths to DataFrame\n\ndf[\"Audio\"] = df[\"Word\"].apply(gen_audio)"
  },
  {
    "objectID": "posts/20250503-anki-part-1/index.html#shuffle-dataframe",
    "href": "posts/20250503-anki-part-1/index.html#shuffle-dataframe",
    "title": "Creating Anki Flashcards From List of Words",
    "section": "Shuffle DataFrame",
    "text": "Shuffle DataFrame\nWe will shuffle the DataFrame to randomize the order of the flashcards.\n\ndf = df.sample(frac=1).reset_index(drop=True)"
  },
  {
    "objectID": "posts/20250503-anki-part-1/index.html#set-up-anki-deck-and-model",
    "href": "posts/20250503-anki-part-1/index.html#set-up-anki-deck-and-model",
    "title": "Creating Anki Flashcards From List of Words",
    "section": "Set Up Anki Deck and Model",
    "text": "Set Up Anki Deck and Model\nWe will use the genanki library to create an Anki deck and model. The model defines the structure of the flashcards, while the deck contains the flashcards themselves. To get unique IDs for the model and deck, we will use random numbers. We will set seed for reproducibility.\n\nimport random\n\nrandom.seed(42)\n\nMODEL_NAME = \"Vocabulary\"\nDECK_NAME = \"Die Stadt\"\nMODEL_ID = random.randrange(1 &lt;&lt; 30, 1 &lt;&lt; 31)\nDECK_ID = random.randrange(1 &lt;&lt; 30, 1 &lt;&lt; 31)"
  },
  {
    "objectID": "posts/20250503-anki-part-1/index.html#add-cards-to-anki-deck",
    "href": "posts/20250503-anki-part-1/index.html#add-cards-to-anki-deck",
    "title": "Creating Anki Flashcards From List of Words",
    "section": "Add Cards to Anki Deck",
    "text": "Add Cards to Anki Deck\nNext, will create a model for the flashcards and add the cards to the Anki deck.\n\nimport genanki\n\nmy_model = genanki.Model(\n    MODEL_ID,\n    MODEL_NAME,\n    fields=[\n        {\"name\": \"Question\"},\n        {\"name\": \"Answer\"},\n        {\"name\": \"Audio\"},\n    ],\n    templates=[\n        {\n            \"name\": \"{{Question}}\",\n            \"qfmt\": '&lt;div class=\"head\"&gt;{{Question}}&lt;/div&gt;',\n            \"afmt\": '&lt;div class=\"head\"&gt;{{Question}}&lt;/div&gt;&lt;hr id=\"answer\"&gt; \\\n                &lt;div class=\"head\"&gt;{{Answer}}&lt;/div&gt; {{Audio}}',\n        },\n    ],\n    css=\"\"\"\n        .head {font-size: x-large;} \n        .spot {text-decoration: underline;} \n        .sentence {font-style: italic; font-size: normal!important;}\n    \"\"\",\n)\n\nmy_deck = genanki.Deck(\n    DECK_ID,\n    DECK_NAME,\n)\n\nfor i, row in df.iterrows():\n    my_note = genanki.Note(\n        model=my_model,\n        fields=[\n            row[\"Translation\"],\n            row[\"Word\"],\n            f\"[sound:{row['Audio']}]\",\n        ],\n    )\n    my_deck.add_note(my_note)"
  },
  {
    "objectID": "posts/20250503-anki-part-1/index.html#export-anki-deck",
    "href": "posts/20250503-anki-part-1/index.html#export-anki-deck",
    "title": "Creating Anki Flashcards From List of Words",
    "section": "Export Anki Deck",
    "text": "Export Anki Deck\nFinally, we will export the Anki deck to a file. The file will be saved in the DATA_DIR directory with the name Die Stadt.apkg. The audio files will be included in the package.\n\nmy_package = genanki.Package(my_deck)\nmy_package.media_files = [f\"{AUDIO_DIR}/{filename}\" for filename in df[\"Audio\"].values]\nmy_package.write_to_file(f\"{DATA_DIR}/{DECK_NAME}.apkg\")"
  },
  {
    "objectID": "posts/20250503-anki-part-1/index.html#conclusion",
    "href": "posts/20250503-anki-part-1/index.html#conclusion",
    "title": "Creating Anki Flashcards From List of Words",
    "section": "Conclusion",
    "text": "Conclusion\nIn this notebook, we learned how to create Anki flashcards from a list of words. We used the googletrans library to translate the words from German to English and the gtts library to generate audio files for the words. Finally, we used the genanki library to create an Anki deck and export it to a file. The generated Anki deck can be imported into Anki app and used for studying the vocabulary."
  },
  {
    "objectID": "posts/20250503-anki-part-1/index.html#references",
    "href": "posts/20250503-anki-part-1/index.html#references",
    "title": "Creating Anki Flashcards From List of Words",
    "section": "References",
    "text": "References\n\nAnki apps\nShared Decks - AnkiWeb"
  },
  {
    "objectID": "posts/20250503-anki-part-1/index.html#next-steps",
    "href": "posts/20250503-anki-part-1/index.html#next-steps",
    "title": "Creating Anki Flashcards From List of Words",
    "section": "Next Steps",
    "text": "Next Steps\nIn the next notebook we will create Anki flashcards from an arbitrary document like an article or a book."
  },
  {
    "objectID": "posts/20240725-views-of-russia/index.html",
    "href": "posts/20240725-views-of-russia/index.html",
    "title": "Exploring Geospatial Insights with R and rnaturalearth",
    "section": "",
    "text": "The article showcases the utilization of the rnaturalearth package for handling geographical data. This package provides valuable tools and functions for working with spatial information, making it a powerful resource for data analysts and researchers interested in geographic analyses.\nToday, I stumbled upon an article discussing the approval ratings of Russia among people from various nations around the world. As I examined the list, which was sorted from worst to best, a hypothesis formed in my mind: Could the distance between this particular country and others correlate with its citizens’ approval of its international affairs? To explore this, I promptly collected data and calculated the geographical distances between the boundaries of Russia and those of the countries in the list. The null hypothesis posits that distance has no impact on approval rates, while the alternative hypothesis suggests that distance does indeed influence approval levels.\n\ntheme_set(theme_minimal())\n\nworld &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\ndf &lt;- read.csv(\"ViewsOfRussia2024.csv\")\n\nby &lt;- join_by(admin == country)\nworld &lt;- left_join(world, df, by)\n\nworld &lt;- world[world$admin != \"Antarctica\", ]\n\nggplot(data = world) + \n  geom_sf(aes(fill = approval)) + \n  scale_fill_viridis_c(option = \"plasma\") + \n  # theme_void() +\n  theme(legend.position = \"bottom\", \n        legend.key.height = unit(5, \"pt\"), \n        legend.key.width = unit(40, \"pt\"), \n        legend.title.position = \"bottom\") + \n  labs(fill = \"% who have a favorable view of Russia\")\n\n\n\n\n\n\n\n\n\ncountries &lt;- ne_countries(returnclass = \"sf\")\nrussia &lt;- filter(countries, grepl(\"Russia\", admin))\n\ninvisible(sf_use_s2(FALSE))\n\ndf &lt;- df |&gt; rowwise() |&gt;\n  mutate(distB = st_distance(russia, countries[countries$admin == country, ])[1])\n\ndf$distB &lt;- as.numeric(sub(\"([0-9\\\\.]+)\", \"\\\\1\", df$distB)) / 1000000\n\nmodel &lt;- lm(approval ~ distB, data = df)\nsummary(model)\n\n\nCall:\nlm(formula = approval ~ distB, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-23.345 -11.519  -4.029  13.302  28.339 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   22.406      3.859   5.806 1.91e-06 ***\ndistB          1.513      0.814   1.859   0.0723 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.02 on 32 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.09743,   Adjusted R-squared:  0.06922 \nF-statistic: 3.454 on 1 and 32 DF,  p-value: 0.07231\n\n\nThe model explains less than 10% of variability. P-value for distance is 0.072, so the null hypothesis cannot be rejected at the level of 0.05. Scatter plot also shows no obvious trend.\n\nqplot(df$distB, df$approval) + \n  geom_point() + \n  stat_smooth(method = \"lm\", se = F, color = \"red\", formula = y ~ x)\n\n\n\n\n\n\n\n\nIt emerged that the geographical distance between boundaries was statistically insignificant. However, I propose an alternative hypothesis in this scenario. Russia, being an exceptionally vast country, shares proximity with Asian nations in its eastern part. Interestingly, these eastern countries exhibit a more favorable attitude toward Russia compared to their European counterparts. One plausible explanation for this discrepancy is the absence of significant Russian territorial interests in Asia. Since Moscow, the capital, lies in the western part of Russia, let’s measure the distance between capitals and explore this further using regression analysis.\n\ncities &lt;- ne_download(type = \"populated_places\", returnclass = \"sf\")\n\nReading layer `ne_110m_populated_places' from data source \n  `/tmp/Rtmp1g0rpV/ne_110m_populated_places.shp' using driver `ESRI Shapefile'\nSimple feature collection with 243 features and 137 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -175.2206 ymin: -41.29207 xmax: 179.2166 ymax: 64.14346\nGeodetic CRS:  WGS 84\n\ncapitals &lt;- cities[cities$FEATURECLA == \"Admin-0 capital\", ]\n\n\ncapitals &lt;- capitals |&gt; distinct(ADM0NAME, .keep_all = TRUE)\nmoscow &lt;- cities[cities$NAME == \"Moscow\", ]\n\ndf &lt;- read.csv(\"ViewsOfRussia2024.csv\")\n\nby &lt;- join_by(country == ADM0NAME)\ndf &lt;- left_join(df, capitals, by) |&gt; select(country, approval, NAME)\n\ndf &lt;- df |&gt; rowwise() |&gt; \n  mutate(distC = st_distance(moscow, capitals[capitals$NAME == NAME, ])[1])\ndf$distC &lt;- as.numeric(sub(\"([0-9\\\\.]+)\", \"\\\\1\", df$distC)) / 1000000\n\nmodel &lt;- lm(approval ~ distC, data = df)\nsummary(model)\n\n\nCall:\nlm(formula = approval ~ distC, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-27.205 -14.005  -1.208  14.432  27.698 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  21.7485     5.1834   4.196 0.000192 ***\ndistC         0.9300     0.6958   1.337 0.190512    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.21 on 33 degrees of freedom\nMultiple R-squared:  0.05135,   Adjusted R-squared:  0.02261 \nF-statistic: 1.786 on 1 and 33 DF,  p-value: 0.1905\n\n\nUnfortunately, using the distance between capitals didn’t yield meaningful results either.\n\nqplot(df$distC, df$approval) + \n  geom_point() + \n  stat_smooth(method = \"lm\", se = F, color = \"red\", formula = y ~ x)\n\n\n\n\n\n\n\n\nIn my search for additional regressors, I included GDP per capita,\n\ndf &lt;- read.csv(\"ViewsOfRussia2024.csv\")\n\nby &lt;- join_by(country == admin)\ndf &lt;- left_join(df, countries, by) |&gt; select(country, approval, gdp_md, pop_est, economy)\n\ndf &lt;- df |&gt; mutate(gdp_pc = 1000 * gdp_md / pop_est)\n\nmodel &lt;- lm(approval ~ gdp_pc, data = df)\nsummary(model)\n\n\nCall:\nlm(formula = approval ~ gdp_pc, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.612  -6.029   1.617   4.936  22.483 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.26819    2.63352  16.050  &lt; 2e-16 ***\ngdp_pc      -0.67906    0.09049  -7.505 1.52e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.15 on 32 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.6377,    Adjusted R-squared:  0.6264 \nF-statistic: 56.32 on 1 and 32 DF,  p-value: 1.521e-08\n\n\nand it yielded promising results. The coefficient associated with GDP showed a remarkably low p-value of 1.52e-08, providing strong evidence against the null hypothesis. The coefficient of determination (R-squared) was also quite favorable at 0.6377, indicating that the model captures a substantial portion of the variation in approval rates. The coefficient with gdp_pc indicates that for every additional thousand USD of GDP per capita, there is a corresponding 0.7 percentage point decrease in the approval rate.\n\nqplot(df$gdp_pc, df$approval) + \n  geom_point() + \n  stat_smooth(method = \"lm\", formula = y ~ x) + \n  labs(x = \"GDP per capita, K\", y = \"% who have a favorable view of Russia\")\n\n\n\n\n\n\n\n\nIn an effort to enhance predictive power, one can explore the possibility of non-linear dependencies. Let’s consider using the logarithm of GDP as a predictor.\n\nmodel &lt;- lm(approval ~ log(gdp_pc), data = df)\nsummary(model)\n\n\nCall:\nlm(formula = approval ~ log(gdp_pc), data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.983  -5.332  -0.769   3.175  28.181 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   58.164      3.828  15.194 3.46e-16 ***\nlog(gdp_pc)  -12.052      1.371  -8.794 4.77e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.125 on 32 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.7073,    Adjusted R-squared:  0.6982 \nF-statistic: 77.33 on 1 and 32 DF,  p-value: 4.77e-10\n\n\nThe resulting model yields an impressive R² value of 0.7073, indicating that it explains the vast amount of the variation. Additionally, the p-value of 4.77e-10 provides the strongest evidence against the null hypothesis.\n\nqplot(log(df$gdp_pc), df$approval) + \n  geom_point() + \n  stat_smooth(method = \"lm\", formula = y ~ x) + \n  labs(x = \"Logarithm of GDP per capita\", y = \"% who have a favorable view of Russia\")\n\n\n\n\n\n\n\n\nHowever, this improved model is more complex and less straightforward to explain. Allow me to attempt an interpretation: If a country’s GDP per capita is 1% lower than another country’s, it tends to have 0.12% more people who approve of Russia.\nNow that we’ve obtained the regression model, we can use it to make predictions for the remaining countries and visualize the results on a map. By assigning colors based on predicted approval rates, we’ll create an informative and visually appealing representation.\n\nworld &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\nworld &lt;- world[world$admin != \"Antarctica\", ]\n\nworld &lt;- world |&gt; mutate(gdp_pc = 1000 * gdp_md / pop_est)\n\ninvisible(na.omit(world, cols = \"gdp_pc\"))\n\npred &lt;- predict(model, world)\n\nworld &lt;- cbind(world, pred)\n\nby &lt;- join_by(admin == country)\nworld &lt;- left_join(world, df, by)\n\nworld &lt;- mutate(world, approval = coalesce(approval, pred))\n\nworld[world$admin == \"Russia\", ]$approval &lt;- NA\n\nggplot(data = world) + \n  geom_sf(aes(fill = approval)) + \n  scale_fill_viridis_c(option = \"plasma\") + \n  theme(legend.position = \"bottom\", \n        legend.key.height = unit(5, \"pt\"), \n        legend.key.width = unit(40, \"pt\"), \n        legend.title.position = \"bottom\") + \n  labs(fill = \"\")"
  },
  {
    "objectID": "posts/20240805-kano-model/index.html",
    "href": "posts/20240805-kano-model/index.html",
    "title": "Kano Method for Prioritization of Features",
    "section": "",
    "text": "The Kano model is a theory for product development and customer satisfaction developed in the 1980s by Professor Noriaki Kano. The model classifies customer preferences into five categories: Must-be Quality, One-dimensional Quality, Attractive Quality, Indifferent Quality, and Reverse Quality. The Kano model is used to prioritize features and functionalities in product development based on customer needs and expectations."
  },
  {
    "objectID": "posts/20240805-kano-model/index.html#categories-of-the-kano-model",
    "href": "posts/20240805-kano-model/index.html#categories-of-the-kano-model",
    "title": "Kano Method for Prioritization of Features",
    "section": "Categories of the Kano Model",
    "text": "Categories of the Kano Model\nMust-be features are basic requirements that customers expect. If these features are not present in a product, customers will be dissatisfied. However, the presence of these features does not necessarily lead to customer satisfaction. Must-be Quality features are considered essential for the product.\n\nExamples: a car must have wheels, a smartphone must have a battery, a website must have a search function.\n\nOne-dimensional features are directly proportional to customer satisfaction. The more these features are present in a product, the more satisfied customers will be. These features are usually explicitly stated by customers and are easy to measure and quantify.\n\nExamples: a car with leather seats, a smartphone with a high-resolution camera, a website with fast loading times.\n\nAttractive features are unexpected features that delight customers. These features are not explicitly requested by customers but can create a positive emotional response when present. Attractive Quality features can differentiate a product from its competitors and create a competitive advantage.\n\nExamples: a car with a built-in navigation system, a smartphone with facial recognition technology, a website with personalized recommendations.\n\nIndifferent features are neither good nor bad from the customer’s perspective. Customers are indifferent to these features, and their presence or absence does not significantly impact customer satisfaction. These features are often considered “nice to have” but not essential.\n\nExamples: a car with cup holders, a smartphone with a stylus, a website with social media integration.\n\nReverse features are features that, when present, can lead to customer dissatisfaction. These features may be perceived as unnecessary or even annoying by customers. It is essential to identify and eliminate Reverse Quality features to prevent negative customer experiences.\n\nExamples: a car with uncomfortable seats, a smartphone with a short battery life, a website with intrusive pop-up ads."
  },
  {
    "objectID": "posts/20240805-kano-model/index.html#prioritizing-features-with-the-kano-model",
    "href": "posts/20240805-kano-model/index.html#prioritizing-features-with-the-kano-model",
    "title": "Kano Method for Prioritization of Features",
    "section": "Prioritizing Features with the Kano Model",
    "text": "Prioritizing Features with the Kano Model\n\n\n\n\n\n\nFigure 1: Example of a Kano diagram.\n\n\n\nWith the Kano model, prioritization of features and functionalities becomes clear and straightforward as that:\na) keep eye on the Must-be Quality features, as they are essential, b) incorporate One-dimensional Quality features to increase customer satisfaction, c) consider Attractive Quality features to create a competitive advantage; d) eliminate Reverse Quality features, and e) save resources by setting Indifferent Quality features as low priority."
  },
  {
    "objectID": "posts/20240805-kano-model/index.html#implementation-of-the-kano-analysis",
    "href": "posts/20240805-kano-model/index.html#implementation-of-the-kano-analysis",
    "title": "Kano Method for Prioritization of Features",
    "section": "Implementation of the Kano Analysis",
    "text": "Implementation of the Kano Analysis\nImplementing the Kano model involves a sequence of steps, beginning with the development of a questionnaire. For each feature, two types of questions are posed: functional and dysfunctional.\n\nThe functional question assesses respondents’ feelings when a feature is present.\nThe dysfunctional question gauges their reactions in the absence of that feature.\n\nEach question offers five possible responses, from “I like it” to “I dislike it.” Subsequently, these responses are classified into the five Kano categories.\n\n\n\n\nTable 1: Classification of answers to the Kano questionnaire.\n\n\n\n\n\n\n\n\n\n\nCategory\n\n\nDysfunctional\n1) I like it\n2) I expect it\n3) I am neutral\n4) I can tolerate it\n5) I dislike it\n\n\nFunctional\n\n\n\n\n\n\n\n\n\n1) I like it\nQuestionable\nAttractive\nAttractive\nAttractive\nOne-dimensional\n\n\n2) I expect it\nReverse\nIndifferent\nIndifferent\nIndifferent\nMust-be\n\n\n3) I am neutral\nReverse\nIndifferent\nIndifferent\nIndifferent\nMust-be\n\n\n4) I can tolerate it\nReverse\nIndifferent\nIndifferent\nIndifferent\nMust-be\n\n\n5) I dislike it\nReverse\nReverse\nReverse\nReverse\nQuestionable\n\n\n\n\n\n\n\n\n\n\nAfter the classification of responses, the next step is to calculate the satisfaction and dissatisfaction scores for each feature. The satisfaction influence score is calculated as the percentage of Attractive and One-dimensional responses relative to the total number of responses.\n\\[ \\text{Satisfaction Influence} = \\dfrac{A + O}{ A + O + M + I } \\times 100 \\%  \\tag{1}\\] The dissatisfaction influence score is calculated as the percentage of One-dimensional and Must-be responses relative to the total number of responses.\n\\[ \\text{Dissatisfaction Influence} =  - \\dfrac{O + M}{A + O + M + I} \\times 100 \\%  \\tag{2}\\]\nThe features are then plotted on a Kano diagram, with the dissatisfaction score on the x-axis and the satisfaction score on the y-axis. The features are categorized based on their position in the diagram: Attractive Quality features in the upper left quadrant, One-dimensional Quality features in the upper right quadrant, Must-be Quality features in the lower right quadrant, and Indifferent features in the lower left quadrant, as depicted in the Figure 1."
  },
  {
    "objectID": "posts/20240805-kano-model/index.html#example-implementation-of-the-kano-analysis",
    "href": "posts/20240805-kano-model/index.html#example-implementation-of-the-kano-analysis",
    "title": "Kano Method for Prioritization of Features",
    "section": "Example implementation of the Kano Analysis",
    "text": "Example implementation of the Kano Analysis\n\nConducting a Kano Survey\nFor illustrative purposes, let’s consider existing dataset with responses to a Kano questionnaire from Doing Research Online: The Kano Model project by Alex Reppel published on GitHub under the GPL-3.0 License. The dataset consists of five csv files containing responses to functional and dysfunctional questions for various features, along with demographic information about the respondents.\n\n\nExploratory Data Analysis\nLet’s explore data. The dataframe of shape (721, 39) includes an ID column, multiple columns with demographic data such as Income_us, Gender, Age, Employment, and Education, as well as responses to functional and dysfunctional questions (F1_functional, F1_dysfunctional, etc), and columns indicating the importance of certain features to the customer (F1_importance, F2_importance, etc).\n\n\n\n\n\n\n\n\nFigure 2: Histogram of respondents’ age\n\n\n\n\n\nThe customers’ age distribution is relatively balanced, with a slight skew towards younger respondents.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Income distribution\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Employment distribution\n\n\n\n\n\n\nThe income distribution is expectedly skewed to the left. The employment distribution shows that the majority of respondents are employed full-time.\nWhile customer responses might vary based on demographic data, Kano analysis does not consider the demographic characteristics of the respondents.\n\n\nAggregating Responses\nNext, we will aggregate the responses to functional and dysfunctional questions for each feature. The table below shows an example of aggregated answers for a feature with ID = F1.\n\n\n\n\nTable 2: Example of aggregated answers for a feature.\n\n\n\n\n\n\n\n\n\n\nDysfunctional\n1) I like it\n2) I expect it\n3) I am neutral\n4) I can tolerate it\n5) I dislike it\n\n\nID\nFunctional\n\n\n\n\n\n\n\n\n\nF1_\n1) I like it\n12\n10\n21\n25\n9\n\n\n2) I expect it\n6\n6\n14\n10\n3\n\n\n3) I am neutral\n15\n25\n72\n29\n2\n\n\n4) I can tolerate it\n5\n13\n12\n10\n5\n\n\n5) I dislike it\n14\n8\n4\n2\n1\n\n\n\n\n\n\n\n\n\n\n\n\nCalculating Satisfaction and Dissatisfaction Scores\nAfter aggregating the responses, we calculate the satisfaction and dissatisfaction scores for each feature using Equation 1 and Equation 2. The table below shows the qualities: Attractive (A), Indifferent (I), Must-be (M), One-dimensional (O), Questionable (Q), Reverse (R), as well as satisfaction (S) and dissatisfaction (D) scores for each feature.\n\n\n\n\nTable 3: Qualities and satisfaction and dissatisfaction scores for each feature.\n\n\n\n\n\n\n\n\n \nID\nQuestion\nA\nI\nM\nO\nQ\nR\nS\nD\n\n\n\n\n0\nF1\nIf your funds are stored in a way that does not have to be linked to your identity, how do you feel?\n17\n57\n3\n3\n4\n16\n24\n-7\n\n\n1\nF2\nIf it is easy to store funds, how do you feel?\n22\n27\n14\n30\n3\n4\n56\n-48\n\n\n2\nF3\nIf you can access your funds wherever and whenever you want, how do you feel?\n15\n18\n14\n47\n3\n3\n66\n-65\n\n\n3\nF4\nIf it is guaranteed that no one else can access your funds without your permission, how do you feel?\n5\n12\n22\n53\n4\n4\n63\n-82\n\n\n4\nF5\nIf relevant information is always easy to find, how do you feel?\n22\n34\n12\n21\n5\n6\n49\n-37\n\n\n5\nF6\nIf you can transfer funds without having to link that transaction to your name, how do you feel?\n26\n50\n2\n5\n4\n14\n38\n-8\n\n\n6\nF7\nIf it is easy to transfer funds, how do you feel?\n20\n24\n17\n34\n4\n2\n57\n-54\n\n\n7\nF8\nIf you can transfer your funds wherever and whenever you want, how do you feel?\n21\n20\n18\n35\n4\n2\n60\n-56\n\n\n8\nF9\nIf funds are transferred almost instantaneous, how do you feel?\n40\n23\n7\n22\n4\n3\n67\n-31\n\n\n9\nF10\nIf it is guaranteed that no one else can manipulate transfers you have initiated, how do you feel?\n5\n15\n25\n49\n4\n2\n58\n-79\n\n\n10\nF11\nIf relevant information on how to make transfers is always easy to find, how do you feel?\n24\n30\n10\n26\n6\n4\n56\n-40\n\n\n\n\n\n\n\n\n\n\nPlotting the Kano Diagram\nThe last step is to plot the features on a Kano diagram. The quadrant in which the feature is located indicates a Kano category. The further the from the center, the higher the influence on satisfaction or dissatisfaction.\n\n\n\n\n\n\n\n\nFigure 5: Kano diagram"
  },
  {
    "objectID": "posts/20240805-kano-model/index.html#application-of-the-kano-model",
    "href": "posts/20240805-kano-model/index.html#application-of-the-kano-model",
    "title": "Kano Method for Prioritization of Features",
    "section": "Application of the Kano Model",
    "text": "Application of the Kano Model\nThe Kano model can be applied in product development to prioritize features and functionalities based on customer needs and expectations. By categorizing features into the five Kano categories, product managers can identify which features are essential, which are nice to have, and which can create a competitive advantage.\nThe Kano model can also help product managers understand customer preferences and make informed decisions about resource allocation and product development. By focusing on Must-be Quality and One-dimensional Quality features, product managers can ensure that the product meets basic customer requirements and maximizes customer satisfaction.\nIn conclusion, the Kano model is a valuable tool for prioritizing features and functionalities in product development. By understanding customer preferences and categorizing features into the five Kano categories, product managers can create products that meet customer needs and expectations, leading to higher customer satisfaction and competitive advantage."
  },
  {
    "objectID": "posts/20250114-airflow-remote-docker/index.html",
    "href": "posts/20250114-airflow-remote-docker/index.html",
    "title": "Run Docker Containers Remotely with Airflow",
    "section": "",
    "text": "Airflow is a powerful tool for automating workflows. Once you start using it, you’ll likely find it a great replacement for cron jobs on Linux machines. One common use case is running Docker containers on a remote machine—a valuable feature for executing data processing tasks or deploying applications.\n\n\n\nApache AirFlow Logo\n\n\nSetting up Airflow to run Docker containers on a remote machine is a straightforward process. While many guides are available online, they often lack complete and up-to-date instructions. Based on my experience, I decided to write this guide. Let’s walk through the necessary steps to get it running."
  },
  {
    "objectID": "posts/20250114-airflow-remote-docker/index.html#introduction",
    "href": "posts/20250114-airflow-remote-docker/index.html#introduction",
    "title": "Run Docker Containers Remotely with Airflow",
    "section": "",
    "text": "Airflow is a powerful tool for automating workflows. Once you start using it, you’ll likely find it a great replacement for cron jobs on Linux machines. One common use case is running Docker containers on a remote machine—a valuable feature for executing data processing tasks or deploying applications.\n\n\n\nApache AirFlow Logo\n\n\nSetting up Airflow to run Docker containers on a remote machine is a straightforward process. While many guides are available online, they often lack complete and up-to-date instructions. Based on my experience, I decided to write this guide. Let’s walk through the necessary steps to get it running."
  },
  {
    "objectID": "posts/20250114-airflow-remote-docker/index.html#remote-host-setup",
    "href": "posts/20250114-airflow-remote-docker/index.html#remote-host-setup",
    "title": "Run Docker Containers Remotely with Airflow",
    "section": "Remote Host Setup",
    "text": "Remote Host Setup\nAssuming you have a remote Linux machine with Docker installed, you will need to configure it to allow remote access through ssh.\n\n1. Create a Dedicated User\nFor the security reasons, it is recommended to use a dedicated user for running remote commands. You can create a new user like this:\nsudo useradd -m airflow\nsudo usermod -aG docker airflow\nThe last command will add the user to the docker group, allowing it to run Docker commands without sudo.\nNote, that if you will run sudo as the airflow user, you will need to add the user to the sudoers file. You can do this by adding the /etc/sudoers.d/airflow file with the following content:\nairflow ALL=(ALL) NOPASSWD: /path/to/command\n\n\n2. Configure SSH Server\nNext, you will need to configure the ssh server to allow remote access. You can do this by editing the /etc/ssh/sshd_config file and adding/editing the following line:\nAllowUsers airflow\nPasswordAuthentication no\nPubkeyAuthentication yes\nThis will allow only the airflow user to access the server through ssh and disable password authentication, requiring the use of ssh keys instead.\n\n\n3. Set Up SSH Keys\nYou will also need to generate an ssh key pair on your Airflow machine and copy the public key to the remote machine. You can do this with the following commands (assuming you are logged in as the user that will run Airflow):\nssh-keygen\ncat ~/.ssh/id_rsa.pub\nThen copy the output of the second command to the ~/.ssh/authorized_keys file on the remote machine:\nmkdir -p /home/airflow/.ssh\necho \"your_public_key\" &gt;&gt; /home/airflow/.ssh/authorized_keys\nMake sure that the .ssh folder and authorized_keys file have the correct permissions:\nchown airflow:airflow /home/airflow/.ssh/authorized_keys\nchown -R airflow:airflow /home/airflow/.ssh\nchmod 600 /home/airflow/.ssh/authorized_keys\nchmod 700 /home/airflow/.ssh\nTest the ssh connection by running the following command from your Airflow machine:\nssh airflow@your_remote_machine_ip\nIf everything is set up correctly, you should be able to connect to the remote machine without being prompted for a password."
  },
  {
    "objectID": "posts/20250114-airflow-remote-docker/index.html#airflow-setup",
    "href": "posts/20250114-airflow-remote-docker/index.html#airflow-setup",
    "title": "Run Docker Containers Remotely with Airflow",
    "section": "Airflow Setup",
    "text": "Airflow Setup\nNow that the remote machine is set up, you can add the task to your Airflow DAG. You will need to use the SSHOperator to run the Docker command on the remote machine. Here is an example of how to do this.\n\n1. Install the required package\nYou will need to install the apache-airflow-providers-ssh package to use the SSHOperator. You can do this by running the following command (in your Airflow environment):\npip install apache-airflow-providers-ssh\n\n\n2. Create the connection\nIn the Airflow UI, go to Admin -&gt; Connections and create a new connection with the following settings:\n\nConnection Id: remote_docker\nConn Type: SSH\nHost: your_remote_machine_ip\nUsername: airflow\nPassword: (leave it blank)\nExtra: {\"key_file\": \"/path/to/your/private/key\"}\n\n\n\n3. Create the DAG\nNow you can create a new DAG that will run the Docker command on the remote machine. Here is an example of how to do this:\n\nfrom airflow.models.dag import DAG\nfrom airflow.providers.ssh.operators.ssh import SSHOperator\nimport pendulum\n\n\nwith DAG(\n    dag_id=\"remote_docker_tasks\",\n    description=\"Runs Docker commands on a remote machine\",\n    schedule='@daily',\n    start_date=pendulum.datetime(2025, 1, 1, 0, 0, 0, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"docker\", \"remote\"],\n) as dag:\n\n    hello_world = SSHOperator(\n        task_id=\"hello_world\",\n        ssh_conn_id=\"remote_docker\",\n        command=\"docker run --rm hello-world\",\n    )\n\n    hello_world\n    \nThis DAG will run the hello-world Docker container on the remote machine every day at midnight. You can modify the command parameter to run any Docker command you want."
  },
  {
    "objectID": "posts/20250114-airflow-remote-docker/index.html#conclusion",
    "href": "posts/20250114-airflow-remote-docker/index.html#conclusion",
    "title": "Run Docker Containers Remotely with Airflow",
    "section": "Conclusion",
    "text": "Conclusion\nIn this guide, I have shown you how to set up Airflow to run Docker containers on a remote machine. I’ll be happy if it saves you a couple of hours of your time. If you have any questions or suggestions, feel free to leave a comment under the LinkedIn article."
  },
  {
    "objectID": "posts/20250321-rag/index.html",
    "href": "posts/20250321-rag/index.html",
    "title": "Implementing a Local Retrieval-Augmented Generation System",
    "section": "",
    "text": "RAG (Retrieval-Augmented Generation) is a technology that combines the search external sources and generation using large language models. It allows for more accurate and informative responses by leveraging the information retrieved from a corpus of text to generate contextually relevant answers. RAG is particularly useful for question-answering tasks where the answer requires external knowledge beyond what is present in the training data of the language model."
  },
  {
    "objectID": "posts/20250321-rag/index.html#what-is-rag",
    "href": "posts/20250321-rag/index.html#what-is-rag",
    "title": "Implementing a Local Retrieval-Augmented Generation System",
    "section": "",
    "text": "RAG (Retrieval-Augmented Generation) is a technology that combines the search external sources and generation using large language models. It allows for more accurate and informative responses by leveraging the information retrieved from a corpus of text to generate contextually relevant answers. RAG is particularly useful for question-answering tasks where the answer requires external knowledge beyond what is present in the training data of the language model."
  },
  {
    "objectID": "posts/20250321-rag/index.html#how-does-rag-work",
    "href": "posts/20250321-rag/index.html#how-does-rag-work",
    "title": "Implementing a Local Retrieval-Augmented Generation System",
    "section": "How does RAG work?",
    "text": "How does RAG work?\nRAG consists of two main components: a retriever and a generator. The retriever is responsible for searching a corpus of text to find relevant information based on the input query. The generator then uses the retrieved information to generate a response. By combining these two components, RAG can produce more informative and contextually relevant responses compared to traditional language models.\n\n\n\n\n\n\n---\nconfig:\n  theme: default\n  look: classic\n---\nflowchart LR\n    A[\"Reference&lt;br&gt;documents&lt;br&gt;📄📄📄\"] --&gt; n1[\"Embedding&lt;br&gt;🔢🔢🔢\"]\n    n1 --&gt; n3[\"Vector database&lt;br&gt;🗄️🗄️🗄️\"]\n    n3 --&gt; n4[\"Retriever&lt;br&gt;🔎\"]\n    n5[\"User query&lt;br&gt;👤💬\"] --&gt; n6[\"Embedding&lt;br&gt;🔢🔢🔢\"]\n    n5 -- 💬 --&gt; n8[\"Augmented&lt;br&gt;query 📄💬&lt;br&gt;\"]\n    n6 --&gt; n4\n    n8 --&gt; n9[\"Large Language&lt;br&gt;Model 🧠\"]\n    n9 --&gt; n10[\"Response&lt;br&gt;🗨️\"]\n    n4 -- 📄 --&gt; n8\n\n\n\n\n\nFigure 1: RAG process overview"
  },
  {
    "objectID": "posts/20250321-rag/index.html#how-to-implement-a-rag-system",
    "href": "posts/20250321-rag/index.html#how-to-implement-a-rag-system",
    "title": "Implementing a Local Retrieval-Augmented Generation System",
    "section": "How to Implement a RAG System",
    "text": "How to Implement a RAG System\nIn this tutorial, we will implement a simple RAG system using a large language model (LLM) and a text retrieval system. We will use the Hugging Face Transformers library to load a pre-trained LLM and the Faiss library to build a vector database for text retrieval. We will then combine these components to create a question-answering system that retrieves relevant information from a corpus of text and generates responses based on the retrieved information.\nLet’s get started by installing the necessary libraries and setting up the components for our RAG system.\n!pip install bitsandbytes faiss-cpu langchain langchain_community \\\nlangchain_huggingface sentence-transformers --quiet\nimport os\nimport requests\nimport json\nimport numpy as np\nimport pickle\nfrom bs4 import BeautifulSoup, SoupStrainer\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import pipeline\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain_core.documents import Document\nfrom langchain.vectorstores.faiss import FAISS\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nData Collection\nWe will parse the Wikipedia category page to extract links to articles.\nfrom urllib.parse import urlparse\n\ndef fetch_links(url):\n    links = []\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    domain = urlparse(url).netloc\n\n    for ul in soup.find_all('ul'):\n        for li in ul.find_all('li'):\n            link = li.find('a')\n            if link and \"href\" in link.attrs:\n                href = link.attrs[\"href\"]\n                if \"/wiki\" in href[:5]:\n                    links.append(f\"https://{domain}{href}\")\n\n    return links\nSet the url variable and get the links.\nurl = 'https://en.wikipedia.org/wiki/Category:Machine_learning_algorithms'\nlinks = fetch_links(url)\nNext, we will download articles as the docs.\nos.environ[\"USER_AGENT\"] = (\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n    \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n    \"Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0\"\n)\n\nloader = WebBaseLoader(\n    links[20:],\n    bs_kwargs={\n        \"parse_only\": SoupStrainer(\"div\", {\"class\": \"mw-body-content\"}),\n    },\n    bs_get_text_kwargs={\"separator\": \" \", \"strip\": True},\n)\ndocs = loader.load()\n\n\nText Splitting and Embedding\nHere we break documents into shorter chunks—overlapping parts that should be provided to the LLM as context.\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nsplit_docs = text_splitter.split_documents(docs)\nWe need to perform quick search for relevant information, so let’s transform texts to embeddings and load them into vector database.\nfrom langchain_huggingface import HuggingFaceEmbeddings\n\nmodel_name = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\nmodel_kwargs = {\"device\": \"cuda\"}\nencode_kwargs = {\"normalize_embeddings\": False}\nembedding = HuggingFaceEmbeddings(\n    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n)\nvector_store = FAISS.from_documents(split_docs, embedding=embedding)\n\n\nQuestion-Answering Pipeline\nHere we define function for retrieving relevant documents from the database.\ndef retrieve(query, top_k=2):\n    documents = vector_store.search(query, \"similarity\")\n    return documents[:top_k]\nWe will use local LLM. Let’s authorize on HuggingFace which is mandatory for downloading certain models.\nfrom huggingface_hub import login\nfrom google.colab import userdata\n\nlogin(token = userdata.get(\"HF_TOKEN\"))\nDownload model, define tokenizer and create config.\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nMODEL_NAME = \"Qwen/Qwen2.5-7B\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    load_in_8bit=True,\n    torch_dtype=torch.bfloat16,\n    device_map=\"cuda\",\n)\nmodel.eval()\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nLet’s put context retrieval and generation pipeline into a function.\ngen_pipeline = pipeline(\n    \"text-generation\", model=model, tokenizer=tokenizer, return_full_text=False\n)\n\n\ndef generate_response(query):\n    relevant_texts = retrieve(query)\n    context = \" \".join([t.model_dump()[\"page_content\"] for t in relevant_texts])\n    prompt = f\"\"\"Answer question using only information provided in the context.\n    If the context contains no relevant information, say \"I couldn't find the information\".\n    Context: '''{context}'''\n    Question: {query}\n    Answer:\n    \"\"\"\n    response = gen_pipeline(prompt)\n    return response[0][\"generated_text\"]"
  },
  {
    "objectID": "posts/20250321-rag/index.html#testing-the-rag-system",
    "href": "posts/20250321-rag/index.html#testing-the-rag-system",
    "title": "Implementing a Local Retrieval-Augmented Generation System",
    "section": "Testing the RAG System",
    "text": "Testing the RAG System\nHere starts our Q&A session.\nquery = \"What is the Actor-critic algorithm in reinforcement learning?\"\n\nanswer = generate_response(query)\nprint(answer)\nThe actor-critic algorithm (AC) is a family of reinforcement learning (RL) algorithms that combine policy-based RL algorithms such as policy gradient methods, and value-based RL algorithms such as value iteration, Q-learning, SARSA, and TD learning. An AC algorithm consists of two main components: an \"actor\" that determines which actions to take according to a policy function, and a \"critic\" that evaluates those actions according to a value function. Some AC algorithms are on-policy, some are off-policy. Some apply to either continuous or discrete action spaces. Some work in both cases.\nquery = \"What is the purpose of backpropagation in neural networks?\"\n\nanswer = generate_response(query)\nprint(answer)\nThe purpose of backpropagation in neural networks is to adjust the weights of the connections between neurons in order to minimize the error between the predicted output and the actual output. This is done by propagating the error backwards through the network, starting from the output layer and moving towards the input layer, hence the name \"backpropagation.\" The goal is to find the optimal set of weights that will allow the network to make accurate predictions on new, unseen data.\nquery = \"Explain the concept of Curriculum learning in machine learning.\"\n\nanswer = generate_response(query)\nprint(answer)\nCurriculum learning in machine learning is a technique that involves gradually introducing more complex concepts or data to a model as it learns. This approach is inspired by the way humans learn, starting with simple concepts and building upon them. In the context provided, it is mentioned that this technique has its roots in the early study of neural networks, particularly in Jeffrey Elman's 1993 paper. Bengio et al. demonstrated successful application of curriculum learning in image classification tasks, such as identifying geometric shapes with increasingly complex forms, and language modeling tasks, such as training with a gradually expanding vocabulary. The authors conclude that curriculum strategies can be beneficial for machine learning models, especially when dealing with complex or large-scale problems.\nquery = \"How does K-nearest neighbors (K-NN) algorithm classify data?\"\n\nanswer = generate_response(query)\nprint(answer)\nThe K-nearest neighbors (K-NN) algorithm classifies data by a plurality vote of its neighbors, with the object being assigned to the class most common among its K nearest neighbors (K is a positive integer, typically small). If K = 1, then the object is simply assigned to the class of that single nearest neighbor.\nquery = \"What is Federated Learning of Cohorts and how does it improve data privacy?\"\n\nanswer = generate_response(query)\nprint(answer)\nFederated Learning of Cohorts (FLoC) is a type of web tracking that groups people into \"cohorts\" based on their browsing history for the purpose of interest-based advertising. It was being developed as a part of Google's Privacy Sandbox initiative, which includes several other advertising-related technologies with bird-themed names. FLoC was being tested in Chrome 89 as a replacement for third-party cookies. Despite \"federated learning\" in the name, FLoC does not utilize any federated learning. FLoC improves data privacy by grouping people into cohorts based on their browsing history, rather than tracking individual users. This means that advertisers can still target users based on their interests, but without the need for individual user data.\nLooks good. Let’s ask something that is not in the context. For instance, there was no articles on Transformer architecture among wiki articles.\n\nOut-of-Context Questions\nquery = \"How does the Transformer architecture improve upon traditional RNNs and LSTMs in NLP tasks?\"\n\nanswer = generate_response(query)\nprint(answer)\nThe Transformer architecture improves upon traditional RNNs and LSTMs in NLP tasks by using self-attention mechanisms to capture long-range dependencies between words in a sentence. This allows the model to process entire sentences at once, rather than sequentially like RNNs and LSTMs. Additionally, the Transformer architecture uses a fixed-size attention mechanism, which makes it more efficient and scalable than RNNs and LSTMs.\nThat’s interesting. To be sure that there’s no information on this topic, let’s check context.\nretrieve(query)\n[\n    Document(\n        id='a2ae5aee-3b78-4804-a983-25d08fb8f5d3', \n        metadata={'source': 'https://en.wikipedia.org/wiki/Loss_functions_for_classification'}, \n        page_content='Andrew Ng Fei-Fei Li Alex Krizhevsky Ilya Sutskever Demis Hassabis David Silver Ian Goodfellow Andrej Karpathy Architectures Neural Turing machine Differentiable neural computer Transformer Vision transformer (ViT) Recurrent neural network (RNN) Long short-term memory (LSTM) Gated recurrent unit (GRU) Echo state network Multilayer perceptron (MLP) Convolutional neural network (CNN) Residual neural network (RNN) Highway network Mamba Autoencoder Variational autoencoder (VAE) Generative adversarial network (GAN) Graph neural network (GNN) Portals Technology Category Artificial neural networks Machine learning List Companies Projects Retrieved from \" https://en.wikipedia.org/w/index.php?title=Loss_functions_for_classification&oldid=1261562183 \"'\n    ),\n    Document(\n        id='b267b523-9330-4b33-bc3a-b4e6edec109f',\n        metadata={'source': 'https://en.wikipedia.org/wiki/Policy_gradient_method'}, \n        page_content='neural computer Transformer Vision transformer (ViT) Recurrent neural network (RNN) Long short-term memory (LSTM) Gated recurrent unit (GRU) Echo state network Multilayer perceptron (MLP) Convolutional neural network (CNN) Residual neural network (RNN) Highway network Mamba Autoencoder Variational autoencoder (VAE) Generative adversarial network (GAN) Graph neural network (GNN) Portals Technology Category Artificial neural networks Machine learning List Companies Projects Retrieved from \" https://en.wikipedia.org/w/index.php?title=Policy_gradient_method&oldid=1280215280 \"'\n    )\n]\nIt appears that the query retrieved random parts of pages mentioning transformers. However, as they contained no valuable information, the answer was fully generated by the LLM. Although the response was accurate, we may want to enhance the retrieval function by setting a threshold for relevancy to minimize the risk of hallucinations.\nLet’s ask a question from the completely different domain.\nquery = \"How does the process of photosynthesis work in plants?\"\n\nanswer = generate_response(query)\nprint(answer)\nI couldn't find the information.\nThis question left unanswered. What about another one?\nquery = \"How does blockchain technology ensure security and decentralization?\"\n\nanswer = generate_response(query)\nprint(answer)\nBlockchain technology ensures security and decentralization through its decentralized nature and cryptographic algorithms. It operates on a distributed network of nodes, where each node maintains a copy of the entire blockchain. This means that no single entity has control over the entire system, making it resistant to tampering and censorship. Additionally, blockchain uses cryptographic algorithms to secure transactions and data, ensuring that only authorized parties can access and modify the information. This combination of decentralization and cryptographic security makes blockchain technology highly secure and decentralized.\nUnexpectedly, one of the documents contained information on this topic, so the answer was generated based on the retrieved context.\nretrieve(query)\n[\n    Document(\n        id='3d968d3b-1889-4435-b329-c9081400e8c4', \n        metadata={'source': 'https://en.wikipedia.org/wiki/Augmented_Analytics'}, \n        page_content='to democratising data: Data Parameterisation and Characterisation. Data Decentralisation using an OS of blockchain and DLT technologies, as well as an independently governed secure data exchange to enable trust. Consent Market-driven Data Monetisation. When it comes to connecting assets, there are two features that will accelerate the adoption and usage of data democratisation: decentralized identity management and business data object monetization of data ownership. It enables multiple individuals and organizations to identify, authenticate, and authorize participants and organizations, enabling them to access services, data or systems across multiple networks, organizations, environments, and use cases. It empowers users and enables a personalized, self-service digital onboarding system so that users can self-authenticate without relying on a central administration function to process their information. Simultaneously, decentralized identity management ensures the user is authorized'\n    ),\n    Document(\n    id='98578608-2a8d-4533-a655-b556202dda7d', \n    metadata={'source': 'https://en.wikipedia.org/wiki/Augmented_Analytics'}, \n    page_content='so that users can self-authenticate without relying on a central administration function to process their information. Simultaneously, decentralized identity management ensures the user is authorized to perform actions subject to the system’s policies based on their attributes (role, department, organization, etc.) and/ or physical location. [ 10 ] Use cases [ edit ] Agriculture  – Farmers collect data on water use, soil temperature, moisture content and crop growth, augmented analytics can be used to make sense of this data and possibly identify insights that the user can then use to make business decisions. [ 11 ] Smart Cities  – Many cities across the United States, known as Smart Cities collect large amounts of data on a daily basis. Augmented analytics can be used to simplify this data in order to increase effectiveness in city management (transportation, natural disasters, etc.). [ 11 ] Analytic Dashboards  – Augmented analytics has the ability to take large data sets and create'\n    )\n]\nOne more question from another domain.\nquery = \"What are the fundamental principles of classical mechanics?\"\n\nanswer = generate_response(query)\nprint(answer)\nI couldn't find the information.\nThis question was left unanswered as expected."
  },
  {
    "objectID": "posts/20250321-rag/index.html#conclusion",
    "href": "posts/20250321-rag/index.html#conclusion",
    "title": "Implementing a Local Retrieval-Augmented Generation System",
    "section": "Conclusion",
    "text": "Conclusion\nIn this tutorial, we implemented a simple RAG system using a large language model and a text retrieval system. We collected articles from Wikipedia, split them into shorter chunks, and transformed them into embeddings for quick search. We then used a local LLM to generate responses based on the retrieved information. The RAG system successfully answered questions related to machine learning algorithms and reinforcement learning. The RAG system demonstrates the potential of combining retrieval and generation techniques to produce informative and contextually relevant answers.\nSource code for this tutorial is available on GitHub."
  },
  {
    "objectID": "posts/20250106-bi-flowchart/index.html",
    "href": "posts/20250106-bi-flowchart/index.html",
    "title": "BI System Blueprint",
    "section": "",
    "text": "In this post, I’d like to present a blueprint for a BI system that I’ve been working on. The system is designed to provide insights into the company’s performance and support data-driven decision-making."
  },
  {
    "objectID": "posts/20250106-bi-flowchart/index.html#intro",
    "href": "posts/20250106-bi-flowchart/index.html#intro",
    "title": "BI System Blueprint",
    "section": "",
    "text": "In this post, I’d like to present a blueprint for a BI system that I’ve been working on. The system is designed to provide insights into the company’s performance and support data-driven decision-making."
  },
  {
    "objectID": "posts/20250106-bi-flowchart/index.html#architecture",
    "href": "posts/20250106-bi-flowchart/index.html#architecture",
    "title": "BI System Blueprint",
    "section": "Architecture",
    "text": "Architecture\nThe BI system primarily revolves around Microsoft Power BI, which serves as the key instrument for creating dozens of reports with hundreds of indicators. Data is sourced and structured using various tools and pipelines, with Google BigQuery acting as the central data warehouse.\nThe system consists of several modules that work together to deliver actionable insights to end-users. The flowchart below illustrates the main components of the system and their interactions.\n\n\n\n\n\n\n---\nconfig:\n  theme: default\n  look: classic\n---\nflowchart BT\n subgraph s5[\"Google BigQuery\"]\n        n51[\"Views\"]\n        n52[\"Tables\"]\n  end\n subgraph s6[\"Extract and transform\"]\n        n11[\"ETL scripts\"]\n        n17[\"dbt\"]\n  end\n subgraph s7[\"Data sources\"]\n        n61[\"DBs\"]\n        n62[\"APIs\"]\n        n63[\"Files\"]\n        n64[\"...\"]\n  end\n    n5[\"Airflow\"] -.-&gt; n11 & n17 & s3[\"Power BI\"] & n20[\"ML models\"]\n    n52 -- Data --&gt; n20 & n51\n    n51 -- Data --&gt; s3\n    n11 -- Table data --&gt; n52\n    n17 -- Models --&gt; n51\n    n20 -- Forecasts --&gt; n52\n    s7 -- Data --&gt; n11\n    style s5 fill:transparent\n    style s6 fill:transparent\n    style s7 fill:transparent\n\n\n\n\nFigure 1: Data and control flows in the BI system"
  },
  {
    "objectID": "posts/20250106-bi-flowchart/index.html#modules",
    "href": "posts/20250106-bi-flowchart/index.html#modules",
    "title": "BI System Blueprint",
    "section": "Modules",
    "text": "Modules\n\nAirflow\nThe ETL process is orchestrated by Apache Airflow, an open-source platform for programmatically authoring, scheduling, and monitoring workflows. Some tasks are triggered by the arrival of new data, while others run at predefined intervals.\n\n\nExtract and Transform\nThe first step in the process is collecting data from various sources such as databases, APIs, and files. The collected data is then stored in Google BigQuery.\n\n\n\n\n\n\n---\nconfig:\n  theme: default\n  look: classic\n---\nflowchart BT\n subgraph s7[\"PBX\"]\n        n53[\"Records\"]\n        n12[\"CDR DB\"]\n  end\n subgraph s8[\"ERP\"]\n        n1[\"ERP reporting&lt;br&gt;subsystem\"]\n  end\n    n1 -- CSV files --&gt; n59[\"SMB share\"]\n    n6[\"Helpdesk software\"] -- Tickets --&gt; n11[\"ETL scripts\"]\n    n7[\"PM software\"] -- Tasks --&gt; n11\n    n8[\"CMS\"] -- SKU metadata --&gt; n11\n    n9[\"IMAP\"] -- Messages --&gt; n11\n    n12 -- Calls metadata --&gt; n11\n    n53 -- WAV files --&gt; n54[\"Text to Speech\"]\n    n11 -- Table data --&gt; n57[\"Google BigQuery\"]\n    n54 -- Transcribed&lt;br&gt;records --&gt; n11\n    n58[\"Google Analytics\"] -- Events flow --&gt; n57\n    n59 -- Reports --&gt; n11\n    style s7 fill:transparent\n    style s8 fill:transparent\n\n\n\n\nFigure 2: ETL subsystem\n\n\n\n\n\n\n\ndbt\nWhile the data in BigQuery is already structured, further transformation may be required to optimize it for analysis. dbt (data build tool) facilitates this by allowing you to define transformations, often as SQL views, and execute them in a reproducible manner.\n\n\nML models\nIn addition to traditional BI analytics, the system incorporates machine learning models to predict future outcomes based on historical data. The predicted values are stored in BigQuery and integrated into the reporting process.\n\n\nPower BI\nThe final step is visualizing the data using Power BI, a business analytics tool that provides interactive visualizations and business intelligence capabilities.\nThe data is fetched from BigQuery and stored in Power BI datasets. Although a direct connection to BigQuery is available, importing the data is necessary to fully leverage the DAX language for creating complex calculations. Data refreshes are triggered by Airflow DAGs after the ETL tasks are completed.\n\n\n\n\n\n\n---\nconfig:\n  theme: default\n  look: classic\n---\nflowchart BT\n subgraph s3[\"Datasets\"]\n        n22[\"Sales\"]\n        n23[\"Supply\"]\n        n24[\"Finance\"]\n        n25[\"HR\"]\n  end\n subgraph s4[\"Reports and dashboards\"]\n        n26[\"Report 2\"]\n        n27[\"Report 3\"]\n        n29[\"Report 1\"]\n        n40[\"Dashboard 1\"]\n        n42[\"...\"]\n        n41[\"Report n\"]\n  end\n subgraph s5[\"Google BigQuery\"]\n        n51[\"Views\"]\n  end\n    n51 --&gt; n24 & n22 & n25 & n23\n    n22 --&gt; n26 & n40\n    n23 --&gt; n27 & n40 & n41\n    n24 --&gt; n29\n    n25 --&gt; n40\n    style s3 fill:transparent\n    style s4 fill:transparent\n    style s5 fill:transparent\n\n\n\n\nFigure 3: Power BI reports and dashboards"
  },
  {
    "objectID": "posts/20250106-bi-flowchart/index.html#conclusion",
    "href": "posts/20250106-bi-flowchart/index.html#conclusion",
    "title": "BI System Blueprint",
    "section": "Conclusion",
    "text": "Conclusion\nIn summary, the BI system outlined in this post is purpose-built to deliver deep insights into the company’s performance and empower data-driven decision-making. By integrating tools like ETL pipelines, Airflow for orchestration, dbt for data transformation, and Power BI for advanced analytics and visualization, it establishes a scalable and robust analytics platform tailored to meet evolving business needs.\n\n\n\n\n\n\n---\nconfig:\n  theme: default\n  look: classic\n---\nflowchart BT\n subgraph s3[\"Datasets\"]\n        n22[\"Sales\"]\n        n23[\"Supply\"]\n        n24[\"Finance\"]\n        n25[\"HR\"]\n  end\n subgraph s4[\"Reports and dashboards\"]\n        n26[\"Report 1\"]\n        n27[\"Report 2\"]\n        n29[\"...\"]\n  end\n subgraph s5[\"Google BigQuery\"]\n        n51[\"Views\"]\n        n52[\"Tables\"]\n  end\n subgraph s7[\"PBX\"]\n        n53[\"Records\"]\n        n12[\"CDR DB\"]\n  end\n subgraph s8[\"ERP system\"]\n        n1[\"ERP reporting&lt;br&gt;subsystem\"]\n  end\n    n51 --&gt; s3\n    s3 --&gt; s4\n    n1 -- CSV files --&gt; n55[\"SMB share\"]\n    n6[\"Helpdesk software\"] -- Tickets --&gt; n11[\"ETL scripts\"]\n    n7[\"PM software\"] -- Tasks --&gt; n11\n    n8[\"CMS\"] -- SKU metadata --&gt; n11\n    n9[\"IMAP\"] -- Messages --&gt; n11\n    n12 -- Calls metadata --&gt; n11\n    n11 -- Table data --&gt; n52\n    n17[\"dbt\"] -- Models --&gt; n51\n    n52 -- Data --&gt; n51 & n20[\"ML models\"]\n    n20 -- Forecasts --&gt; n52\n    n53 -- WAV files --&gt; n54[\"Text to Speech\"]\n    n54 -- Transcribed&lt;br&gt;records --&gt; n11\n    n55 -- Reports --&gt; n11\n\n    style s3 fill:transparent\n    style s4 fill:transparent\n    style s5 fill:transparent\n    style s7 fill:transparent\n    style s8 fill:transparent\n\n\n\n\nFigure 4: BI system overview"
  },
  {
    "objectID": "posts/20250214-valentines/index.html",
    "href": "posts/20250214-valentines/index.html",
    "title": "Nerdy Valentine’s in Python, R, and Matlab",
    "section": "",
    "text": "Let’s celebrate Valentine’s Day with some nerdy love! In this post, we will create heart-shaped plots using Python, R, and Matlab. These heart-shaped plots are a fun and creative way to express your love for programming and data visualization. Let’s get started!"
  },
  {
    "objectID": "posts/20250214-valentines/index.html#python",
    "href": "posts/20250214-valentines/index.html#python",
    "title": "Nerdy Valentine’s in Python, R, and Matlab",
    "section": "Python",
    "text": "Python\nHere is a Python code snippet to plot a 3D heart shape using Matplotlib:\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nfrom matplotlib.ticker import LinearLocator, FormatStrFormatter\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef heart_3d(x, y, z):\n    return (\n        (x**2 + (9 / 4) * y**2 + z**2 - 1) ** 3 - x**2 * z**3 - (9 / 80) * y**2 * z**3\n    )\n\n\nbbox = (-1.5, 1.5)\n\nxmin, xmax, ymin, ymax, zmin, zmax = bbox * 3\nfig = plt.figure()\n\nax = fig.add_subplot(111, projection=\"3d\")\nA = np.linspace(xmin, xmax, 100)  # resolution of the contour\nB = np.linspace(xmin, xmax, 40)  # number of slices\nA1, A2 = np.meshgrid(A, A)  # grid on which the contour is plotted\n\nfor z in B:  # plot contours in the XY plane\n    X, Y = A1, A2\n    Z = heart_3d(X, Y, z)\n    cset = ax.contour(X, Y, Z + z, [z], zdir=\"z\", colors=(\"r\",))\n\nfor y in B:  # plot contours in the XZ plane\n    X, Z = A1, A2\n    Y = heart_3d(X, y, Z)\n    cset = ax.contour(X, Y + y, Z, [y], zdir=\"y\", colors=(\"r\",))\n\nfor x in B:  # plot contours in the YZ plane\n    Y, Z = A1, A2\n    X = heart_3d(x, Y, Z)\n    cset = ax.contour(X + x, Y, Z, [x], zdir=\"x\", colors=(\"r\",))\n\n_ = ax.set_zlim3d(zmin, zmax);\n_ = ax.set_xlim3d(xmin, xmax);\n_ = ax.set_ylim3d(ymin, ymax);\n\nax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\nax.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\nax.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n\n\nplt.show()"
  },
  {
    "objectID": "posts/20250214-valentines/index.html#r",
    "href": "posts/20250214-valentines/index.html#r",
    "title": "Nerdy Valentine’s in Python, R, and Matlab",
    "section": "R",
    "text": "R\nIn R, we can use the rgl and misc3d packages to create 3D mesh plots. Here is the R code to plot a heart shape:\n\noptions(rgl.useNULL = TRUE) # Use NULL device if display is not available\n\nlibrary(misc3d)\nlibrary(rgl)\n\nheart_3d &lt;- function(x, y, z) {\n  (x^2 + (9 / 4) * y^2 + z^2 - 1)^3 - x^2 * z^3 - (9 / 80) * y^2 * z^3\n}\n\n# Create a grid of points in 3D space\nx &lt;- seq(-1.5, 1.5, length.out = 50)\ny &lt;- seq(-1.5, 1.5, length.out = 50)\nz &lt;- seq(-1.5, 1.5, length.out = 50)\n\n# Generate 3D grid of function values\ngrid &lt;- expand.grid(x = x, y = y, z = z)\nvalues &lt;- with(grid, heart_3d(x, y, z))\n\n# Reshape to 3D array for contour3d\ndim_values &lt;- c(length(x), length(y), length(z))\nvalues &lt;- array(values, dim = dim_values)\n\n# Create 3D contour plot\ncontour3d(values,\n  level = 0,\n  x = x, y = y, z = z, col.mesh = \"red\", alpha = 0.5, engine = \"grid\"\n)"
  },
  {
    "objectID": "posts/20250214-valentines/index.html#matlab",
    "href": "posts/20250214-valentines/index.html#matlab",
    "title": "Nerdy Valentine’s in Python, R, and Matlab",
    "section": "Matlab",
    "text": "Matlab\nThe code for plotting 3D shape in Matlab is much more laconic than in Python or R. Here is how to plot a heart shape in Matlab:\n\n% volume data\nstep = 0.05;\n[X,Y,Z] = meshgrid(-3:step:3, -3:step:3, -3:step:3);\nF = (-(X.^2).*(Z.^3)-(9/80).*(Y.^2).*(Z.^3))+((X.^2)+(9/4).*(Y.^2)+(Z.^2)-1).^3;\n\n% wireframe\npatch(isosurface(X,Y,Z,F,0), 'FaceColor','w', 'EdgeColor','r')\ndaspect([1 1 1])\nview(3)\naxis tight equal\nset(gcf, 'Color','w')\n\n\n\n\n\nNow you have three beautiful heart-shaped plots created using Python, R, and Matlab. Share these plots with your loved ones and spread the nerdy love this Valentine’s Day!"
  }
]