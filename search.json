[
  {
    "objectID": "posts/20240927-euro-tech-money/index.html",
    "href": "posts/20240927-euro-tech-money/index.html",
    "title": "European Tech Salaries",
    "section": "",
    "text": "Recently, I stumbled upon a Reddit post where someone was gathering salary data from the tech sector throughout Europe. It piqued my interest to explore how these salaries vary among various countries and positions. So, I chose to employ R for data collection and analysis."
  },
  {
    "objectID": "posts/20240927-euro-tech-money/index.html#introduction",
    "href": "posts/20240927-euro-tech-money/index.html#introduction",
    "title": "European Tech Salaries",
    "section": "",
    "text": "Recently, I stumbled upon a Reddit post where someone was gathering salary data from the tech sector throughout Europe. It piqued my interest to explore how these salaries vary among various countries and positions. So, I chose to employ R for data collection and analysis."
  },
  {
    "objectID": "posts/20240927-euro-tech-money/index.html#data-collection",
    "href": "posts/20240927-euro-tech-money/index.html#data-collection",
    "title": "European Tech Salaries",
    "section": "Data Collection",
    "text": "Data Collection\nLet’s start by fetching the data from the Google Sheet.\n\n\nShow the code\n\ndocument &lt;- \"1iTNwiAQ0s5iD6RqI7B30uWqQ8wNJqRnmHvxo5zRffu8\"\nsheet &lt;- \"603717461\"\nurl = sprintf(\n  \"https://docs.google.com/spreadsheets/d/%s/gviz/tq?tqx=out:csv&sheet=%s\",\n  document,\n  sheet\n)\ndownload.file(url, destfile = \"data/euro-tech-money.csv\", mode = \"wb\")\n\nOnce downloaded, we can load the data into our R environment.\n\n\nShow the code\ndata &lt;- read.csv(\"data/euro-tech-money.csv\")\n\n\nThe dataset consists of 572 observations in 12 columns named Job.Title, Company, City, Seniority, Pre.Tax.TC, After.Tax.TC, Yearly.Savings, Lifestyle, Household.Size, Share.of.Household.Expenses, Country, and Timestamp. See summary below.\n\n\nShow summary\n\n\n\n  Job.Title           Company              City            Seniority        \n Length:572         Length:572         Length:572         Length:572        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n   Pre.Tax.TC      After.Tax.TC     Yearly.Savings    Lifestyle        \n Min.   : 13646   Min.   :  10000   Min.   : -2000   Length:572        \n 1st Qu.: 48000   1st Qu.:  32600   1st Qu.:  8000   Class :character  \n Median : 70000   Median :  46500   Median : 17000   Mode  :character  \n Mean   : 85140   Mean   :  58791   Mean   : 23915                     \n 3rd Qu.: 99600   3rd Qu.:  65190   3rd Qu.: 30000                     \n Max.   :700000   Max.   :1575000   Max.   :270000                     \n NA's   :11       NA's   :23        NA's   :68                         \n Household.Size  Share.of.Household.Expenses   Country         \n Min.   :1.000   Min.   :  0.0               Length:572        \n 1st Qu.:1.000   1st Qu.: 65.0               Class :character  \n Median :2.000   Median :100.0               Mode  :character  \n Mean   :1.853   Mean   : 82.3                                 \n 3rd Qu.:2.000   3rd Qu.:100.0                                 \n Max.   :7.000   Max.   :100.0                                 \n NA's   :22      NA's   :55                                    \n  Timestamp        \n Length:572        \n Class :character  \n Mode  :character"
  },
  {
    "objectID": "posts/20240927-euro-tech-money/index.html#geography",
    "href": "posts/20240927-euro-tech-money/index.html#geography",
    "title": "European Tech Salaries",
    "section": "Geography",
    "text": "Geography\nLet’s take a look at the location of respondents. We’re going to load a map of Europe and plot the cities where the respondents are located. The map will also show the number of respondents in each city and their median salary in USD.\n\n\nShow the code\nlibrary(giscoR)\nlibrary(maps)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(ggrepel)\n\n# Load the map of Europe\neurope &lt;- gisco_get_countries(\n  region = \"Europe\",\n  resolution = 1,\n  cache_dir = \"/tmp/giscoR\"\n)\n\n# Get the cities\ncities &lt;- world.cities |&gt;\n  filter(str_to_upper(country.etc) %in% unique(str_to_upper(data$Country))) |&gt;\n  select(name, country.etc, long, lat)\n\n# Change case in the column to title case\ndata &lt;- data |&gt; mutate(City = str_to_title(City))\n\n# Group responses data frame by the city\nby_city &lt;- select(data, City, Pre.Tax.TC) |&gt;\n  group_by(City) |&gt;\n  summarise(\n    resp_count = n(),\n    median_salary = median(Pre.Tax.TC, na.rm = TRUE)\n  )\n# Join the cities and responses data frames\nby &lt;- join_by(name == City)\n\nby_city &lt;- inner_join(cities, by_city, by)\n\np &lt;- by_city |&gt;\n  arrange(resp_count) |&gt;\n  mutate(name = factor(name, unique(name))) |&gt;\n  ggplot() +\n  geom_sf(\n    data = europe,\n    fill = \"grey\",\n    alpha = 0.3\n  ) +\n  geom_point(\n    aes(\n      x = long,\n      y = lat,\n      size = resp_count,\n      color = median_salary\n    ),\n    alpha = 0.9\n  ) +\n  scale_color_viridis_c(\n    trans = \"log\", option = \"plasma\",\n    breaks = c(25000, 50000, 100000, 200000, 400000)\n  ) +\n  theme_void() +\n  ylim(35, 65) +\n  xlim(-15, 40)\n\np1 &lt;- p + theme(\n  legend.position = \"none\",\n  plot.margin = grid::unit(c(50, 50, 50, 50), \"pt\")\n)\n\nggsave(\"image.png\", plot = p1, width = 8, height = 8)\n\np + geom_text_repel(\n  data = by_city |&gt; arrange(resp_count) |&gt; tail(20),\n  aes(x = long, y = lat, label = name),\n  size = 4\n) +\n  theme(\n    legend.position = \"right\",\n    legend.key.height = unit(20, \"pt\"),\n    legend.box.margin = margin(0, 0, 0, 20)\n  )\n\n\n\n\n\n\n\n\nFigure 1: Geography of respondents"
  },
  {
    "objectID": "posts/20240927-euro-tech-money/index.html#salaries-by-city",
    "href": "posts/20240927-euro-tech-money/index.html#salaries-by-city",
    "title": "European Tech Salaries",
    "section": "Salaries by City",
    "text": "Salaries by City\nThe total number of cities is 105. Below is a list of the top 10 and bottom 10 cities with at least 5 respondents, ranked by median salary. While these plots can provide a general idea of the salary distribution, it is not the best idea to compare salaries across cities directly, as the salary may vary depending on other factors like job title or seniority.\n\n\nShow the code\nlibrary(scales)\n\ncities_ranked &lt;- data |&gt;\n  inner_join(cities, by = c(\"City\" = \"name\")) |&gt;\n  group_by(City) |&gt;\n  summarize(median_salary = median(Pre.Tax.TC), resp_count = n()) |&gt;\n  filter(resp_count &gt; 4) |&gt;\n  arrange(desc(median_salary))\n\n\ndata &lt;- data |&gt;\n  mutate(city_rank = ifelse(City %in% cities_ranked$City[1:10], \"Top\", \"Other\"))\ndata &lt;- data |&gt;\n  mutate(city_rank = ifelse(City %in% tail(cities_ranked, 10)$City, \"Tail\", city_rank))\ndata &lt;- data |&gt;\n  mutate(city_rank = factor(city_rank, levels = c(\"Top\", \"Tail\")))\n\nxlim &lt;- c(\n  0,\n  max(data$Pre.Tax.TC)\n)\n\ndata |&gt;\n  inner_join(cities_ranked, by = c(\"City\" = \"City\")) |&gt;\n  filter(city_rank %in% c(\"Top\", \"Tail\")) |&gt;\n  ggplot(aes(\n    x = reorder(factor(City), Pre.Tax.TC, median),\n    y = Pre.Tax.TC\n  )) +\n  facet_wrap(~ as.factor(city_rank), scales = \"free_y\") +\n  geom_boxplot(aes(colour = median_salary)) +\n  scale_color_viridis_c(\n    trans = \"log\",\n    option = \"plasma\",\n    begin = 0.,\n    end = 0.85,\n    breaks = c(25000, 50000, 100000, 200000, 400000)\n  ) +\n  theme_minimal() +\n  coord_cartesian(xlim = xlim) +\n  coord_flip() +\n  labs(\n    x = \"\",\n    y = \"\"\n  ) +\n  theme(legend.position = \"none\") +\n  scale_y_continuous(labels = scales::comma)\n\n\n\n\n\n\n\n\nFigure 2: Top and bottom cities by median salary"
  },
  {
    "objectID": "posts/20240927-euro-tech-money/index.html#salaries-by-country-and-job-title",
    "href": "posts/20240927-euro-tech-money/index.html#salaries-by-country-and-job-title",
    "title": "European Tech Salaries",
    "section": "Salaries by Country and Job Title",
    "text": "Salaries by Country and Job Title\nLet’s visualize the median salary by country. Since salaries vary by position, it’s important to include job titles on the axis. However, with 249 distinct job titles, we need to group them into broader categories for clarity.\nIn the following plot, the size of the point represents the number of respondents, and the color represents the median salary. The text on the plot shows the median salary for each country and job category.\n\n\nShow the code\ndata1 &lt;- data |&gt; inner_join(cities, by = c(\"City\" = \"name\"))\n\ndata1 &lt;- data1 |&gt; left_join(read.csv(\"data/job-categories.csv\"), by = \"Job.Title\")\n\ncountry_job_title &lt;- data1 |&gt;\n  # Convert to long format\n  as_tibble() |&gt;\n  group_by(Country, Job.Category) |&gt;\n  summarize(\n    resp_count = n(),\n    median_salary = median(Pre.Tax.TC)\n  ) |&gt;\n  ungroup()\n\n\ncountry_job_title |&gt;\n  ggplot(aes(x = Job.Category, y = reorder(Country, desc(Country)))) +\n  geom_point(aes(size = resp_count, colour = median_salary), shape = 21, stroke = T, alpha = 0.9) +\n  scale_color_viridis_c(\n    trans = \"log\",\n    option = \"plasma\",\n    breaks = c(25000, 50000, 100000, 200000)\n  ) +\n  scale_size_area(max_size = 8) +\n  geom_text(\n    aes(\n      y = as.numeric(as.factor(reorder(Country, desc(Country)))) - sqrt(resp_count) / 20,\n      label = format(median_salary, big.mark = \",\"),\n    ),\n    vjust = 1.5,\n    hjust = 0.5,\n    colour = \"grey30\",\n    size = 2.5\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    axis.ticks.y = element_blank(),\n    legend.position = \"none\",\n    # legend.position = \"right\",\n    # legend.key.height = unit(20, \"pt\"),\n    # legend.key.width = unit(5, \"pt\"),\n    # legend.box.margin = margin(0, 0, 0, 10),\n    # legend.title = element_blank()\n  ) +\n  labs(x = NULL, y = NULL)\n\n\n\n\n\n\n\n\nFigure 3: Median salary by country and job title\n\n\n\n\n\nAs we can see, salaries vary significantly by country and job title. Directly comparing salaries across these groups can be misleading due to differences in other influencing factors. To better understand the contribution of each factor, let’s build a linear regression model that accounts for these variables and potentially others, while controlling for confounding factors.\n\n\nShow the code\ndata1 &lt;- data |&gt; inner_join(cities, by = c(\"City\" = \"name\"))\ndata1 &lt;- data1 |&gt; left_join(read.csv(\"data/job-categories.csv\"), by = \"Job.Title\")\nmodel &lt;- lm(Pre.Tax.TC ~ 0 + Country + Job.Category + Seniority, data = data1)"
  },
  {
    "objectID": "posts/20240927-euro-tech-money/index.html#factors-affecting-salary",
    "href": "posts/20240927-euro-tech-money/index.html#factors-affecting-salary",
    "title": "European Tech Salaries",
    "section": "Factors Affecting Salary",
    "text": "Factors Affecting Salary\nThe linear model, built using Country, Job Category, and Seniority, demonstrates decent predictive power, with an adjusted R-squared value of 0.82. The model’s coefficients represent the effect (in USD) of each factor on salary. Let’s visualize these coefficients to gain a clearer understanding of the impact of each factor.\n\n\nShow the code\nlibrary(tibble)\n\nmodel_coef &lt;- summary(model)$coefficients |&gt;\n  data.frame() |&gt;\n  rownames_to_column(\"value\") |&gt;\n  mutate(\n    effect = Estimate,\n    error = `Std..Error`,\n    p.value = `Pr...t..`,\n    significant = p.value &lt; 0.05\n  )\n\nmodel_coef &lt;- model_coef |&gt;\n  mutate(\n    variable = case_when(\n      str_detect(value, \"Country\") ~ \"Country\",\n      str_detect(value, \"Job.Category\") ~ \"Job Category\",\n      str_detect(value, \"Seniority\") ~ \"Seniority\"\n    )\n  )\n\nmodel_coef &lt;- model_coef |&gt; mutate(value = sub(\"Country\", \"\", value))\nmodel_coef &lt;- model_coef |&gt; mutate(value = sub(\"Job.Category\", \"\", value))\nmodel_coef &lt;- model_coef |&gt; mutate(value = sub(\"Seniority\", \"\", value))\n\ncountry_coef &lt;- model_coef |&gt;\n  filter(variable == \"Country\") |&gt;\n  arrange(desc(effect))\ncountry_coef &lt;- country_coef |&gt; mutate(rownumber = 1:nrow(country_coef))\n\nxlim &lt;- c(\n  min(country_coef$effect - country_coef$error),\n  max(country_coef$effect + 1.1 * country_coef$error)\n)\nxscale &lt;- c(0, 50000, 100000, 150000)\n\n\nThe plot below illustrates the effect of each country on salary. The dot represents the estimated effect, while the error bars show the standard error. The color of the plot elements and the accompanying text indicate the significance of the effect. Only a few countries have a statistically significant impact on salary at the 0.05 level. However, it is generally better practice to consider the overall differences in salaries across countries.\n\nShow the code\ncountry_coef[1:17, ] |&gt;\n  ggplot(aes(x = effect, y = reorder(value, effect), colour = significant)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = effect - error, xmax = effect + error),\n    height = .2\n  ) +\n  geom_text(aes(label = paste(\"p =\", format(p.value, digits = 2))),\n    vjust = 1.5,\n    hjust = -0.2,\n    colour = \"grey55\",\n    fill = \"white\",\n    size = 2.5\n  ) +\n  geom_vline(\n    xintercept = 0,\n    linetype = \"dashed\",\n    colour = \"grey55\"\n  ) +\n  theme_minimal() +\n  coord_cartesian(xlim = xlim) +\n  scale_x_discrete(limits = xscale, labels = scales::comma) +\n  theme(legend.position = \"none\") +\n  labs(x = \"\", y = \"\")\ncountry_coef[18:34, ] |&gt;\n  ggplot(aes(x = effect, y = reorder(value, effect), colour = significant)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = effect - error, xmax = effect + error),\n    height = .2\n  ) +\n  geom_text(aes(label = paste(\"p =\", format(p.value, digits = 2))),\n    vjust = 1.5,\n    hjust = -0.2,\n    colour = \"grey55\",\n    fill = \"white\",\n    size = 2.5\n  ) +\n  geom_vline(\n    xintercept = 0,\n    linetype = \"dashed\",\n    colour = \"grey55\"\n  ) +\n  theme_minimal() +\n  coord_cartesian(xlim = xlim) +\n  scale_x_discrete(limits = xscale, labels = scales::comma) +\n  theme(legend.position = \"none\") +\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Effect of country on salary\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext two plots show the effect of job category and seniority on the salary. The interpretation is similar to the previous plot.\n\nShow the code\nmodel_coef |&gt;\n  filter(variable == \"Job Category\") |&gt;\n  ggplot(aes(x = effect, y = reorder(value, effect), colour = significant)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = effect - error, xmax = effect + error),\n    height = .2\n  ) +\n  geom_text(aes(label = paste(\"p =\", format(p.value, digits = 2))),\n    vjust = 1.5,\n    hjust = -0.2,\n    colour = \"grey55\",\n    fill = \"white\",\n    size = 2.5\n  ) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  theme_minimal() +\n  scale_x_discrete(limits = c(-50000, 50000), labels = scales::comma) +\n  theme(legend.position = \"none\") +\n  labs(x = \"\", y = \"\")\nmodel_coef |&gt;\n  filter(variable == \"Seniority\") |&gt;\n  mutate(value = str_replace(value, \"Senior Staff / \", \"\")) |&gt;\n  ggplot(aes(x = effect, y = reorder(value, effect), colour = significant)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = effect - error, xmax = effect + error),\n    height = .2\n  ) +\n  geom_text(aes(label = paste(\"p =\", format(p.value, digits = 2))),\n    vjust = 1.5,\n    hjust = -0.2,\n    colour = \"grey55\",\n    fill = \"white\",\n    size = 2.5\n  ) +\n  geom_vline(\n    xintercept = 0,\n    linetype = \"dashed\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  scale_x_discrete(limits = c(-50000, 50000)) +\n  labs(x = \"\", y = \"\") +\n  scale_x_continuous(labels = scales::comma)\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Effect of job title on salary\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Effect of seniority on salary"
  },
  {
    "objectID": "posts/20240927-euro-tech-money/index.html#taxation",
    "href": "posts/20240927-euro-tech-money/index.html#taxation",
    "title": "European Tech Salaries",
    "section": "Taxation",
    "text": "Taxation\nNet salary is what matters most to employees, but it differs from gross salary. Taxes and social security contributions can significantly reduce take-home pay. Let’s calculate the net salary for each respondent and visualize the distribution of net salaries by country.\n\n\nShow the code\nlibrary(tidyr)\n\nnet_by_country &lt;- select(data, Country, City, Pre.Tax.TC, After.Tax.TC) |&gt;\n  inner_join(cities, by = c(\"City\" = \"name\")) |&gt;\n  mutate(Tax = Pre.Tax.TC - After.Tax.TC) |&gt;\n  group_by(Country) |&gt;\n  summarise(\n    After.Tax.TC = mean(After.Tax.TC, na.rm = TRUE),\n    Tax = mean(Tax, na.rm = TRUE),\n    Net = After.Tax.TC,\n    Tax.Percent = Tax / (Net + Tax)\n  ) |&gt;\n  gather(key = \"variable\", value = \"value\", -Country, -Net, -Tax.Percent)\n\nnet_by_country$variable &lt;-\n  factor(net_by_country$variable, levels = c(\"Tax\", \"After.Tax.TC\"))\n\nnet_by_country$Country &lt;-\n  factor(net_by_country$Country,\n    levels = unique(net_by_country$Country[order(net_by_country$Net)])\n  )\n\nnet_by_country |&gt;\n  ggplot(aes(x = Country, y = value, fill = variable)) +\n  geom_col(width = 0.75, alpha = 0.9) +\n  geom_text(\n    aes(label = ifelse(variable == \"Tax\",\n      scales::percent(Tax.Percent, accuracy = 1), \"\"\n    )),\n    position = position_stack(vjust = 0.5),\n    colour = \"grey30\", size = 2.5\n  ) +\n  coord_flip() +\n  theme_minimal() +\n  labs(x = \"\", y = \"\", fill = \"\") +\n  scale_y_continuous(labels = scales::comma)\n\n\n\n\n\n\n\n\nFigure 7: Net salary by country\n\n\n\n\n\nWe can see that the tax burden varies significantly across countries. While the net salary is the most important factor for employees, it is also important to consider the cost of living in each country."
  },
  {
    "objectID": "posts/20240927-euro-tech-money/index.html#cost-of-living",
    "href": "posts/20240927-euro-tech-money/index.html#cost-of-living",
    "title": "European Tech Salaries",
    "section": "Cost of Living",
    "text": "Cost of Living\nWe have net salary, but how far does it go in each country? Let’s calculate the cost of living for each respondent and visualize the distribution of costs by country.\nWe have the yearly savings for each respondent, which is the difference between the net salary and the cost of living. Having known household size and share of household expenses, we can calculate the cost of living for a household of a certain size. For this, we are going to employ the linear regression.\n\n\nShow the code\ndata1 &lt;- data |&gt; inner_join(cities, by = c(\"City\" = \"name\"))\n# let's exclude unrealistic values\ndata1 &lt;- data1 |&gt; filter(Share.of.Household.Expenses &gt; 10)\ndata1 &lt;- data1 |&gt; filter(Yearly.Savings &lt; After.Tax.TC)\n\ndata1 &lt;- data1 |&gt; mutate(Cost.of.Living = (After.Tax.TC - Yearly.Savings) / Share.of.Household.Expenses * 100)\n# let's convert Household.Size to a factor\ndata1 &lt;- data1 |&gt; mutate(Household.Size = as.factor(round(Household.Size)))\n\nmodel &lt;- lm(Cost.of.Living ~ 0 + Country + Household.Size, data = data1)\n\nmodel_coef &lt;- summary(model)$coefficients |&gt;\n  data.frame() |&gt;\n  rownames_to_column(\"value\") |&gt;\n  mutate(\n    effect = Estimate,\n    error = `Std..Error`,\n    p.value = `Pr...t..`,\n    significant = p.value &lt; 0.05\n  )\n\nmodel_coef &lt;- model_coef |&gt;\n  mutate(\n    variable = case_when(\n      str_detect(value, \"Country\") ~ \"Country\",\n      str_detect(value, \"Household.Size\") ~ \"Household Size\",\n      str_detect(value, \"Lifestyle\") ~ \"Lifestyle\"\n    )\n  )\n\nmodel_coef &lt;- model_coef |&gt; mutate(value = sub(\"Country\", \"\", value))\nmodel_coef &lt;- model_coef |&gt; mutate(value = sub(\"Household.Size\", \"\", value))\nmodel_coef &lt;- model_coef |&gt; mutate(value = sub(\"Lifestyle\", \"\", value))\n\ncountry_coef &lt;- model_coef |&gt;\n  filter(variable == \"Country\") |&gt;\n  arrange(desc(effect))\ncountry_coef &lt;- country_coef |&gt;\n  mutate(rownumber = 1:nrow(country_coef))\n\nxlim &lt;- c(\n  min(country_coef$effect - country_coef$error),\n  max(country_coef$effect + 2 * country_coef$error)\n)\nxscale &lt;- c(0, 25000, 50000, 75000, 100000)\n\n\nThe model Cost.of.Living ~ 0 + Country + Household.Size has an adjusted R-squared value of 0.75. We will not use Lifestyle as a factor in the model, as some of the levels have a small number of observations.\nLet’s look at the coefficients for the country variable. Basically, the effect represents cost of living for a single person.\n\nShow the code\ncountry_coef[1:17, ] |&gt;\n  ggplot(aes(x = effect, y = reorder(value, effect), colour = significant)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = effect - error, xmax = effect + error),\n    height = .2\n  ) +\n  geom_text(aes(label = paste(\"p =\", format(p.value, digits = 2))),\n    vjust = 1.5,\n    hjust = -0.2,\n    colour = \"grey55\",\n    fill = \"white\",\n    size = 2.5\n  ) +\n  geom_vline(\n    xintercept = 0,\n    linetype = \"dashed\",\n    colour = \"grey55\"\n  ) +\n  theme_minimal() +\n  coord_cartesian(xlim = xlim) +\n  scale_x_discrete(limits = xscale, labels = scales::comma) +\n  theme(legend.position = \"none\") +\n  labs(x = \"\", y = \"\")\ncountry_coef[18:34, ] |&gt;\n  ggplot(aes(x = effect, y = reorder(value, effect), colour = significant)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = effect - error, xmax = effect + error),\n    height = .2\n  ) +\n  geom_text(aes(label = paste(\"p =\", format(p.value, digits = 2))),\n    vjust = 1.5,\n    hjust = -0.2,\n    colour = \"grey55\",\n    fill = \"white\",\n    size = 2.5\n  ) +\n  geom_vline(\n    xintercept = 0,\n    linetype = \"dashed\",\n    colour = \"grey55\"\n  ) +\n  theme_minimal() +\n  coord_cartesian(xlim = xlim) +\n  scale_x_discrete(limits = xscale, labels = scales::comma) +\n  theme(legend.position = \"none\") +\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Effect of country on cost of living\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext, we will look at the effect of household size on the cost of living. As expected, total cost of living for the two people is higher than for a single person, but the cost per person is lower for a larger household.\n\n\nShow the code\nxscale &lt;- c(0, 10000, 20000, 30000, 40000, 50000)\nxlim &lt;- c(-10000, 60000)\n\nmodel_coef |&gt;\n  filter(variable == \"Household Size\") |&gt;\n  ggplot(aes(x = effect, y = reorder(value, desc(value)), colour = significant)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = effect - error, xmax = effect + error),\n    height = .2\n  ) +\n  geom_text(aes(label = paste(\"p =\", format(p.value, digits = 2))),\n    vjust = 1.5,\n    hjust = -0.2,\n    colour = \"grey55\",\n    fill = \"white\",\n    size = 2.5\n  ) +\n  geom_vline(\n    xintercept = 0,\n    linetype = \"dashed\",\n    colour = \"grey55\"\n  ) +\n  theme_minimal() +\n  coord_cartesian(xlim = xlim) +\n  scale_x_discrete(limits = xscale, labels = scales::comma) +\n  theme(legend.position = \"none\") +\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nFigure 9: Effect of household size on cost of living"
  },
  {
    "objectID": "posts/20240927-euro-tech-money/index.html#savings",
    "href": "posts/20240927-euro-tech-money/index.html#savings",
    "title": "European Tech Salaries",
    "section": "Savings",
    "text": "Savings\nFinally, let’s look at the distribution of yearly savings by country. We will employ the same approach as before, using a linear regression model to understand the factors affecting savings. We will use Country, Household.Size, Job.Category, Seniority, and Share.of.Household.Expenses as predictors.\n\n\nShow the code\ndata1 &lt;- data |&gt; inner_join(cities, by = c(\"City\" = \"name\"))\n# let's exclude unrealistic values\ndata1 &lt;- data1 |&gt; filter(Share.of.Household.Expenses &gt; 10)\ndata1 &lt;- data1 |&gt; filter(Yearly.Savings &lt; After.Tax.TC)\n\n# let's convert Household.Size to a factor\ndata1 &lt;- data1 |&gt; mutate(Household.Size = as.factor(round(Household.Size)))\ndata1 &lt;- data1 |&gt; mutate(Share.of.Household.Expenses = Share.of.Household.Expenses / 100)\ndata1 &lt;- data1 |&gt; left_join(read.csv(\"data/job-categories.csv\"), by = \"Job.Title\")\n\nmodel &lt;- lm(Yearly.Savings ~ 0 + Country + Household.Size + Job.Category + Seniority + Share.of.Household.Expenses, data = data1)\n\nmodel_coef &lt;- summary(model)$coefficients |&gt;\n  data.frame() |&gt;\n  rownames_to_column(\"value\") |&gt;\n  mutate(\n    effect = Estimate,\n    error = `Std..Error`,\n    p.value = `Pr...t..`,\n    significant = p.value &lt; 0.05\n  )\n\nmodel_coef &lt;- model_coef |&gt;\n  mutate(\n    variable = case_when(\n      str_detect(value, \"Country\") ~ \"Country\",\n      str_detect(value, \"Household.Size\") ~ \"Household Size\",\n      str_detect(value, \"Lifestyle\") ~ \"Lifestyle\"\n    )\n  )\n\nmodel_coef &lt;- model_coef |&gt; mutate(value = sub(\"Country\", \"\", value))\nmodel_coef &lt;- model_coef |&gt; mutate(value = sub(\"Household.Size\", \"\", value))\nmodel_coef &lt;- model_coef |&gt; mutate(value = sub(\"Lifestyle\", \"\", value))\n\ncountry_coef &lt;- model_coef |&gt;\n  filter(variable == \"Country\") |&gt;\n  arrange(desc(effect))\ncountry_coef &lt;- country_coef |&gt; mutate(rownumber = 1:nrow(country_coef))\n\nxlim &lt;- c(\n  min(country_coef$effect - country_coef$error),\n  max(country_coef$effect + 1.2 * country_coef$error)\n)\nxscale &lt;- c(0, 25000, 50000, 75000)\n\n\nThis model performs on a mediocre level, with an adjusted R-squared value of 0.56. The coefficients for the country variable represent the expected yearly savings for a single person.\n\nShow the code\ncountry_coef[1:17, ] |&gt;\n  ggplot(aes(x = effect, y = reorder(value, effect), colour = significant)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = effect - error, xmax = effect + error),\n    height = .2\n  ) +\n  geom_text(aes(label = paste(\"p =\", format(p.value, digits = 2))),\n    vjust = 1.5,\n    hjust = -0.2,\n    colour = \"grey55\",\n    fill = \"white\",\n    size = 2.5\n  ) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", colour = \"grey55\") +\n  theme_minimal() +\n  coord_cartesian(xlim = xlim) +\n  scale_x_discrete(limits = xscale, labels = scales::comma) +\n  theme(legend.position = \"none\") +\n  labs(x = \"\", y = \"\")\ncountry_coef[18:34, ] |&gt;\n  ggplot(aes(x = effect, y = reorder(value, effect), colour = significant)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = effect - error, xmax = effect + error),\n    height = .2\n  ) +\n  geom_text(aes(label = paste(\"p =\", format(p.value, digits = 2))),\n    vjust = 1.5,\n    hjust = -0.2,\n    colour = \"grey55\",\n    fill = \"white\",\n    size = 2.5\n  ) +\n  geom_vline(\n    xintercept = 0,\n    linetype = \"dashed\",\n    colour = \"grey55\"\n  ) +\n  theme_minimal() +\n  coord_cartesian(xlim = xlim) +\n  scale_x_discrete(limits = xscale, labels = scales::comma) +\n  theme(legend.position = \"none\") +\n  labs(x = \"\", y = \"\")\n\n\n\n\n\n\n\n\n\n\n\nFigure 10: Effect of country on yearly savings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOnly a few countries show a statistically significant impact on yearly savings. While differences in salary and cost of living across countries and roles are concrete factors, individual spending habits and lifestyle choices play an even more significant role in determining yearly savings."
  },
  {
    "objectID": "posts/20240927-euro-tech-money/index.html#conclusion",
    "href": "posts/20240927-euro-tech-money/index.html#conclusion",
    "title": "European Tech Salaries",
    "section": "Conclusion",
    "text": "Conclusion\nThis analysis offers several key insights. While the following conclusions are statistically significant at the 0.05 level, they may not apply to every individual case.\n\nThe top three countries for gross salary are Belgium, Switzerland, and the United Kingdom. These same countries also lead in net salary.\nBelgium has the highest tax burden, followed by Germany and the UK.\nSwitzerland has the highest cost of living, with Belgium and the UK also ranking high. Hungary, Spain, and Italy are among the least expensive countries.\nThe highest yearly savings in the sample are found in Georgia and Romania, followed by the UK, Switzerland, and Denmark."
  },
  {
    "objectID": "posts/20241105-airflow-filesensor/index.html",
    "href": "posts/20241105-airflow-filesensor/index.html",
    "title": "Using Airflow FileSensor for Triggering ETL Process",
    "section": "",
    "text": "In this article, we will discuss how to use the Airflow FileSensor to trigger an ETL process. We will walk through the process of setting up a FileSensor in Airflow and using it to monitor a directory for new files. Once a new file is detected, the ETL process will be triggered automatically. This can be a useful technique for automating data processing tasks that rely on the availability of new files."
  },
  {
    "objectID": "posts/20241105-airflow-filesensor/index.html#abstract",
    "href": "posts/20241105-airflow-filesensor/index.html#abstract",
    "title": "Using Airflow FileSensor for Triggering ETL Process",
    "section": "",
    "text": "In this article, we will discuss how to use the Airflow FileSensor to trigger an ETL process. We will walk through the process of setting up a FileSensor in Airflow and using it to monitor a directory for new files. Once a new file is detected, the ETL process will be triggered automatically. This can be a useful technique for automating data processing tasks that rely on the availability of new files."
  },
  {
    "objectID": "posts/20241105-airflow-filesensor/index.html#introduction",
    "href": "posts/20241105-airflow-filesensor/index.html#introduction",
    "title": "Using Airflow FileSensor for Triggering ETL Process",
    "section": "Introduction",
    "text": "Introduction\nAirflow is a popular open-source platform for orchestrating complex data workflows. It allows users to define, schedule, and monitor workflows as directed acyclic graphs (DAGs). Airflow provides a wide range of operators that can be used to perform various tasks, such as executing SQL queries, transferring files, and sending emails.\n\n\n\nApache Airflow Logo\n\n\nOne common use case for Airflow is to automate ETL (Extract, Transform, Load) processes. ETL processes involve extracting data from various sources, transforming it into a usable format, and loading it into a data warehouse or other storage system. Airflow provides operators that can be used to perform each step of the ETL process, making it easy to build and schedule complex data pipelines.\nIn this article, we will focus on the Extract step of the ETL process and discuss how to use the Airflow FileSensor to trigger an ETL process when new files become available."
  },
  {
    "objectID": "posts/20241105-airflow-filesensor/index.html#example-use-case",
    "href": "posts/20241105-airflow-filesensor/index.html#example-use-case",
    "title": "Using Airflow FileSensor for Triggering ETL Process",
    "section": "Example Use Case",
    "text": "Example Use Case\nSuppose we have a source system that generates CSV files on a scheduled basis. We want to build an ETL process that reads these CSV files, transforms the data, and loads it into a database. To automate this process, we can use the Airflow FileSensor to monitor a directory for changes and trigger the ETL process.\nTo signal the availability of new files, we can program source system to add a new file to the directory after new CSV files are created. The FileSensor will detect the presence of the new file and trigger the ETL process automatically."
  },
  {
    "objectID": "posts/20241105-airflow-filesensor/index.html#setting-up-the-dag",
    "href": "posts/20241105-airflow-filesensor/index.html#setting-up-the-dag",
    "title": "Using Airflow FileSensor for Triggering ETL Process",
    "section": "Setting up the DAG",
    "text": "Setting up the DAG\nTrigger DAG can be set by creating a new Python file in the dags directory of your Airflow installation. For example, you can create a new file called reports_trigger.py with the following content:\nimport pendulum\n\nfrom Airflow.models.dag import DAG\nfrom Airflow.operators.bash import BashOperator\n\nfrom Airflow.sensors.filesystem import FileSensor\nfrom Airflow.operators.trigger_dagrun import TriggerDagRunOperator\n\n# Where to look for the file\nFILEPATH = \"/mnt/Reports/READY\"\n# The DAG to trigger\nDAG_ID = \"reports_uploader\"\n# Central European Time\nTZ = \"CET\"\n\nwith DAG(\n    dag_id=\"reports_trigger\",\n    description=\"Monitors the appearance of a file and starts a DAG\",\n    # Every 2 hours\n    schedule=\"5 */2 * * *\",\n    start_date=pendulum.datetime(2024, 11, 5, 0, 0, 0, tz=TZ),\n    # Don't run backfill\n    catchup=False,\n    tags=[\"trigger\"],\n    # Only one run at a time\n    max_active_runs=1,\n) as dag:\n\n    check_flag = FileSensor(\n        task_id=\"check_flag\",\n        filepath=FILEPATH,\n    )\n\n    remove_flag = BashOperator(\n        task_id=\"remove_flag\",\n        bash_command=f\"rm -f {FILEPATH}\",\n    )\n\n    trigger_dag = TriggerDagRunOperator(\n        task_id=\"trigger_dag\",\n        trigger_dag_id=DAG_ID,\n        logical_date=pendulum.now().add(seconds=5),\n    )\n    \n    # Set the order of the tasks\n    check_flag &gt;&gt; remove_flag &gt;&gt; trigger_dag\nIn this example, we define a new DAG called reports_trigger that monitors the appearance of a file READY in the /mnt/Reports directory. When a new file is detected, the next task remove_flag deletes it, and another DAG called reports_uploader is triggered for execution.\n\n\n\nDAG in Graph View"
  },
  {
    "objectID": "posts/20241105-airflow-filesensor/index.html#setting-up-the-source-system",
    "href": "posts/20241105-airflow-filesensor/index.html#setting-up-the-source-system",
    "title": "Using Airflow FileSensor for Triggering ETL Process",
    "section": "Setting up the source system",
    "text": "Setting up the source system\nThe source system should be programmed to create a new file named READY in the /mnt/Reports directory after new CSV files are created. There is a variety of ways to achieve this, depending on the source system and the tools available.\nIf the source system is a Windows machine, you can create a batch file that creates the READY file and schedule it to run after the CSV files are generated. For example, you can create a batch file called create_flag.cmd with the following content:\necho &gt; C:\\Users\\admin\\Reports\\%1\nThe argument would be the name of the file to create, READY in this case. If you use the Windows Task Scheduler, add new action to run the batch file with the argument READY.\nIf the source system is a Linux machine, you can create file using the touch command:\ntouch /mnt/Reports/READY"
  },
  {
    "objectID": "posts/20241105-airflow-filesensor/index.html#setting-up-the-etl-process",
    "href": "posts/20241105-airflow-filesensor/index.html#setting-up-the-etl-process",
    "title": "Using Airflow FileSensor for Triggering ETL Process",
    "section": "Setting up the ETL Process",
    "text": "Setting up the ETL Process\nThe ETL process is defined within a distinct DAG named reports_uploader, located in a separate Python file in the dags directory of the Airflow installation."
  },
  {
    "objectID": "posts/20241105-airflow-filesensor/index.html#conclusion",
    "href": "posts/20241105-airflow-filesensor/index.html#conclusion",
    "title": "Using Airflow FileSensor for Triggering ETL Process",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, we discussed how to use the Airflow FileSensor to trigger an ETL process when new files become available and how to set up a DAG to monitor a directory for changes. We also discussed how to program the source system to create a new file to signal the availability of new data.\nI hope this article has been helpful in understanding how to use the Airflow FileSensor for triggering ETL processes. If you have any further questions or comments, please feel free to leave them int the comments section."
  },
  {
    "objectID": "posts/20240721-welcome/index.html",
    "href": "posts/20240721-welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome and take care!"
  },
  {
    "objectID": "posts/20240725-views-of-russia/index.html",
    "href": "posts/20240725-views-of-russia/index.html",
    "title": "Exploring Geospatial Insights with R and rnaturalearth",
    "section": "",
    "text": "The article showcases the utilization of the rnaturalearth package for handling geographical data. This package provides valuable tools and functions for working with spatial information, making it a powerful resource for data analysts and researchers interested in geographic analyses.\nToday, I stumbled upon an article discussing the approval ratings of Russia among people from various nations around the world. As I examined the list, which was sorted from worst to best, a hypothesis formed in my mind: Could the distance between this particular country and others correlate with its citizens’ approval of its international affairs? To explore this, I promptly collected data and calculated the geographical distances between the boundaries of Russia and those of the countries in the list. The null hypothesis posits that distance has no impact on approval rates, while the alternative hypothesis suggests that distance does indeed influence approval levels.\n\ntheme_set(theme_minimal())\n\nworld &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\ndf &lt;- read.csv(\"ViewsOfRussia2024.csv\")\n\nby &lt;- join_by(admin == country)\nworld &lt;- left_join(world, df, by)\n\nworld &lt;- world[world$admin != \"Antarctica\", ]\n\nggplot(data = world) + \n  geom_sf(aes(fill = approval)) + \n  scale_fill_viridis_c(option = \"plasma\") + \n  # theme_void() +\n  theme(legend.position = \"bottom\", \n        legend.key.height = unit(5, \"pt\"), \n        legend.key.width = unit(40, \"pt\"), \n        legend.title.position = \"bottom\") + \n  labs(fill = \"% who have a favorable view of Russia\")\n\n\n\n\n\n\n\n\n\ncountries &lt;- ne_countries(returnclass = \"sf\")\nrussia &lt;- filter(countries, grepl(\"Russia\", admin))\n\ninvisible(sf_use_s2(FALSE))\n\ndf &lt;- df |&gt; rowwise() |&gt;\n  mutate(distB = st_distance(russia, countries[countries$admin == country, ])[1])\n\ndf$distB &lt;- as.numeric(sub(\"([0-9\\\\.]+)\", \"\\\\1\", df$distB)) / 1000000\n\nmodel &lt;- lm(approval ~ distB, data = df)\nsummary(model)\n\n\nCall:\nlm(formula = approval ~ distB, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-23.345 -11.519  -4.029  13.302  28.339 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   22.406      3.859   5.806 1.91e-06 ***\ndistB          1.513      0.814   1.859   0.0723 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.02 on 32 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.09743,   Adjusted R-squared:  0.06922 \nF-statistic: 3.454 on 1 and 32 DF,  p-value: 0.07231\n\n\nThe model explains less than 10% of variability. P-value for distance is 0.072, so the null hypothesis cannot be rejected at the level of 0.05. Scatter plot also shows no obvious trend.\n\nqplot(df$distB, df$approval) + \n  geom_point() + \n  stat_smooth(method = \"lm\", se = F, color = \"red\", formula = y ~ x)\n\n\n\n\n\n\n\n\nIt emerged that the geographical distance between boundaries was statistically insignificant. However, I propose an alternative hypothesis in this scenario. Russia, being an exceptionally vast country, shares proximity with Asian nations in its eastern part. Interestingly, these eastern countries exhibit a more favorable attitude toward Russia compared to their European counterparts. One plausible explanation for this discrepancy is the absence of significant Russian territorial interests in Asia. Since Moscow, the capital, lies in the western part of Russia, let’s measure the distance between capitals and explore this further using regression analysis.\n\ncities &lt;- ne_download(type = \"populated_places\", returnclass = \"sf\")\n\nReading layer `ne_110m_populated_places' from data source \n  `/tmp/Rtmp1g0rpV/ne_110m_populated_places.shp' using driver `ESRI Shapefile'\nSimple feature collection with 243 features and 137 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -175.2206 ymin: -41.29207 xmax: 179.2166 ymax: 64.14346\nGeodetic CRS:  WGS 84\n\ncapitals &lt;- cities[cities$FEATURECLA == \"Admin-0 capital\", ]\n\n\ncapitals &lt;- capitals |&gt; distinct(ADM0NAME, .keep_all = TRUE)\nmoscow &lt;- cities[cities$NAME == \"Moscow\", ]\n\ndf &lt;- read.csv(\"ViewsOfRussia2024.csv\")\n\nby &lt;- join_by(country == ADM0NAME)\ndf &lt;- left_join(df, capitals, by) |&gt; select(country, approval, NAME)\n\ndf &lt;- df |&gt; rowwise() |&gt; \n  mutate(distC = st_distance(moscow, capitals[capitals$NAME == NAME, ])[1])\ndf$distC &lt;- as.numeric(sub(\"([0-9\\\\.]+)\", \"\\\\1\", df$distC)) / 1000000\n\nmodel &lt;- lm(approval ~ distC, data = df)\nsummary(model)\n\n\nCall:\nlm(formula = approval ~ distC, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-27.205 -14.005  -1.208  14.432  27.698 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  21.7485     5.1834   4.196 0.000192 ***\ndistC         0.9300     0.6958   1.337 0.190512    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.21 on 33 degrees of freedom\nMultiple R-squared:  0.05135,   Adjusted R-squared:  0.02261 \nF-statistic: 1.786 on 1 and 33 DF,  p-value: 0.1905\n\n\nUnfortunately, using the distance between capitals didn’t yield meaningful results either.\n\nqplot(df$distC, df$approval) + \n  geom_point() + \n  stat_smooth(method = \"lm\", se = F, color = \"red\", formula = y ~ x)\n\n\n\n\n\n\n\n\nIn my search for additional regressors, I included GDP per capita,\n\ndf &lt;- read.csv(\"ViewsOfRussia2024.csv\")\n\nby &lt;- join_by(country == admin)\ndf &lt;- left_join(df, countries, by) |&gt; select(country, approval, gdp_md, pop_est, economy)\n\ndf &lt;- df |&gt; mutate(gdp_pc = 1000 * gdp_md / pop_est)\n\nmodel &lt;- lm(approval ~ gdp_pc, data = df)\nsummary(model)\n\n\nCall:\nlm(formula = approval ~ gdp_pc, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.612  -6.029   1.617   4.936  22.483 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.26819    2.63352  16.050  &lt; 2e-16 ***\ngdp_pc      -0.67906    0.09049  -7.505 1.52e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.15 on 32 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.6377,    Adjusted R-squared:  0.6264 \nF-statistic: 56.32 on 1 and 32 DF,  p-value: 1.521e-08\n\n\nand it yielded promising results. The coefficient associated with GDP showed a remarkably low p-value of 1.52e-08, providing strong evidence against the null hypothesis. The coefficient of determination (R-squared) was also quite favorable at 0.6377, indicating that the model captures a substantial portion of the variation in approval rates. The coefficient with gdp_pc indicates that for every additional thousand USD of GDP per capita, there is a corresponding 0.7 percentage point decrease in the approval rate.\n\nqplot(df$gdp_pc, df$approval) + \n  geom_point() + \n  stat_smooth(method = \"lm\", formula = y ~ x) + \n  labs(x = \"GDP per capita, K\", y = \"% who have a favorable view of Russia\")\n\n\n\n\n\n\n\n\nIn an effort to enhance predictive power, one can explore the possibility of non-linear dependencies. Let’s consider using the logarithm of GDP as a predictor.\n\nmodel &lt;- lm(approval ~ log(gdp_pc), data = df)\nsummary(model)\n\n\nCall:\nlm(formula = approval ~ log(gdp_pc), data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.983  -5.332  -0.769   3.175  28.181 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   58.164      3.828  15.194 3.46e-16 ***\nlog(gdp_pc)  -12.052      1.371  -8.794 4.77e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.125 on 32 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.7073,    Adjusted R-squared:  0.6982 \nF-statistic: 77.33 on 1 and 32 DF,  p-value: 4.77e-10\n\n\nThe resulting model yields an impressive R² value of 0.7073, indicating that it explains the vast amount of the variation. Additionally, the p-value of 4.77e-10 provides the strongest evidence against the null hypothesis.\n\nqplot(log(df$gdp_pc), df$approval) + \n  geom_point() + \n  stat_smooth(method = \"lm\", formula = y ~ x) + \n  labs(x = \"Logarithm of GDP per capita\", y = \"% who have a favorable view of Russia\")\n\n\n\n\n\n\n\n\nHowever, this improved model is more complex and less straightforward to explain. Allow me to attempt an interpretation: If a country’s GDP per capita is 1% lower than another country’s, it tends to have 0.12% more people who approve of Russia.\nNow that we’ve obtained the regression model, we can use it to make predictions for the remaining countries and visualize the results on a map. By assigning colors based on predicted approval rates, we’ll create an informative and visually appealing representation.\n\nworld &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\nworld &lt;- world[world$admin != \"Antarctica\", ]\n\nworld &lt;- world |&gt; mutate(gdp_pc = 1000 * gdp_md / pop_est)\n\ninvisible(na.omit(world, cols = \"gdp_pc\"))\n\npred &lt;- predict(model, world)\n\nworld &lt;- cbind(world, pred)\n\nby &lt;- join_by(admin == country)\nworld &lt;- left_join(world, df, by)\n\nworld &lt;- mutate(world, approval = coalesce(approval, pred))\n\nworld[world$admin == \"Russia\", ]$approval &lt;- NA\n\nggplot(data = world) + \n  geom_sf(aes(fill = approval)) + \n  scale_fill_viridis_c(option = \"plasma\") + \n  theme(legend.position = \"bottom\", \n        legend.key.height = unit(5, \"pt\"), \n        legend.key.width = unit(40, \"pt\"), \n        legend.title.position = \"bottom\") + \n  labs(fill = \"\")"
  },
  {
    "objectID": "posts/20240731-customers-graphs/index.html",
    "href": "posts/20240731-customers-graphs/index.html",
    "title": "Merging Customers Records Using Graphs in Python",
    "section": "",
    "text": "Traditional relational databases and spreadsheets fall short in capturing complex relationships among customers. Enter graph theory – a powerful framework for representing and analyzing interconnected data. By visualizing customer relationships as a graph, we can uncover hidden patterns, identify clusters, and improve data quality."
  },
  {
    "objectID": "posts/20240731-customers-graphs/index.html#introduction",
    "href": "posts/20240731-customers-graphs/index.html#introduction",
    "title": "Merging Customers Records Using Graphs in Python",
    "section": "",
    "text": "Traditional relational databases and spreadsheets fall short in capturing complex relationships among customers. Enter graph theory – a powerful framework for representing and analyzing interconnected data. By visualizing customer relationships as a graph, we can uncover hidden patterns, identify clusters, and improve data quality."
  },
  {
    "objectID": "posts/20240731-customers-graphs/index.html#motivation",
    "href": "posts/20240731-customers-graphs/index.html#motivation",
    "title": "Merging Customers Records Using Graphs in Python",
    "section": "Motivation",
    "text": "Motivation\nOver time, customers records can become fragmented and duplicated. For example, a customer may use multiple email addresses or phone numbers when interacting with a company. Creating a new record for each email or phone can lead to duplicate records for the same customer. This is especially common in B2B scenarios, where customers often have multiple representatives. Furthermore, some customers represent multiple companies, and their records may be duplicated across different companies.\nDoing any type of marketing analysis on such dataset can lead to incorrect results. We cannot be sure about the latest purchase, the total amount spent, or the number of orders. Is this customer a loyal one or not? Is that customer a new one or not? Is this customer going to leave us or they just started buying from another company? Do we need to send a discount to this customer or not? To answer these questions, we need to have customers database defragmented and deduplicated.\nMerging records manually can be time-consuming and error-prone. By using graphs, we can represent the relationships between customers, emails, and phones and find groups of connected customers. This can help us identify duplicate records and perform actions depending on our business logic."
  },
  {
    "objectID": "posts/20240731-customers-graphs/index.html#sample-data",
    "href": "posts/20240731-customers-graphs/index.html#sample-data",
    "title": "Merging Customers Records Using Graphs in Python",
    "section": "Sample Data",
    "text": "Sample Data\nWe have three datasets: customers, emails, and phones. Each customer can have multiple emails and phones. The names, emails, and phones are generated randomly and do not correspond to real people, though the structure of the data is similar to what you might find in a real-world scenario. In fact, it is the sample taken from the real data, but the names and other personal information are generated randomly to replace the actual ones.\n\ncustomers = pd.read_csv(\"data/customers.csv\")\nemails = pd.read_csv(\"data/emails.csv\")\nphones = pd.read_csv(\"data/phones.csv\")\n\nTake a look at the data.\n\ncustomers.head(3)\n\n\n\n\n\n\n\n\ncustomer_id\nname\n\n\n\n\n0\n330087\nWilliam Sparks\n\n\n1\n443237\nJoseph Williams\n\n\n2\n329867\nEddie Porter\n\n\n\n\n\n\n\n\n\nLength: 1000 Unique: 1000\n\n\n\nemails.head(3)\n\n\n\n\n\n\n\n\ncustomer_id\nemail\n\n\n\n\n0\n599100\nbrian12@example.net\n\n\n1\n330087\nemyers@example.com\n\n\n2\n25494\ncindymurphy@example.net\n\n\n\n\n\n\n\n\n\nLength: 957 Unique: 626 Duplicated: 331 Empty: 0\n\n\n\nphones.head(3)\n\n\n\n\n\n\n\n\ncustomer_id\nphone\n\n\n\n\n0\n15962\n876.997.0254\n\n\n1\n99723\n001-706-213-0362\n\n\n2\n99723\n886.527.4420x90003\n\n\n\n\n\n\n\n\n\nLength: 855 Unique: 524 Duplicated: 331 Empty: 0"
  },
  {
    "objectID": "posts/20240731-customers-graphs/index.html#icons-for-nodes",
    "href": "posts/20240731-customers-graphs/index.html#icons-for-nodes",
    "title": "Merging Customers Records Using Graphs in Python",
    "section": "Icons for Nodes",
    "text": "Icons for Nodes\nNext chunk of code creates a dictionary of icons for different types of nodes in the graph. It will be used later to visualize the subgraphs.\n\nimport PIL\n\nicons = {\n    \"customer\": \"icons/customer.png\",\n    \"phone\": \"icons/phone.png\",\n    \"email\": \"icons/email.png\",\n}\n\nimages = {k: PIL.Image.open(fname) for k, fname in icons.items()}"
  },
  {
    "objectID": "posts/20240731-customers-graphs/index.html#creating-a-graph",
    "href": "posts/20240731-customers-graphs/index.html#creating-a-graph",
    "title": "Merging Customers Records Using Graphs in Python",
    "section": "Creating a Graph",
    "text": "Creating a Graph\nLet’s create graph and add nodes. Each node will represent a customer, email, or phone. We will use the images dictionary to assign an image to each node, but it’s not necessary for the procedure, as well as setting the type of the node.\n\nG = nx.Graph()\n\nnodes = []\n\nfor x in emails[\"email\"].dropna().unique():\n    G.add_node(x, image=images[\"email\"], type=\"email\")\n\nfor x in phones[\"phone\"].dropna().unique():\n    G.add_node(x, image=images[\"phone\"], type=\"phone\")\n\nfor x in customers[\"customer_id\"].unique():\n    G.add_node(x, image=images[\"customer\"], type=\"customer\")\n\nNext, we will add edges to the graph. The edges will connect customers with their emails and phones.\n\nedges = []\n\nfor x in customers[[\"customer_id\"]].merge(emails).values:\n    edges.append(x)\n\nfor x in customers[[\"customer_id\"]].merge(phones).values:\n    edges.append(x)\n\nG.add_edges_from(edges)"
  },
  {
    "objectID": "posts/20240731-customers-graphs/index.html#finding-groups-of-connected-customers",
    "href": "posts/20240731-customers-graphs/index.html#finding-groups-of-connected-customers",
    "title": "Merging Customers Records Using Graphs in Python",
    "section": "Finding Groups of Connected Customers",
    "text": "Finding Groups of Connected Customers\nCustomers that share the same email or phone will be connected by the edges. Let’s find groups of connected customers.\n\ngroups = list(nx.connected_components(G))\nprint(\"Groups:\", len(groups))\n\nGroups: 559"
  },
  {
    "objectID": "posts/20240731-customers-graphs/index.html#visualizing-the-graph",
    "href": "posts/20240731-customers-graphs/index.html#visualizing-the-graph",
    "title": "Merging Customers Records Using Graphs in Python",
    "section": "Visualizing the Graph",
    "text": "Visualizing the Graph\nThe number of connected components is rather large to visualize all of them, and most of the groups will contain only a few nodes. Let’s find the groups with the largest number of nodes and visualize them.\n\ndf = pd.DataFrame([groups]).T\ndf.columns = [\"group\"]\ndf[\"size\"] = df[\"group\"].apply(len)\ndf[\"size\"].hist(bins=20, log=True)\nplt.title(\"Group Size Distribution\")\nplt.show();\nplt.close()\n\n\n\n\n\n\n\n\nThe simplest way to visualize the graph is to use the draw function from the networkx library. We will use the nx.draw function to visualize the graph. We will create a grid of subplots and visualize the top groups. Parameter seed is set to 42 to make the layout reproducible.\n\nfig, axes = plt.subplots(3, 4, figsize=(8, 6))\n\ntop_groups = list(\n    df.sort_values(\n        \"size\",\n        ascending=False,\n    )\n    .head(len(axes.flatten()))\n    .index\n)\n\nfor i, g in enumerate(top_groups):\n    ax = axes.flatten()[i]\n    subgraph = G.subgraph(groups[g])\n    pos = nx.spring_layout(subgraph, seed=42)\n    nx.draw(\n        subgraph,\n        pos=pos,\n        with_labels=False,\n        node_size=25,\n        ax=ax,\n    )\n    ax.set_title(f\"Group {g}\");\n\nplt.tight_layout()\nplt.show();\nplt.close()\n\n\n\n\n\n\n\n\nThere are literally constellations of different shapes and sizes. Let’s visualize some of them in more detail."
  },
  {
    "objectID": "posts/20240731-customers-graphs/index.html#visualizing-subgraphs",
    "href": "posts/20240731-customers-graphs/index.html#visualizing-subgraphs",
    "title": "Merging Customers Records Using Graphs in Python",
    "section": "Visualizing Subgraphs",
    "text": "Visualizing Subgraphs\nLet’s visualize one of the largest group in more detail. We will use the nx.draw_networkx_edges function to draw the edges and the imshow function to display the icons of the nodes. We will also add the customer id to the customers’ nodes. The value of parameter seed is set to the same value as in the previous chunk to keep the layout. You can change it to see different layouts.\n\nsubgraph = G.subgraph(groups[91])\nfig, ax = plt.subplots(figsize=(8, 8))\n\npos = nx.spring_layout(subgraph, seed=42)\n\nnx.draw_networkx_edges(\n    subgraph,\n    pos=pos,\n    ax=ax,\n    arrows=True,\n    arrowstyle=\"-\",\n    min_source_margin=10,\n    min_target_margin=10,\n)\n\n\ntr_figure = ax.transData.transform\ntr_axes = fig.transFigure.inverted().transform\n\n\nicon_size = (ax.get_xlim()[1] - ax.get_xlim()[0]) * 0.015\nicon_center = icon_size / 2.0\n\nfor n in subgraph.nodes:\n    xf, yf = tr_figure(pos[n])\n    xa, ya = tr_axes((xf, yf))\n    a = plt.axes([xa - icon_center, ya - icon_center, icon_size, icon_size])\n    a.imshow(subgraph.nodes[n][\"image\"])\n    if G.nodes[n][\"type\"] == \"customer\":\n        a.text(\n            0.5,\n            0.5,\n            n,\n            ha=\"center\",\n            va=\"center\",\n            fontsize=8,\n            color=\"red\",\n            backgroundcolor=\"white\",\n            bbox=dict(color=\"white\", facecolor=\"white\", alpha=0.5),\n        )\n    a.axis(\"off\")\n\nsns.despine(left=True, bottom=True)\n\nplt.show();\nplt.close()"
  },
  {
    "objectID": "posts/20240731-customers-graphs/index.html#why-graphs-and-not-sql",
    "href": "posts/20240731-customers-graphs/index.html#why-graphs-and-not-sql",
    "title": "Merging Customers Records Using Graphs in Python",
    "section": "Why Graphs and Not SQL?",
    "text": "Why Graphs and Not SQL?\nWe can see here that the customers in this group form pretty complex relationships. A customer may be connected to another one by the phone numbers, and the other one may be connected to the third one by the email, forming a chain of connections. I believe that it is nearly impossible to find this kind of relationship using SQL. The more complex the relationships are, the more time and effort it will take to find them using SQL. For example, if we have a chain of 10 customers, where each customer is connected to the next one by the phone number, it will take 10 joins to find this chain using SQL. If we have 100 customers in the chain, it will take 100 joins to find it using SQL, and the query will probably never complete. But it takes fractions of a second to find it using the graph."
  },
  {
    "objectID": "posts/20240731-customers-graphs/index.html#assigning-groups-to-customers",
    "href": "posts/20240731-customers-graphs/index.html#assigning-groups-to-customers",
    "title": "Merging Customers Records Using Graphs in Python",
    "section": "Assigning Groups to Customers",
    "text": "Assigning Groups to Customers\nFinally, we will assign a group to each customer. For that, we will expand the groups list and create a new DataFrame with the group_id and customer_id columns.\n\ndf = pd.DataFrame([groups]).T\ndf.columns = [\"customer_id\"]\ndf = df.explode(\"customer_id\")\ndf[\"group_id\"] = df.index\ndf.tail(3)\n\n\n\n\n\n\n\n\ncustomer_id\ngroup_id\n\n\n\n\n557\n601053\n557\n\n\n558\n571.212.7377x69843\n558\n\n\n558\n590385\n558\n\n\n\n\n\n\n\nNote that customer_id column contains phone numbers and emails as well as customer ids, but when we merge the data, there will remain only the customer ids.\n\ncustomers = customers.merge(df)\ncustomers.head(3)\n\n\n\n\n\n\n\n\ncustomer_id\nname\ngroup_id\n\n\n\n\n0\n330087\nWilliam Sparks\n1\n\n\n1\n443237\nJoseph Williams\n4\n\n\n2\n329867\nEddie Porter\n6\n\n\n\n\n\n\n\nLet’s check the number of customers and unique customer ids to make sure that we didn’t lose any customers neither we added duplicates.\n\nlen(customers), len(customers[\"customer_id\"].unique())\n\n(1000, 1000)\n\n\nLooks good. Now we can save the data to the file.\n\ncustomers.to_csv(\"data/customers_grouped.csv\", index=False)"
  },
  {
    "objectID": "posts/20240731-customers-graphs/index.html#conclusion",
    "href": "posts/20240731-customers-graphs/index.html#conclusion",
    "title": "Merging Customers Records Using Graphs in Python",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, we explored how to merge customer records using graphs. We created a graph of customers, emails, and phones and found groups of connected customers. We assigned a group to each customer and saved the data to a file. This approach can help us identify duplicate records and perform actions depending on our business logic. We also visualized the graph and subgraphs to better understand the relationships between customers. This can be useful for marketing analysis, customer segmentation, and other tasks that require a deep understanding of customer relationships."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "A Systems Engineer’s Take on Data"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Systems, Data, and a Dash of Curiosity",
    "section": "",
    "text": "CV Week 2024\n\n\n\n\n\n\nCompVis\n\n\nML\n\n\n\n\n\n\n\n\n\nDec 18, 2024\n\n\nAleksei\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Airflow FileSensor for Triggering ETL Process\n\n\n\n\n\n\nETL\n\n\nAirflow\n\n\n\n\n\n\n\n\n\nNov 5, 2024\n\n\nAleksei\n\n\n\n\n\n\n\n\n\n\n\n\nEuropean Tech Salaries\n\n\n\n\n\n\nMoney\n\n\nR\n\n\n\n\n\n\n\n\n\nSep 27, 2024\n\n\nAleksei\n\n\n\n\n\n\n\n\n\n\n\n\nPython Library for Russian Macroeconomics Data\n\n\n\n\n\n\nTimeseries\n\n\nMacroeconomics\n\n\nPython\n\n\n\n\n\n\n\n\n\nAug 22, 2024\n\n\nAleksei\n\n\n\n\n\n\n\n\n\n\n\n\nKano Method for Prioritization of Features\n\n\n\n\n\n\nMarketing\n\n\nProduct\n\n\nPython\n\n\n\n\n\n\n\n\n\nAug 5, 2024\n\n\nAleksei\n\n\n\n\n\n\n\n\n\n\n\n\nMerging Customers Records Using Graphs in Python\n\n\n\n\n\n\nPython\n\n\nGraphs\n\n\n\n\n\n\n\n\n\nJul 31, 2024\n\n\nAleksei\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Geospatial Insights with R and rnaturalearth\n\n\n\n\n\n\nR\n\n\nGeo\n\n\n\n\n\n\n\n\n\nJul 25, 2024\n\n\nAleksei\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nNews\n\n\n\n\n\n\n\n\n\nJul 21, 2024\n\n\nAleksei\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/20241218-cv-week/index.html",
    "href": "posts/20241218-cv-week/index.html",
    "title": "CV Week 2024",
    "section": "",
    "text": "In November 2024, the Yandex School of Data Analysis (YSDA) and Yandex conducted an open online intensive course on computer vision, focusing on generative diffusion models that underpin many visual services."
  },
  {
    "objectID": "posts/20241218-cv-week/index.html#introduction",
    "href": "posts/20241218-cv-week/index.html#introduction",
    "title": "CV Week 2024",
    "section": "",
    "text": "In November 2024, the Yandex School of Data Analysis (YSDA) and Yandex conducted an open online intensive course on computer vision, focusing on generative diffusion models that underpin many visual services."
  },
  {
    "objectID": "posts/20241218-cv-week/index.html#format-and-structure",
    "href": "posts/20241218-cv-week/index.html#format-and-structure",
    "title": "CV Week 2024",
    "section": "Format and Structure",
    "text": "Format and Structure\nDates: November 25–29, 2024.\nFormat: All lectures and seminars were pre-recorded and broadcast on YouTube. During the broadcasts, participants could ask questions in the comments and interact with instructors and fellow participants via a dedicated Telegram channel.\nParticipation and Certification: The broadcasts were open to all without selection. However, to receive a certificate, participants needed to complete Qualifying assignments on the Yandex Contest platform and submit a final project."
  },
  {
    "objectID": "posts/20241218-cv-week/index.html#lectures-and-seminars",
    "href": "posts/20241218-cv-week/index.html#lectures-and-seminars",
    "title": "CV Week 2024",
    "section": "Lectures and Seminars",
    "text": "Lectures and Seminars\nThe intensive course’s schedule consisted of lectures and seminars. Five days from Monday to Friday, in the evenings, there were a lecture for approximately an hour and then a seminar where the instructor provided with the implementation of the different aspects of image generation approaches.\nYouTube Playlist\nNovember 25\n\nLecture 1: Introduction to Diffusion Models (Lecturer: Dmitry Baranchuk, Researcher, Yandex Research).\nSeminar 1: Basic Implementation of Diffusion Models (Instructor: Sergey Kastrulin, Researcher, Yandex Research).\n\nNovember 26\n\nLecture 2: Formulating Diffusion Models through Stochastic and Ordinary Differential Equations (Lecturer: Mikhail Romanov, Developer, Computer Vision Service).\nSeminar 2: Implementing an Efficient Sampler (Instructor: Mikhail Romanov).\n\nNovember 27\n\nLecture 3: Architectures of Diffusion Models, Training and Sampling Methods, and Text-to-Image Generation (Lecturer: Denis Kuznedelev, Researcher, Yandex Research).\nSeminar 3: Generating Images from Text Descriptions (Instructor: Denis Kuznedelev).\n\nNovember 28\n\nLecture 4: Distillation of Diffusion Models Using ODE-Based Methods (Lecturer: Nikita Starodubtsev, Researcher, Yandex Research ML Residency).\nLecture 5: Distillation of Diffusion Models Without ODEs (Lecturer: Dmitry Baranchuk).\nSeminar 4: Implementing Consistent Models for Text-to-Image Generation (Instructor: Nikita Starodubtsev).\n\nNovember 29\n\nLecture 6: Fine-Tuning Diffusion Models Using Reinforcement Learning Methods (Lecturer: Alexander Shishenya, Developer, Computer Vision Service).\nLecture 7: YandexART — Industrial Diffusion Model (Lecturer: Sergey Kastrulin)."
  },
  {
    "objectID": "posts/20241218-cv-week/index.html#qualifying-assignment",
    "href": "posts/20241218-cv-week/index.html#qualifying-assignment",
    "title": "CV Week 2024",
    "section": "Qualifying Assignment",
    "text": "Qualifying Assignment\nThe Qualifying Assignment was a prerequisite for accessing the final project and comprised three programming tasks designed to assess participants’ proficiency in fundamental machine learning concepts and PyTorch implementation.\n\n1. Reshape a Tensor\nThe first task required reshaping a list or tensor by swapping its first two dimensions without utilizing PyTorch or NumPy methods. For example:\n\\(\\begin{bmatrix}1 & 2 & 3\\\\4 & 5 & 6\\end{bmatrix}\\) should be transformed into \\(\\begin{bmatrix}1 & 4 \\\\ 2 & 5 \\\\ 3 & 6\\end{bmatrix}\\).\nThis task tested participants’ understanding of tensor manipulation at a fundamental level, emphasizing the importance of grasping underlying data structures without relying on high-level library functions.\n\n\n2. Encoder and Decoder Architecture\nThe second task involved implementing the encoder and decoder components of a Variational Autoencoder (VAE) architecture. VAEs are generative models that learn to represent data in a latent space, enabling the generation of new, similar data points. This task assessed participants’ ability to construct neural network architectures.\n\n\n\nVariational autoencoder architecture\n\n\n\n\n3. VAE Loss Function\nThe third task required identifying and correcting errors in the VAE loss function implementation. To validate the correctness of the function, participants trained the VAE and performed inference on the MNIST digits dataset. The VAE loss function typically comprises two components:\n\nReconstruction Loss: Measures how well the decoder reconstructs the input data.\nKullback-Leibler (KL) Divergence: Regularizes the learned latent space to align with a predefined distribution, often a standard normal distribution.\n\nAccurate implementation of this loss function is crucial for the VAE to learn meaningful latent representations and generate coherent outputs.\n\n\n\nReconstructed digits using VAE\n\n\nEach of the first two tasks was worth up to 2 points, while the third task could earn up to 4 points. A minimum of 6 points was required to pass, ensuring that participants had a solid grasp of the necessary concepts to proceed to the final project."
  },
  {
    "objectID": "posts/20241218-cv-week/index.html#final-project",
    "href": "posts/20241218-cv-week/index.html#final-project",
    "title": "CV Week 2024",
    "section": "Final Project",
    "text": "Final Project\nIn the final project, participants had to distill a multi-step diffusion model into a more efficient, few-step student model, significantly enhancing generation speed.\nThe project focused on implementing the Consistency Distillation framework, a method that accelerates diffusion models by enforcing self-consistency along learned trajectories.\nParticipants would apply this technique to distill the Stable Diffusion 1.5 (SD 1.5) model, a latent text-to-image diffusion model capable of generating photo-realistic images from textual descriptions.\nThe project comprised eight tasks, each building upon the previous, guiding participants toward developing a proficient model capable of generating images in just four steps.\n\nTeacher Model\nThe initial model for our experiments is Stable Diffusion 1.5, a pre-trained latent text-to-image diffusion model. This serves as the “teacher” model in our distillation process.\nFor the prompt: “A sad puppy with large eyes”, running the model with 50 steps and a guidance_scale of 7.5 produces the following high-quality images:\n\nHowever, when the number of steps is reduced to 4, the output becomes blurry and lacks detail:\n\nThis demonstrates the trade-off between the number of sampling steps and image quality in diffusion models. Our goal is to bridge this gap by using techniques like Consistency Distillation to achieve similar quality with fewer steps.\n\n\nConsistency Training\nThe model will be trained on a subset of the COCO Dataset comprising 5,000 images. To reduce memory consumption, we will train LoRA (Low-Rank Adaptation) adapters for the U-Net convolutional neural network instead of fine-tuning the entire model.\nThis approach significantly decreases the number of trainable parameters and activation memory, enhancing efficiency during training.\nAdditionally, implementing techniques such as gradient checkpointing can further reduce memory usage, albeit with a potential increase in training time.\nBy employing these strategies, we aim to achieve effective model performance while operating within the memory constraints of our training environment.\nThe result with guidance_scale = 2 looks sharper, but the quality is still far from desired.\n\n\n\nConsistency Distillation\nIn the Consistency Distillation phase, we enhance the model’s quality by incorporating a teacher model within the U-Net architecture, utilizing LoRA adapters trained in the previous step. This integration refines the student’s learning process, leading to significantly improved image generation results.\nConsistency Distillation is a technique that accelerates diffusion models by enforcing self-consistency along learned trajectories. By aligning the student model’s outputs with those of the teacher model, the student learns to produce high-quality images in fewer steps. Implementing this method with LoRA adapters allows for efficient training, as LoRA reduces the number of trainable parameters, thereby decreasing memory consumption and computational load.\n\n\n\nMulti-boundary Consistency Distillation\nIn the Multi-boundary Consistency Distillation phase, we draw inspiration from recent advancements in generative modeling, particularly the work by researchers from Google DeepMind. In their paper “Multistep Consistency Models” (arXiv:2403.06807v3), they propose a method that interpolates between consistency models and diffusion models, allowing for a trade-off between sampling speed and quality.\nBy employing 2 to 8 sampling steps, this approach achieves performance comparable to traditional diffusion models but with significantly reduced computational resources. This reduction in sampling steps leads to decreased memory usage and computational load, making the models more efficient without compromising output quality.\nImplementing this technique involves training the model to maintain consistency across multiple steps, effectively enabling it to generate high-quality samples in fewer iterations. This advancement is particularly beneficial for applications requiring rapid generation or operating under resource constraints.\n\n\n\nGraded Assignments\nThere were 4 automatically graded assignments and one teacher graded assignment.\n\n1. Implementation of DDIM Solver Step\nIn diffusion models, the forward diffusion process gradually transforms images into noise, following the distribution:\n\\[ q(\\mathbf{x}_t | \\mathbf{x}_0)= {N}(\\mathbf{x}_t | \\alpha_t \\mathbf{x}_0, \\sigma^2_t I)\\]\nAt time step \\(t\\), the noisy image \\(\\mathbf{x}_t\\) can be represented as: \\(\\mathbf{x}_t = \\alpha_t \\mathbf{x}_0 + \\sigma_t \\epsilon\\), where \\(\\epsilon{\\sim} {N}(0, I)\\).\nThe goal of the diffusion model is to solve the inverse problem, reconstructing an image from noise. This reverse process is formulated as an ordinary differential equation (ODE):\n\\(dx = \\left[ f(\\mathbf{x}, t) - \\frac{1}{2} \\nabla_{\\mathbf{x}_t} \\log p_t(\\mathbf{x}) \\right] dt\\), where \\(f(\\mathbf{x}, t)\\) is known from the given noise process, and \\(\\nabla_{\\mathbf{x}_t} \\log p_t(\\mathbf{x}_t)\\) (score function) is estimated using a neural network: \\(s_\\theta(\\mathbf{x}_t, t) \\approx \\nabla_{\\mathbf{x}_t} \\log p_t(\\mathbf{x}_t)\\).\nThus, having an estimate for \\(\\nabla_{\\mathbf{x}_t} \\log p_t(\\mathbf{x})\\), we can solve this ODE starting from random noise and generate a picture.\nFor this assignment, participants implemented the step from \\(\\mathbf{x}_t\\) to \\(\\mathbf{x}_s\\) using the Denoising Diffusion Implicit Models (DDIM) framework:\n\\[\\mathbf{x}_s = DDIM(\\epsilon_\\theta, \\mathbf{x}_t, t, s) = \\alpha_s \\cdot \\left(\\frac{\\mathbf{x}_t - \\sigma_t \\epsilon_\\theta}{\\alpha_t} \\right) + \\sigma_s \\epsilon_\\theta\\]\nMost of the function was pre-written by the instructors, but participants were required to correctly set \\(\\alpha_t\\) and \\(\\sigma_t\\) using the DDIMScheduler. This task tested participants’ understanding of the underlying mathematical concepts and their ability to implement them in code.\n\n\n2. Implementation of Noise Process\nThe second task involved implementing the function q_sample(x, t, scheduler, noise) to simulate the forward diffusion process. The function follows the mathematical formulation:\n\\(q(\\mathbf{x}_t | \\mathbf{x}_0)= {N}(\\mathbf{x}_t | \\alpha_t \\mathbf{x}_0, \\sigma^2_t I)\\) where the noisy image \\(x_t\\) at time \\(t\\) is calculated as:\n\\(\\mathbf{x}_t = \\alpha_t \\mathbf{x}_0 + \\sigma_t \\epsilon\\), where \\(\\epsilon{\\sim} {N}(0, I)\\).\nParticipants were required to:\n\nUse the scheduler to retrieve \\(\\alpha_t\\) and \\(\\sigma_t\\) values for the given timestep \\(t\\).\nGenerate random noise \\(\\epsilon \\sim \\mathcal{N}(0, I)\\) and apply it to the formula above to simulate the noisy image \\(\\mathbf{x}_t\\).\nHandle edge cases for boundary points, ensuring the process remains valid when \\(t=0\\).\n\nThis assignment focused on implementing the forward diffusion process accurately while accounting for the nuances of boundary conditions, which added complexity to the task.\n\n\n3. Consistency Training\nConsistency distillation leverages the teacher model to obtain the second point on the ODE trajectory, which can also be computed using the DDIM formula. The task required participants to derive the function for this computation based on the definitions of the noise process and the score function.\n\\(\\epsilon_\\theta(x_t, t) = - \\sigma_t s_\\theta(x_t, t)\\)\n\\(s_\\theta(x_t, t) \\approx \\nabla_{x_t} \\log q(x_t) = \\mathop{\\mathbb{E}}_{\\mathbf{x}\\sim p_{data}}\\left [ \\nabla_{\\mathbf{x}_t} \\log q(\\mathbf{x}_t | \\mathbf{x}) \\vert \\mathbf{x}_t \\right ] \\approx \\nabla_{\\mathbf{x}_t} \\log q(\\mathbf{x}_t \\vert \\mathbf{x})\\)\n\n\n4. Multi-boundary Timesteps\nThe objective of this task was to implement the get_multi_boundary_timesteps function, which generates boundary points for multi-step consistency training. These boundary points define the timesteps used in sampling and play a critical role in the effectiveness of the distillation process.\nThe task itself was somewhat ambiguous, leaving room for interpretation. Participants were expected to:\n\nUnderstand how multi-boundary timesteps are utilized in multi-step consistency models. Implement the function to output the appropriate boundary points.\nEnsure that the timesteps are well-distributed and align with the requirements of the consistency distillation framework.\n\n\n\n5. Generated Images\nOnce all the graded and non-graded tasks were completed, the next step was to train the Multi-boundary Consistency Model. This involved applying the techniques and frameworks developed in earlier tasks to produce high-quality generated images.\nAfter training, participants were required to upload the trained model along with generated results for assessment. This provided an opportunity to showcase the practical implementation of multi-step consistency techniques.\nThe trained model was evaluated based on its ability to generate images that were consistent and high-quality, even with a reduced number of sampling steps.\n\n\n\n\n\n\n\n\n\nConsistency Distillation\n\n\n\n\n\n\n\nMulti-boundary Consistency Distillation\n\n\n\n\n\n\nSampling Prompts\n\nA sad puppy with large eyes.\nAstronaut in a jungle, cold color palette, muted colors, detailed, 8k.\nA photo of beautiful mountain with realistic sunset and blue lake, highly detailed, masterpiece.\nA girl with pale blue hair and a cami tank top.\nA lighthouse in a giant wave, origami style.\nBelle epoque, christmas, red house in the forest, photo realistic, 8k.\nA small cactus with a happy face in the Sahara desert.\nGreen commercial building with refrigerator and refrigeration units outside."
  },
  {
    "objectID": "posts/20241218-cv-week/index.html#conclusion",
    "href": "posts/20241218-cv-week/index.html#conclusion",
    "title": "CV Week 2024",
    "section": "Conclusion",
    "text": "Conclusion\nParticipating in YSDA’s intensive training programs has been both challenging and rewarding. My first experience was during GPT Week in 2023, which did not require a qualifying assignment. The final project involved training a model to summarize articles, with flexibility regarding the architecture and implementation details.\nIn contrast, CV Week 2024 presented a more rigorous experience. Both the qualifying assignment and the final project demanded significant effort, especially given my limited background in computer vision. This intensive pushed me to expand my knowledge and skills, making the experience both demanding and educational.\nFor those interested in exploring similar topics, YSDA offers a range of online courses. These resources can provide a solid foundation for tackling advanced subjects in data analysis and machine learning."
  },
  {
    "objectID": "posts/20240805-kano-model/index.html",
    "href": "posts/20240805-kano-model/index.html",
    "title": "Kano Method for Prioritization of Features",
    "section": "",
    "text": "The Kano model is a theory for product development and customer satisfaction developed in the 1980s by Professor Noriaki Kano. The model classifies customer preferences into five categories: Must-be Quality, One-dimensional Quality, Attractive Quality, Indifferent Quality, and Reverse Quality. The Kano model is used to prioritize features and functionalities in product development based on customer needs and expectations."
  },
  {
    "objectID": "posts/20240805-kano-model/index.html#categories-of-the-kano-model",
    "href": "posts/20240805-kano-model/index.html#categories-of-the-kano-model",
    "title": "Kano Method for Prioritization of Features",
    "section": "Categories of the Kano Model",
    "text": "Categories of the Kano Model\nMust-be features are basic requirements that customers expect. If these features are not present in a product, customers will be dissatisfied. However, the presence of these features does not necessarily lead to customer satisfaction. Must-be Quality features are considered essential for the product.\n\nExamples: a car must have wheels, a smartphone must have a battery, a website must have a search function.\n\nOne-dimensional features are directly proportional to customer satisfaction. The more these features are present in a product, the more satisfied customers will be. These features are usually explicitly stated by customers and are easy to measure and quantify.\n\nExamples: a car with leather seats, a smartphone with a high-resolution camera, a website with fast loading times.\n\nAttractive features are unexpected features that delight customers. These features are not explicitly requested by customers but can create a positive emotional response when present. Attractive Quality features can differentiate a product from its competitors and create a competitive advantage.\n\nExamples: a car with a built-in navigation system, a smartphone with facial recognition technology, a website with personalized recommendations.\n\nIndifferent features are neither good nor bad from the customer’s perspective. Customers are indifferent to these features, and their presence or absence does not significantly impact customer satisfaction. These features are often considered “nice to have” but not essential.\n\nExamples: a car with cup holders, a smartphone with a stylus, a website with social media integration.\n\nReverse features are features that, when present, can lead to customer dissatisfaction. These features may be perceived as unnecessary or even annoying by customers. It is essential to identify and eliminate Reverse Quality features to prevent negative customer experiences.\n\nExamples: a car with uncomfortable seats, a smartphone with a short battery life, a website with intrusive pop-up ads."
  },
  {
    "objectID": "posts/20240805-kano-model/index.html#prioritizing-features-with-the-kano-model",
    "href": "posts/20240805-kano-model/index.html#prioritizing-features-with-the-kano-model",
    "title": "Kano Method for Prioritization of Features",
    "section": "Prioritizing Features with the Kano Model",
    "text": "Prioritizing Features with the Kano Model\n\n\n\n\n\n\nFigure 1: Example of a Kano diagram.\n\n\n\nWith the Kano model, prioritization of features and functionalities becomes clear and straightforward as that:\na) keep eye on the Must-be Quality features, as they are essential, b) incorporate One-dimensional Quality features to increase customer satisfaction, c) consider Attractive Quality features to create a competitive advantage; d) eliminate Reverse Quality features, and e) save resources by setting Indifferent Quality features as low priority."
  },
  {
    "objectID": "posts/20240805-kano-model/index.html#implementation-of-the-kano-analysis",
    "href": "posts/20240805-kano-model/index.html#implementation-of-the-kano-analysis",
    "title": "Kano Method for Prioritization of Features",
    "section": "Implementation of the Kano Analysis",
    "text": "Implementation of the Kano Analysis\nImplementing the Kano model involves a sequence of steps, beginning with the development of a questionnaire. For each feature, two types of questions are posed: functional and dysfunctional.\n\nThe functional question assesses respondents’ feelings when a feature is present.\nThe dysfunctional question gauges their reactions in the absence of that feature.\n\nEach question offers five possible responses, from “I like it” to “I dislike it.” Subsequently, these responses are classified into the five Kano categories.\n\n\n\n\nTable 1: Classification of answers to the Kano questionnaire.\n\n\n\n\n\n\n\n\n\n\nCategory\n\n\nDysfunctional\n1) I like it\n2) I expect it\n3) I am neutral\n4) I can tolerate it\n5) I dislike it\n\n\nFunctional\n\n\n\n\n\n\n\n\n\n1) I like it\nQuestionable\nAttractive\nAttractive\nAttractive\nOne-dimensional\n\n\n2) I expect it\nReverse\nIndifferent\nIndifferent\nIndifferent\nMust-be\n\n\n3) I am neutral\nReverse\nIndifferent\nIndifferent\nIndifferent\nMust-be\n\n\n4) I can tolerate it\nReverse\nIndifferent\nIndifferent\nIndifferent\nMust-be\n\n\n5) I dislike it\nReverse\nReverse\nReverse\nReverse\nQuestionable\n\n\n\n\n\n\n\n\n\n\nAfter the classification of responses, the next step is to calculate the satisfaction and dissatisfaction scores for each feature. The satisfaction influence score is calculated as the percentage of Attractive and One-dimensional responses relative to the total number of responses.\n\\[ \\text{Satisfaction Influence} = \\dfrac{A + O}{ A + O + M + I } \\times 100 \\%  \\tag{1}\\] The dissatisfaction influence score is calculated as the percentage of One-dimensional and Must-be responses relative to the total number of responses.\n\\[ \\text{Dissatisfaction Influence} =  - \\dfrac{O + M}{A + O + M + I} \\times 100 \\%  \\tag{2}\\]\nThe features are then plotted on a Kano diagram, with the dissatisfaction score on the x-axis and the satisfaction score on the y-axis. The features are categorized based on their position in the diagram: Attractive Quality features in the upper left quadrant, One-dimensional Quality features in the upper right quadrant, Must-be Quality features in the lower right quadrant, and Indifferent features in the lower left quadrant, as depicted in the Figure 1."
  },
  {
    "objectID": "posts/20240805-kano-model/index.html#example-implementation-of-the-kano-analysis",
    "href": "posts/20240805-kano-model/index.html#example-implementation-of-the-kano-analysis",
    "title": "Kano Method for Prioritization of Features",
    "section": "Example implementation of the Kano Analysis",
    "text": "Example implementation of the Kano Analysis\n\nConducting a Kano Survey\nFor illustrative purposes, let’s consider existing dataset with responses to a Kano questionnaire from Doing Research Online: The Kano Model project by Alex Reppel published on GitHub under the GPL-3.0 License. The dataset consists of five csv files containing responses to functional and dysfunctional questions for various features, along with demographic information about the respondents.\n\n\nExploratory Data Analysis\nLet’s explore data. The dataframe of shape (721, 39) includes an ID column, multiple columns with demographic data such as Income_us, Gender, Age, Employment, and Education, as well as responses to functional and dysfunctional questions (F1_functional, F1_dysfunctional, etc), and columns indicating the importance of certain features to the customer (F1_importance, F2_importance, etc).\n\n\n\n\n\n\n\n\nFigure 2: Histogram of respondents’ age\n\n\n\n\n\nThe customers’ age distribution is relatively balanced, with a slight skew towards younger respondents.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Income distribution\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Employment distribution\n\n\n\n\n\n\nThe income distribution is expectedly skewed to the left. The employment distribution shows that the majority of respondents are employed full-time.\nWhile customer responses might vary based on demographic data, Kano analysis does not consider the demographic characteristics of the respondents.\n\n\nAggregating Responses\nNext, we will aggregate the responses to functional and dysfunctional questions for each feature. The table below shows an example of aggregated answers for a feature with ID = F1.\n\n\n\n\nTable 2: Example of aggregated answers for a feature.\n\n\n\n\n\n\n\n\n\n\nDysfunctional\n1) I like it\n2) I expect it\n3) I am neutral\n4) I can tolerate it\n5) I dislike it\n\n\nID\nFunctional\n\n\n\n\n\n\n\n\n\nF1_\n1) I like it\n12\n10\n21\n25\n9\n\n\n2) I expect it\n6\n6\n14\n10\n3\n\n\n3) I am neutral\n15\n25\n72\n29\n2\n\n\n4) I can tolerate it\n5\n13\n12\n10\n5\n\n\n5) I dislike it\n14\n8\n4\n2\n1\n\n\n\n\n\n\n\n\n\n\n\n\nCalculating Satisfaction and Dissatisfaction Scores\nAfter aggregating the responses, we calculate the satisfaction and dissatisfaction scores for each feature using Equation 1 and Equation 2. The table below shows the qualities: Attractive (A), Indifferent (I), Must-be (M), One-dimensional (O), Questionable (Q), Reverse (R), as well as satisfaction (S) and dissatisfaction (D) scores for each feature.\n\n\n\n\nTable 3: Qualities and satisfaction and dissatisfaction scores for each feature.\n\n\n\n\n\n\n\n\n \nID\nQuestion\nA\nI\nM\nO\nQ\nR\nS\nD\n\n\n\n\n0\nF1\nIf your funds are stored in a way that does not have to be linked to your identity, how do you feel?\n17\n57\n3\n3\n4\n16\n24\n-7\n\n\n1\nF2\nIf it is easy to store funds, how do you feel?\n22\n27\n14\n30\n3\n4\n56\n-48\n\n\n2\nF3\nIf you can access your funds wherever and whenever you want, how do you feel?\n15\n18\n14\n47\n3\n3\n66\n-65\n\n\n3\nF4\nIf it is guaranteed that no one else can access your funds without your permission, how do you feel?\n5\n12\n22\n53\n4\n4\n63\n-82\n\n\n4\nF5\nIf relevant information is always easy to find, how do you feel?\n22\n34\n12\n21\n5\n6\n49\n-37\n\n\n5\nF6\nIf you can transfer funds without having to link that transaction to your name, how do you feel?\n26\n50\n2\n5\n4\n14\n38\n-8\n\n\n6\nF7\nIf it is easy to transfer funds, how do you feel?\n20\n24\n17\n34\n4\n2\n57\n-54\n\n\n7\nF8\nIf you can transfer your funds wherever and whenever you want, how do you feel?\n21\n20\n18\n35\n4\n2\n60\n-56\n\n\n8\nF9\nIf funds are transferred almost instantaneous, how do you feel?\n40\n23\n7\n22\n4\n3\n67\n-31\n\n\n9\nF10\nIf it is guaranteed that no one else can manipulate transfers you have initiated, how do you feel?\n5\n15\n25\n49\n4\n2\n58\n-79\n\n\n10\nF11\nIf relevant information on how to make transfers is always easy to find, how do you feel?\n24\n30\n10\n26\n6\n4\n56\n-40\n\n\n\n\n\n\n\n\n\n\nPlotting the Kano Diagram\nThe last step is to plot the features on a Kano diagram. The quadrant in which the feature is located indicates a Kano category. The further the from the center, the higher the influence on satisfaction or dissatisfaction.\n\n\n\n\n\n\n\n\nFigure 5: Kano diagram"
  },
  {
    "objectID": "posts/20240805-kano-model/index.html#application-of-the-kano-model",
    "href": "posts/20240805-kano-model/index.html#application-of-the-kano-model",
    "title": "Kano Method for Prioritization of Features",
    "section": "Application of the Kano Model",
    "text": "Application of the Kano Model\nThe Kano model can be applied in product development to prioritize features and functionalities based on customer needs and expectations. By categorizing features into the five Kano categories, product managers can identify which features are essential, which are nice to have, and which can create a competitive advantage.\nThe Kano model can also help product managers understand customer preferences and make informed decisions about resource allocation and product development. By focusing on Must-be Quality and One-dimensional Quality features, product managers can ensure that the product meets basic customer requirements and maximizes customer satisfaction.\nIn conclusion, the Kano model is a valuable tool for prioritizing features and functionalities in product development. By understanding customer preferences and categorizing features into the five Kano categories, product managers can create products that meet customer needs and expectations, leading to higher customer satisfaction and competitive advantage."
  },
  {
    "objectID": "posts/20240821-sophisthse/index.html",
    "href": "posts/20240821-sophisthse/index.html",
    "title": "Python Library for Russian Macroeconomics Data",
    "section": "",
    "text": "The article introduces a Python library called sophisthse that provides access to Russian macroeconomic statistics time series published by the National Research University Higher School of Economics (HSE). The library aims to simplify the process of working with Russian macroeconomic data and facilitate the analysis of economic trends in the country. The article demonstrates how to use the library to list available time series, download data, and visualize the time series data."
  },
  {
    "objectID": "posts/20240821-sophisthse/index.html#acknowledgments",
    "href": "posts/20240821-sophisthse/index.html#acknowledgments",
    "title": "Python Library for Russian Macroeconomics Data",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThe library is a port of the sophisthse R package developed by Boris Demeshev. The original package provides similar functionality for working with Russian macroeconomic data in R. The Python version of the library is designed to offer the same features to Python users interested in analyzing Russian economic indicators."
  },
  {
    "objectID": "posts/20240821-sophisthse/index.html#source-data",
    "href": "posts/20240821-sophisthse/index.html#source-data",
    "title": "Python Library for Russian Macroeconomics Data",
    "section": "Source Data",
    "text": "Source Data\nThe sophist.hse.ru website contains wide range of Russian macroeconomic statistics time series, including data on GDP, inflation, unemployment, industrial production, retail sales, and more. The data is originally sourced from the Federal State Statistics Service of the Russian Federation (Rosstat) and other official sources."
  },
  {
    "objectID": "posts/20240821-sophisthse/index.html#features",
    "href": "posts/20240821-sophisthse/index.html#features",
    "title": "Python Library for Russian Macroeconomics Data",
    "section": "Features",
    "text": "Features\nThe sophisthse library provides the following features:\n\nListing available tables with time series data\nDownloading and caching time series data"
  },
  {
    "objectID": "posts/20240821-sophisthse/index.html#installation",
    "href": "posts/20240821-sophisthse/index.html#installation",
    "title": "Python Library for Russian Macroeconomics Data",
    "section": "Installation",
    "text": "Installation\nYou can install the library using pip:\npip install sophisthse"
  },
  {
    "objectID": "posts/20240821-sophisthse/index.html#usage",
    "href": "posts/20240821-sophisthse/index.html#usage",
    "title": "Python Library for Russian Macroeconomics Data",
    "section": "Usage",
    "text": "Usage\nHere is an example of how to use the library to download and plot the time series data.\n\nList Available Time Series\nTime series are published on the sophist.hse.ru website. Initializing sophisthse class will download the list of available tables. You can list the available tables using the list_tables method:\n\nfrom sophisthse import sophisthse\n\nsph = sophisthse()\ntables = sph.list_tables()\ntables\n\n\n\n\n\n\n\n\ndate\nname\n\n\n\n\n0\n2020-05-27 15:46:00\nAGR_M_I - Copy\n\n\n1\n2024-08-08 09:10:00\nAGR_M_I\n\n\n2\n2024-08-08 09:10:00\nAGR_Q_I\n\n\n3\n2024-02-11 19:10:00\nAGR_Y_DIRI\n\n\n4\n2024-07-26 20:04:00\nAPCI3N\n\n\n...\n...\n...\n\n\n284\n2010-03-29 12:57:00\nvrp\n\n\n285\n2010-03-30 12:41:00\nvrp_r\n\n\n286\n2024-08-08 09:12:00\nWAG_M\n\n\n287\n2024-06-07 15:03:00\nWAG_Q\n\n\n288\n2024-03-13 19:36:00\nWAG_Y\n\n\n\n\n289 rows × 2 columns\n\n\n\nThe column named date contains timestamp of the latest update. The dates in this column hint that some tables may be out of date. Let’s take a look at the date of the latest update of the tables.\n\ntables[\"year\"] = tables[\"date\"].dt.year\ntables.groupby(\"year\").size().plot(kind=\"bar\")\n\n\n\n\n\n\n\n\nThe 116 out of 289 are being updated through 2024. Let’s take a look at the most recently updated tables.\n\ntables[\"date\"] = tables[\"date\"].dt.date\n(\n    tables.groupby(\"date\")\n    .agg({\"name\": lambda x: \", \".join(x)})\n    .reset_index()\n    .sort_values(\"date\", ascending=False)\n    .head()\n)\n\n\n\n\n\n\n\n\ndate\nname\n\n\n\n\n38\n2024-08-08\nAGR_M_I, AGR_Q_I, BBR_EA2_M_I, BBR_EA2_Q_I, CN...\n\n\n37\n2024-07-26\nAPCI3N, BMPM3N, CCPM3N, CPPNF3N, ECOG3N, EMAM3...\n\n\n36\n2024-06-07\nWAG_Q\n\n\n35\n2024-06-06\nGOV_Q, M_Q\n\n\n34\n2024-04-05\nINVFC_Q, INVFC_Q_I\n\n\n\n\n\n\n\n\n\nDownload and Plot Time Series\nAccording to the sophist.hse.ru website table AGR_M_I provides the index of real agricultural production in column AGR_M_DIRI, and seasonally adjusted one in the AGR_M_DIRI_SA. Let’s download it and take a look at the data.\n\nagr_m_i = sph.get_table(\"AGR_M_I\")\nagr_m_i.tail()\n\n\n\n\n\n\n\n\nAGR_M_DIRI\nAGR_M_DIRI_SA\n\n\nT\n\n\n\n\n\n\n2024-02\n110.0\n353.3\n\n\n2024-03\n169.6\n355.7\n\n\n2024-04\n190.3\n357.2\n\n\n2024-05\n208.4\n358.0\n\n\n2024-06\n213.2\n358.5\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAfter calling get_table, the data is cached locally. The next time get_table is called, the data will be loaded from the cache. However, if the table is updated on the server, which is checked when the sophisthse class is initialized, the new data will be loaded from the website.\n\n\nThe method returns a pandas DataFrame, so we can plot the data using the plot method.\n\nagr_m_i.plot()\n\n\n\n\n\n\n\n\n\n\nAdditional Example\nAccording to the sophist.hse.ru website table FINMAR_M contains data on the financial market.\n\nfinmar = sph.get_table(\"FINMAR_M\")\nfinmar.tail()\n\n\n\n\n\n\n\n\nRDEXRO_M\nRDEXRM_M\nRTS_M\nIB_M\nGKO_M\nDEP_M\nCR_M\n\n\nT\n\n\n\n\n\n\n\n\n\n\n\n2024-03\n92.37\n92.35\n1131.21\nNaN\nNaN\nNaN\nNaN\n\n\n2024-04\n91.78\n93.05\n1165.23\nNaN\nNaN\nNaN\nNaN\n\n\n2024-05\n89.79\n90.05\n1185.57\nNaN\nNaN\nNaN\nNaN\n\n\n2024-06\n85.75\n85.75\n1134.30\nNaN\nNaN\nNaN\nNaN\n\n\n2024-07\n86.33\n86.11\n1092.61\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nThe first columns are:\n\nRDEXRO_M - Official exchange rate of the ruble against the US dollar.\nRDEXRM_M - Exchange rate of the US dollar on the Moscow Exchange.\nRTS_M - Average RTS index.\n\nThe last four columns show NaNs for the most recent periods. Let’s take a look at the missing values.\n\nsns.heatmap(~finmar.isnull(), cbar=False, cmap=\"Blues\")\n\n\n\n\n\n\n\n\nThe heatmap shows that the data in the IB_M, GKO_M, DEP_M, and CR_M columns is not available for the recent years, while the RDEXRO_M, RDEXRM_M, and RTS_M columns have complete data.\nLet’s plot the RTS_M column. Due to the economic fluctuations, the RTS index has a lot of volatility. We will add the moving average to the plot.\n\nfinmar[\"RTS_M\"].plot(label=\"RTS index\")\nfinmar[\"RTS_M\"].rolling(12, center=True).mean().plot(label=\"12-months MA\")\nplt.legend()"
  },
  {
    "objectID": "posts/20240821-sophisthse/index.html#further-investigation",
    "href": "posts/20240821-sophisthse/index.html#further-investigation",
    "title": "Python Library for Russian Macroeconomics Data",
    "section": "Further Investigation",
    "text": "Further Investigation\nLooking at the tables, one can assume that the suffixes _M, _Q and _Y in the names mean that the data was sampled monthly, quarterly and annually, respectively. Let’s check this assumption.\n\ndef guess_period(name: str):\n    if \"_M\" in name:\n        return \"month\"\n    if \"_Q\" in name:\n        return \"quarter\"\n    if \"_Y\" in name:\n        return \"year\"\n    return \"unknown\"\n\n\ntables[\"period\"] = tables[\"name\"].apply(guess_period)\n\ntables.groupby(\"period\").size().plot(kind=\"bar\")\n\n\n\n\n\n\n\n\nThe bar plot shows that the assumption is partially correct. There are tables with unknown sampling frequency. This information can be drawn from the actual series after downloading data. It’s not necessary for demonstrative purposes, so we will skip this step.\nLet’s calculate the correlation matrix for the monthly data.\n\nfrom time import sleep\nfrom random import randint\nimport pandas as pd\n\n\nmonthly_tables = tables.query(\"period == 'month' & year &gt;= 2023\")\n\ndf = pd.DataFrame(index=pd.period_range(start=\"2014-01-01\", end=\"2025-01-01\", freq=\"M\"))\nfor table in monthly_tables[\"name\"]:\n    df_add = sph.get_table(table)\n    # We don't want to overload the server\n    # sleep(randint(1, 3))\n    df = df.merge(df_add, left_index=True, right_index=True)\n\n# We do not want to deal with NaNs, at least at this time :)\ndf = df.dropna(how=\"any\", axis=1)\n\n# Keep only columns with \"_M\" suffix to skip derived columns\ncolumns = [col for col in df.columns if col.endswith(\"_M\")]\n\ncorr = df[columns].corr()\n\nsns.heatmap(\n    corr,\n    vmin=-1.0,\n    vmax=1.0,\n    xticklabels=corr.columns.values,  # type: ignore\n    yticklabels=corr.columns.values,  # type: ignore\n    cmap=\"coolwarm\",\n)\n\n\n\n\n\n\n\n\nThe last column (or row) is the WAG_C_M. This is a monthly series of the average nominal wage. It correlates with many economics indicators, and it worth noting that CBEX_M (Consolidated Budget Expenditures) explains almost 84% of WAG_C_M variability. Let’s take a look at these two series.\n\ndf_norm = df[columns].apply(lambda x: (x - x.mean()) / x.std())\ndf_norm[[\"WAG_C_M\", \"CBEX_M\"]].plot()\n\n\n\n\n\n\n\n\nThe plot shows that the WAG_C_M series follows the CBEX_M series with great accuracy. This is an interesting observation that can be used in further analysis."
  },
  {
    "objectID": "posts/20240821-sophisthse/index.html#conclusion",
    "href": "posts/20240821-sophisthse/index.html#conclusion",
    "title": "Python Library for Russian Macroeconomics Data",
    "section": "Conclusion",
    "text": "Conclusion\nThe sophisthse library provides a convenient way to access and work with Russian macroeconomic time series data. The library simplifies the process of downloading and caching data, making it easier to analyze economic trends in Russia. The library is a valuable resource for researchers, analysts, and anyone interested in studying the Russian economy."
  }
]