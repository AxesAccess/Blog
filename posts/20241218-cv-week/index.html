<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.5">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Aleksei">
<meta name="dcterms.date" content="2024-12-18">

<title>CV Week 2024 – Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-ff4371ef257df69894857e99c6ad0d06.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-4a1bf7c3264c6d434bc0885489b31088.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-H2HYLD7LV5"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-H2HYLD7LV5', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/AxesAccess"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/telecamera/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">CV Week 2024</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">CompVis</div>
                <div class="quarto-category">ML</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Aleksei </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 18, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>In November 2024, the Yandex School of Data Analysis (YSDA) and Yandex conducted an open online intensive course on computer vision, focusing on generative diffusion models that underpin many visual services.</p>
</section>
<section id="format-and-structure" class="level2">
<h2 class="anchored" data-anchor-id="format-and-structure">Format and Structure</h2>
<p><strong>Dates</strong>: November 25–29, 2024.</p>
<p><strong>Format</strong>: All lectures and seminars were pre-recorded and broadcast on YouTube. During the broadcasts, participants could ask questions in the comments and interact with instructors and fellow participants via a dedicated Telegram channel.</p>
<p><strong>Participation and Certification</strong>: The broadcasts were open to all without selection. However, to receive a certificate, participants needed to complete Qualifying assignments on the Yandex Contest platform and submit a final project.</p>
</section>
<section id="lectures-and-seminars" class="level2">
<h2 class="anchored" data-anchor-id="lectures-and-seminars">Lectures and Seminars</h2>
<p>The intensive course’s schedule consisted of lectures and seminars. Five days from Monday to Friday, in the evenings, there were a lecture for approximately an hour and then a seminar where the instructor provided with the implementation of the different aspects of image generation approaches.</p>
<p><a href="https://www.youtube.com/live/E6fzcThxGK0?si=2NuhmHIAKBqXuKW3">YouTube Playlist</a></p>
<p><strong>November 25</strong></p>
<ul>
<li><p>Lecture 1: Introduction to Diffusion Models (Lecturer: Dmitry Baranchuk, Researcher, Yandex Research).</p></li>
<li><p>Seminar 1: Basic Implementation of Diffusion Models (Instructor: Sergey Kastrulin, Researcher, Yandex Research).</p></li>
</ul>
<p><strong>November 26</strong></p>
<ul>
<li><p>Lecture 2: Formulating Diffusion Models through Stochastic and Ordinary Differential Equations (Lecturer: Mikhail Romanov, Developer, Computer Vision Service).</p></li>
<li><p>Seminar 2: Implementing an Efficient Sampler (Instructor: Mikhail Romanov).</p></li>
</ul>
<p><strong>November 27</strong></p>
<ul>
<li><p>Lecture 3: Architectures of Diffusion Models, Training and Sampling Methods, and Text-to-Image Generation (Lecturer: Denis Kuznedelev, Researcher, Yandex Research).</p></li>
<li><p>Seminar 3: Generating Images from Text Descriptions (Instructor: Denis Kuznedelev).</p></li>
</ul>
<p><strong>November 28</strong></p>
<ul>
<li><p>Lecture 4: Distillation of Diffusion Models Using ODE-Based Methods (Lecturer: Nikita Starodubtsev, Researcher, Yandex Research ML Residency).</p></li>
<li><p>Lecture 5: Distillation of Diffusion Models Without ODEs (Lecturer: Dmitry Baranchuk).</p></li>
<li><p>Seminar 4: Implementing Consistent Models for Text-to-Image Generation (Instructor: Nikita Starodubtsev).</p></li>
</ul>
<p><strong>November 29</strong></p>
<ul>
<li><p>Lecture 6: Fine-Tuning Diffusion Models Using Reinforcement Learning Methods (Lecturer: Alexander Shishenya, Developer, Computer Vision Service).</p></li>
<li><p>Lecture 7: YandexART — Industrial Diffusion Model (Lecturer: Sergey Kastrulin).</p></li>
</ul>
</section>
<section id="qualifying-assignment" class="level2">
<h2 class="anchored" data-anchor-id="qualifying-assignment">Qualifying Assignment</h2>
<p>The Qualifying Assignment was a prerequisite for accessing the final project and comprised three programming tasks designed to assess participants’ proficiency in fundamental machine learning concepts and PyTorch implementation.</p>
<section id="reshape-a-tensor" class="level3">
<h3 class="anchored" data-anchor-id="reshape-a-tensor">1. Reshape a Tensor</h3>
<p>The first task required reshaping a list or tensor by swapping its first two dimensions without utilizing PyTorch or NumPy methods. For example:</p>
<p><span class="math inline">\(\begin{bmatrix}1 &amp; 2 &amp; 3\\4 &amp; 5 &amp; 6\end{bmatrix}\)</span> should be transformed into <span class="math inline">\(\begin{bmatrix}1 &amp; 4 \\ 2 &amp; 5 \\ 3 &amp; 6\end{bmatrix}\)</span>.</p>
<p>This task tested participants’ understanding of tensor manipulation at a fundamental level, emphasizing the importance of grasping underlying data structures without relying on high-level library functions.</p>
</section>
<section id="encoder-and-decoder-architecture" class="level3">
<h3 class="anchored" data-anchor-id="encoder-and-decoder-architecture">2. Encoder and Decoder Architecture</h3>
<p>The second task involved implementing the encoder and decoder components of a Variational Autoencoder (VAE) architecture. VAEs are generative models that learn to represent data in a latent space, enabling the generation of new, similar data points. This task assessed participants’ ability to construct neural network architectures.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="vae.png" class="img-fluid figure-img"></p>
<figcaption>Variational autoencoder architecture</figcaption>
</figure>
</div>
</section>
<section id="vae-loss-function" class="level3">
<h3 class="anchored" data-anchor-id="vae-loss-function">3. VAE Loss Function</h3>
<p>The third task required identifying and correcting errors in the VAE loss function implementation. To validate the correctness of the function, participants trained the VAE and performed inference on the MNIST digits dataset. The VAE loss function typically comprises two components:</p>
<ul>
<li><p>Reconstruction Loss: Measures how well the decoder reconstructs the input data.</p></li>
<li><p>Kullback-Leibler (KL) Divergence: Regularizes the learned latent space to align with a predefined distribution, often a standard normal distribution.</p></li>
</ul>
<p>Accurate implementation of this loss function is crucial for the VAE to learn meaningful latent representations and generate coherent outputs.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="qualifying-mnist.png" class="img-fluid figure-img"></p>
<figcaption>Reconstructed digits using VAE</figcaption>
</figure>
</div>
<p>Each of the first two tasks was worth up to 2 points, while the third task could earn up to 4 points. A minimum of 6 points was required to pass, ensuring that participants had a solid grasp of the necessary concepts to proceed to the final project.</p>
</section>
</section>
<section id="final-project" class="level2">
<h2 class="anchored" data-anchor-id="final-project">Final Project</h2>
<p>In the final project, participants had to distill a multi-step diffusion model into a more efficient, few-step student model, significantly enhancing generation speed.</p>
<p>The project focused on implementing the Consistency Distillation framework, a method that accelerates diffusion models by enforcing self-consistency along learned trajectories.</p>
<p>Participants would apply this technique to distill the <a href="https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5">Stable Diffusion 1.5</a> (SD 1.5) model, a latent text-to-image diffusion model capable of generating photo-realistic images from textual descriptions.</p>
<p>The project comprised eight tasks, each building upon the previous, guiding participants toward developing a proficient model capable of generating images in just four steps.</p>
<section id="teacher-model" class="level3">
<h3 class="anchored" data-anchor-id="teacher-model">Teacher Model</h3>
<p>The initial model for our experiments is Stable Diffusion 1.5, a pre-trained latent text-to-image diffusion model. This serves as the “teacher” model in our distillation process.</p>
<p>For the prompt: “A sad puppy with large eyes”, running the model with 50 steps and a guidance_scale of 7.5 produces the following high-quality images:</p>
<p><img src="sd-50-steps.png" class="img-fluid"></p>
<p>However, when the number of steps is reduced to 4, the output becomes blurry and lacks detail:</p>
<p><img src="sd-4-steps.png" class="img-fluid"></p>
<p>This demonstrates the trade-off between the number of sampling steps and image quality in diffusion models. Our goal is to bridge this gap by using techniques like Consistency Distillation to achieve similar quality with fewer steps.</p>
</section>
<section id="consistency-training" class="level3">
<h3 class="anchored" data-anchor-id="consistency-training">Consistency Training</h3>
<p>The model will be trained on a subset of the COCO Dataset comprising 5,000 images. To reduce memory consumption, we will train LoRA (Low-Rank Adaptation) adapters for the U-Net convolutional neural network instead of fine-tuning the entire model.</p>
<p>This approach significantly decreases the number of trainable parameters and activation memory, enhancing efficiency during training.</p>
<p>Additionally, implementing techniques such as gradient checkpointing can further reduce memory usage, albeit with a potential increase in training time.</p>
<p>By employing these strategies, we aim to achieve effective model performance while operating within the memory constraints of our training environment.</p>
<p>The result with <code>guidance_scale = 2</code> looks sharper, but the quality is still far from desired.</p>
<p><img src="cm.png" class="img-fluid"></p>
</section>
<section id="consistency-distillation" class="level3">
<h3 class="anchored" data-anchor-id="consistency-distillation">Consistency Distillation</h3>
<p>In the Consistency Distillation phase, we enhance the model’s quality by incorporating a teacher model within the U-Net architecture, utilizing LoRA adapters trained in the previous step. This integration refines the student’s learning process, leading to significantly improved image generation results.</p>
<p>Consistency Distillation is a technique that accelerates diffusion models by enforcing self-consistency along learned trajectories. By aligning the student model’s outputs with those of the teacher model, the student learns to produce high-quality images in fewer steps. Implementing this method with LoRA adapters allows for efficient training, as LoRA reduces the number of trainable parameters, thereby decreasing memory consumption and computational load.</p>
<p><img src="cd.png" class="img-fluid"></p>
</section>
<section id="multi-boundary-consistency-distillation" class="level3">
<h3 class="anchored" data-anchor-id="multi-boundary-consistency-distillation">Multi-boundary Consistency Distillation</h3>
<p>In the Multi-boundary Consistency Distillation phase, we draw inspiration from recent advancements in generative modeling, particularly the work by researchers from Google DeepMind. In their paper “Multistep Consistency Models” (<a href="https://arxiv.org/pdf/2403.06807">arXiv:2403.06807v3</a>), they propose a method that interpolates between consistency models and diffusion models, allowing for a trade-off between sampling speed and quality.</p>
<p>By employing 2 to 8 sampling steps, this approach achieves performance comparable to traditional diffusion models but with significantly reduced computational resources. This reduction in sampling steps leads to decreased memory usage and computational load, making the models more efficient without compromising output quality.</p>
<p>Implementing this technique involves training the model to maintain consistency across multiple steps, effectively enabling it to generate high-quality samples in fewer iterations. This advancement is particularly beneficial for applications requiring rapid generation or operating under resource constraints.</p>
<p><img src="mbcd.png" class="img-fluid"></p>
</section>
<section id="graded-assignments" class="level3">
<h3 class="anchored" data-anchor-id="graded-assignments">Graded Assignments</h3>
<p>There were 4 automatically graded assignments and one teacher graded assignment.</p>
<section id="implementation-of-ddim-solver-step" class="level4">
<h4 class="anchored" data-anchor-id="implementation-of-ddim-solver-step">1. Implementation of DDIM Solver Step</h4>
<p>In diffusion models, the forward diffusion process gradually transforms images into noise, following the distribution:</p>
<p><span class="math display">\[ q(\mathbf{x}_t | \mathbf{x}_0)= {N}(\mathbf{x}_t | \alpha_t \mathbf{x}_0, \sigma^2_t I)\]</span></p>
<p>At time step <span class="math inline">\(t\)</span>, the noisy image <span class="math inline">\(\mathbf{x}_t\)</span> can be represented as: <span class="math inline">\(\mathbf{x}_t = \alpha_t \mathbf{x}_0 + \sigma_t \epsilon\)</span>, where <span class="math inline">\(\epsilon{\sim} {N}(0, I)\)</span>.</p>
<p>The goal of the diffusion model is to solve the inverse problem, reconstructing an image from noise. This reverse process is formulated as an ordinary differential equation (ODE):</p>
<p><span class="math inline">\(dx = \left[ f(\mathbf{x}, t) - \frac{1}{2} \nabla_{\mathbf{x}_t} \log p_t(\mathbf{x}) \right] dt\)</span>, where <span class="math inline">\(f(\mathbf{x}, t)\)</span> is known from the given noise process, and <span class="math inline">\(\nabla_{\mathbf{x}_t} \log p_t(\mathbf{x}_t)\)</span> (<em>score function</em>) is estimated using a neural network: <span class="math inline">\(s_\theta(\mathbf{x}_t, t) \approx \nabla_{\mathbf{x}_t} \log p_t(\mathbf{x}_t)\)</span>.</p>
<p>Thus, having an estimate for <span class="math inline">\(\nabla_{\mathbf{x}_t} \log p_t(\mathbf{x})\)</span>, we can solve this ODE starting from random noise and generate a picture.</p>
<p>For this assignment, participants implemented the step from <span class="math inline">\(\mathbf{x}_t\)</span> to <span class="math inline">\(\mathbf{x}_s\)</span> using the Denoising Diffusion Implicit Models (DDIM) framework:</p>
<p><span class="math display">\[\mathbf{x}_s = DDIM(\epsilon_\theta, \mathbf{x}_t, t, s) = \alpha_s \cdot \left(\frac{\mathbf{x}_t - \sigma_t \epsilon_\theta}{\alpha_t} \right) + \sigma_s \epsilon_\theta\]</span></p>
<p>Most of the function was pre-written by the instructors, but participants were required to correctly set <span class="math inline">\(\alpha_t\)</span> and <span class="math inline">\(\sigma_t\)</span> using the DDIMScheduler. This task tested participants’ understanding of the underlying mathematical concepts and their ability to implement them in code.</p>
</section>
<section id="implementation-of-noise-process" class="level4">
<h4 class="anchored" data-anchor-id="implementation-of-noise-process">2. Implementation of Noise Process</h4>
<p>The second task involved implementing the function <code>q_sample(x, t, scheduler, noise)</code> to simulate the forward diffusion process. The function follows the mathematical formulation:</p>
<p><span class="math inline">\(q(\mathbf{x}_t | \mathbf{x}_0)= {N}(\mathbf{x}_t | \alpha_t \mathbf{x}_0, \sigma^2_t I)\)</span> where the noisy image <span class="math inline">\(x_t\)</span> at time <span class="math inline">\(t\)</span> is calculated as:</p>
<p><span class="math inline">\(\mathbf{x}_t = \alpha_t \mathbf{x}_0 + \sigma_t \epsilon\)</span>, where <span class="math inline">\(\epsilon{\sim} {N}(0, I)\)</span>.</p>
<p>Participants were required to:</p>
<ol type="1">
<li><p>Use the <strong>scheduler</strong> to retrieve <span class="math inline">\(\alpha_t\)</span> and <span class="math inline">\(\sigma_t\)</span> values for the given timestep <span class="math inline">\(t\)</span>.</p></li>
<li><p>Generate random noise <span class="math inline">\(\epsilon \sim \mathcal{N}(0, I)\)</span> and apply it to the formula above to simulate the noisy image <span class="math inline">\(\mathbf{x}_t\)</span>.</p></li>
<li><p>Handle edge cases for boundary points, ensuring the process remains valid when <span class="math inline">\(t=0\)</span>.</p></li>
</ol>
<p>This assignment focused on implementing the forward diffusion process accurately while accounting for the nuances of boundary conditions, which added complexity to the task.</p>
</section>
<section id="consistency-training-1" class="level4">
<h4 class="anchored" data-anchor-id="consistency-training-1">3. Consistency Training</h4>
<p>Consistency distillation leverages the teacher model to obtain the second point on the ODE trajectory, which can also be computed using the DDIM formula. The task required participants to derive the function for this computation based on the definitions of the noise process and the score function.</p>
<p><span class="math inline">\(\epsilon_\theta(x_t, t) = - \sigma_t s_\theta(x_t, t)\)</span></p>
<p><span class="math inline">\(s_\theta(x_t, t) \approx \nabla_{x_t} \log q(x_t) = \mathop{\mathbb{E}}_{\mathbf{x}\sim p_{data}}\left [ \nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t | \mathbf{x}) \vert \mathbf{x}_t \right ] \approx \nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t \vert \mathbf{x})\)</span></p>
</section>
<section id="multi-boundary-timesteps" class="level4">
<h4 class="anchored" data-anchor-id="multi-boundary-timesteps">4. Multi-boundary Timesteps</h4>
<p>The objective of this task was to implement the get_multi_boundary_timesteps function, which generates boundary points for multi-step consistency training. These boundary points define the timesteps used in sampling and play a critical role in the effectiveness of the distillation process.</p>
<p>The task itself was somewhat ambiguous, leaving room for interpretation. Participants were expected to:</p>
<ul>
<li>Understand how multi-boundary timesteps are utilized in multi-step consistency models. Implement the function to output the appropriate boundary points.</li>
<li>Ensure that the timesteps are well-distributed and align with the requirements of the consistency distillation framework.</li>
</ul>
</section>
<section id="generated-images" class="level4">
<h4 class="anchored" data-anchor-id="generated-images">5. Generated Images</h4>
<p>Once all the graded and non-graded tasks were completed, the next step was to train the Multi-boundary Consistency Model. This involved applying the techniques and frameworks developed in earlier tasks to produce high-quality generated images.</p>
<p>After training, participants were required to upload the trained model along with generated results for assessment. This provided an opportunity to showcase the practical implementation of multi-step consistency techniques.</p>
<p>The trained model was evaluated based on its ability to generate images that were consistent and high-quality, even with a reduced number of sampling steps.</p>
<div>

</div>
<div class="quarto-layout-panel" data-layout="[[1,1], [1]]">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="CV_week_cd.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Consistency Distillation"><img src="CV_week_cd.png" class="img-fluid figure-img" alt="Consistency Distillation"></a></p>
<figcaption>Consistency Distillation</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="CV_week_mbcd.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Multi-boundary Consistency Distillation"><img src="CV_week_mbcd.png" class="img-fluid figure-img" alt="Multi-boundary Consistency Distillation"></a></p>
<figcaption>Multi-boundary Consistency Distillation</figcaption>
</figure>
</div>
</div>
</div>
</div>
<section id="sampling-prompts" class="level5">
<h5 class="anchored" data-anchor-id="sampling-prompts">Sampling Prompts</h5>
<ol type="1">
<li>A sad puppy with large eyes.</li>
<li>Astronaut in a jungle, cold color palette, muted colors, detailed, 8k.</li>
<li>A photo of beautiful mountain with realistic sunset and blue lake, highly detailed, masterpiece.</li>
<li>A girl with pale blue hair and a cami tank top.</li>
<li>A lighthouse in a giant wave, origami style.</li>
<li>Belle epoque, christmas, red house in the forest, photo realistic, 8k.</li>
<li>A small cactus with a happy face in the Sahara desert.</li>
<li>Green commercial building with refrigerator and refrigeration units outside.</li>
</ol>
</section>
</section>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Participating in YSDA’s intensive training programs has been both challenging and rewarding. My first experience was during <a href="https://shad.yandex.ru/gptweek">GPT Week</a> in 2023, which did not require a qualifying assignment. The final project involved training a model to summarize articles, with flexibility regarding the architecture and implementation details.</p>
<p>In contrast, CV Week 2024 presented a more rigorous experience. Both the qualifying assignment and the final project demanded significant effort, especially given my limited background in computer vision. This intensive pushed me to expand my knowledge and skills, making the experience both demanding and educational.</p>
<p>For those interested in exploring similar topics, YSDA offers a range of online courses. These resources can provide a solid foundation for tackling advanced subjects in data analysis and machine learning.</p>
<p>Source code and presentations can be found in my GitHub repository: <a href="https://github.com/AxesAccess/CV_week">CV_Week</a>.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/AlekseiPrishchepo\.github\.io\/Blog\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>